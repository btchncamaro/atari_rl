{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs.shape: (210, 160, 3)\n",
      "env.action_space: Discrete(6)\n",
      "[0.11849965 0.00589975 0.8756006 ]\n",
      "test_softmax: [0.13103449 0.13103449 0.13824064 0.13103449 0.33762142 0.13103449]\n",
      "multinomial_action_array: [0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "\n",
    "# To plot pretty figures and animations\n",
    "%matplotlib nbagg\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "####### Space Invaders\n",
    "load_model           = True\n",
    "environment_name     = \"SpaceInvaders-v0\"\n",
    "discrete_actions     = 6\n",
    "saver_file           = \"./models/space_invaders_rl\"\n",
    "height               = 210\n",
    "width                = 160\n",
    "channels             = 1\n",
    "frames_captured      = 5\n",
    "learning_rate        = .000000004  #.00000002\n",
    "n_epochs             = 1001\n",
    "use_ai_every_x_epoch = 1\n",
    "discount_decay_rate  = 0.95\n",
    "frame_limit          = 2000\n",
    "max_score            = 5\n",
    "\n",
    "####### Pitfall\n",
    "# environment_name     = \"Pitfall-v0\"\n",
    "# discrete_actions     = 18\n",
    "# load_model           = False\n",
    "# saver_file           = \"./models/pitfall_rl\"\n",
    "# height               = 210\n",
    "# width                = 160\n",
    "# channels             = 1\n",
    "# frames_captured      = 5\n",
    "# learning_rate        =.00001\n",
    "# n_epochs             = 11\n",
    "# use_ai_every_x_epoch = 5\n",
    "# discount_decay_rate  = 0.95\n",
    "# frame_limit          = 1000\n",
    "\n",
    "####### River Raid\n",
    "# environment_name     = \"Riverraid-v0\"\n",
    "# discrete_actions     = 18\n",
    "# load_model           = True\n",
    "# saver_file           = \"./models/pitfall_rl\"\n",
    "# height               = 210\n",
    "# width                = 160\n",
    "# channels             = 1\n",
    "# frames_captured      = 5\n",
    "# learning_rate        =.00001\n",
    "# n_epochs             = 501\n",
    "# use_ai_every_x_epoch = 5\n",
    "# discount_decay_rate  = 0.95\n",
    "# frame_limit          = 1000\n",
    "\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    img = obs\n",
    "    img = img.mean(axis=2) # to greyscale\n",
    "    #img = (img - 128) / 128 - 1 # normalize from -1. to 1.\n",
    "    img = img / 256.0  # normalize from 0 to 1.\n",
    "    return img\n",
    "\n",
    "def show_observation(image, title=\"Image\"):\n",
    "    plt.figure(figsize=(11, 7))\n",
    "    plt.subplot(121)\n",
    "    plt.title(title)\n",
    "    plt.imshow(image) #cmap=\"gray\"\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x)\n",
    "    sum_ex = np.sum( np.exp(x))\n",
    "    return ex/sum_ex\n",
    "    \n",
    "env = gym.make(environment_name)\n",
    "observation = env.reset()\n",
    "print(\"obs.shape: {}\".format(observation.shape)) #obs.shape: (210, 160, 3)\n",
    "print(\"env.action_space: {}\".format(env.action_space)) #env.action_space: Discrete(9)\n",
    "\n",
    "for step in range(102):\n",
    "    observation, reward_float, done_bool, info_dict = env.step(1)\n",
    "    obs_greyscale = preprocess_observation(observation)\n",
    "\n",
    "#show_observation(observation)\n",
    "#show_observation(obs_greyscale)\n",
    "    \n",
    "print (softmax([1,-2,3]))\n",
    "\n",
    "test_softmax = softmax([4.3210541e-25, 5.4929095e-33, 5.3535387e-02, 1.2303401e-42, 9.4646466e-01, 1.9473004e-27])\n",
    "print (\"test_softmax: {}\".format(test_softmax))\n",
    "\n",
    "multinomial_action_array = np.random.multinomial(1, test_softmax)\n",
    "print (\"multinomial_action_array: {}\".format(multinomial_action_array))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "eps_min = 0.00\n",
    "eps_max = 0.5\n",
    "eps_decay_steps = 25000\n",
    "\n",
    "def helper_discount_rewards(rewards, discount_rate, begin_index, end_index):\n",
    "    '''\n",
    "    Takes in rewards and applies discount rate\n",
    "    '''\n",
    "    discounted_rewards = np.zeros(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "        \n",
    "    relevant_discounted_rewards = discounted_rewards[begin_index:end_index]\n",
    "    \n",
    "    reward_mean = relevant_discounted_rewards.mean()\n",
    "    reward_std = relevant_discounted_rewards.std()\n",
    "        \n",
    "    #return discounted_rewards\n",
    "    return [(discounted_reward - reward_mean)/reward_std for discounted_reward in discounted_rewards]\n",
    "\n",
    "def epsilon_greedy(optimal_action, number_outputs, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    \n",
    "    if step % 1000 == 0:\n",
    "        print(\"step: \" + str(step) + \", epsilon: \" + str(epsilon))\n",
    "        \n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(number_outputs-1) # random action\n",
    "    else:\n",
    "        return optimal_action # optimal action\n",
    "\n",
    "# print(\"epsilon_greedy(4): \" + str(epsilon_greedy(4,9,50))) \n",
    "# print(\"epsilon_greedy(4): \" + str(epsilon_greedy(4,9,5000)))\n",
    "# print(\"epsilon_greedy(4): \" + str(epsilon_greedy(4,9,500000)))\n",
    "    \n",
    "def action_to_one_hot(action, possible_action_count):\n",
    "    #9 possible positions of the joystick \n",
    "    #(0=center, 1=up, 2=right, 3=left, 4=down, 5=upper-right, 6=upper-left, 7=lower-right, 8=lower-left)\n",
    "    \n",
    "    return_array = np.zeros(possible_action_count)\n",
    "    action_int = int(action)\n",
    "    \n",
    "    return_array[action_int] = 1.0\n",
    "\n",
    "    return return_array\n",
    "\n",
    "def get_average_logits (logits_list, discounted_rewards):\n",
    "\n",
    "    logit_sums = np.zeros(len(logits_list[0][0]))\n",
    "    logit_sums_counter = np.ones(len(logits_list[0][0]))\n",
    "\n",
    "    for this_logit, this_reward in zip(logits_list, discounted_rewards):\n",
    "        temp_array = np.zeros(len(logits_list[0][0]))\n",
    "        temp_counter_array = np.zeros(len(logits_list[0][0]))\n",
    "        \n",
    "        action = np.argmax(this_logit)\n",
    "        temp_array[action] = this_logit[0][action]\n",
    "        logit_sums = logit_sums + temp_array*this_reward\n",
    "        temp_counter_array[action] = 1\n",
    "        logit_sums_counter = logit_sums_counter + temp_counter_array\n",
    "        \n",
    "    return (logit_sums/logit_sums_counter)\n",
    "\n",
    "\n",
    "# print(\"action_to_one_hot(3, 9): \" + str(action_to_one_hot(3.0, 9)))\n",
    "# print(\"action_to_one_hot(9, 9): \" + str(action_to_one_hot(8, 9)))\n",
    "\n",
    "#print(get_average_logits(all_logits, discounted_rewards))\n",
    "\n",
    "\n",
    "#print(all_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden1: Tensor(\"hidden1/Elu:0\", shape=(?, 512), dtype=float32)\n",
      "hidden2: Tensor(\"hidden2/Elu:0\", shape=(?, 256), dtype=float32)\n",
      "hidden3: Tensor(\"hidden3/Elu:0\", shape=(?, 256), dtype=float32)\n",
      "logits: Tensor(\"output/BiasAdd:0\", shape=(?, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "conv1_fmaps = 32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 2\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "conv2_fmaps = 64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "n_hidden_in = 64 * 263 * 40  # conv3 has 64 maps of 525x80 each\n",
    "\n",
    "n_inputs  = discrete_actions\n",
    "n_outputs = discrete_actions\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 256\n",
    "n_hidden3 = 256\n",
    "n_hidden4 = 256\n",
    "n_hidden5 = 256\n",
    "n_hidden6 = 256\n",
    "n_hidden7 = 128\n",
    "n_hidden8 = 64\n",
    "\n",
    "dropout_keep_prob = 1.0\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#with tf.name_scope(\"inputs\"):\n",
    "#tf_input_frame = tf.placeholder(tf.float32, shape=(None, height * width * channels))\n",
    "tf_input_frame = tf.placeholder(tf.float32, shape=(None, height*frames_captured, width, channels))\n",
    "tf_input_value = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
    "tf_input_learning_rate = tf.placeholder(tf.float32)\n",
    "tf_dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "tf_reward = tf.placeholder(tf.float32)\n",
    "    \n",
    "#with tf.name_scope(\"hidden\"):\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "\n",
    "convs   = [16,32]\n",
    "kerns   = [8,8]\n",
    "strides = [4,4]\n",
    "pads    = 'valid'\n",
    "activ   = tf.nn.elu\n",
    "\n",
    "# Policy Network\n",
    "conv1 = tf.layers.conv2d(\n",
    "        inputs = tf_input_frame,\n",
    "        filters = convs[0],\n",
    "        kernel_size = kerns[0],\n",
    "        strides = strides[0],\n",
    "        padding = pads,\n",
    "        activation = activ,\n",
    "        name='conv1')\n",
    "\n",
    "conv2 = tf.layers.conv2d(\n",
    "        inputs=conv1,\n",
    "        filters = convs[1],\n",
    "        kernel_size = kerns[1],\n",
    "        strides = strides[1],\n",
    "        padding = pads,\n",
    "        activation = activ,\n",
    "        name='conv2')\n",
    "\n",
    "flat = tf.layers.flatten(conv2)\n",
    "\n",
    "\n",
    "\n",
    "hidden1 = tf.layers.dense(flat, n_hidden1, activation=tf.nn.elu, name=\"hidden1\", kernel_initializer=initializer)\n",
    "#hidden1_drop = tf.nn.dropout(hidden1, tf_dropout_keep_prob)\n",
    "hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.elu, name=\"hidden2\", kernel_initializer=initializer)\n",
    "#hidden2_drop = tf.nn.dropout(hidden2, tf_dropout_keep_prob)\n",
    "hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.elu, name=\"hidden3\", kernel_initializer=initializer)\n",
    "\n",
    "hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.elu, name=\"hidden4\", kernel_initializer=initializer)\n",
    "\n",
    "hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.elu, name=\"hidden5\", kernel_initializer=initializer)\n",
    "\n",
    "hidden6 = tf.layers.dense(hidden5, n_hidden6, activation=tf.nn.elu, name=\"hidden6\", kernel_initializer=initializer)\n",
    "\n",
    "hidden7 = tf.layers.dense(hidden6, n_hidden7, activation=tf.nn.elu, name=\"hidden7\", kernel_initializer=initializer)\n",
    "\n",
    "hidden8 = tf.layers.dense(hidden7, n_hidden8, activation=tf.nn.elu, name=\"hidden8\", kernel_initializer=initializer)\n",
    "\n",
    "#with tf.name_scope(\"output\"):\n",
    "logits = tf.layers.dense(hidden8, n_outputs, name=\"output\")\n",
    "\n",
    "#with tf.name_scope(\"train\"):\n",
    "#cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=tf_input_value, logits=logits)\n",
    "#loss = tf.reduce_mean(cross_entropy)\n",
    "loss = tf.reduce_mean(tf.square(logits-tf_input_value))\n",
    "optimizer = tf.train.AdamOptimizer(tf_input_learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print (\"hidden1: \" + str(hidden1))\n",
    "print (\"hidden2: \" + str(hidden2))\n",
    "print (\"hidden3: \" + str(hidden3))\n",
    "print (\"logits: \" + str(logits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model before training: ./models/space_invaders_rl\n",
      "INFO:tensorflow:Restoring parameters from ./models/space_invaders_rl\n",
      "Using strict AI actions\n",
      "Death at frame 173\n",
      "Death at frame 501\n",
      "Death at frame 615\n",
      "Epoch: 0, frames: 616, score: 210.0, average loss: 0.11620613068751116\n",
      "actions trained:           [ 76. 222.  73.  92. 117.  35.]\n",
      "actions out while training:[ 77. 224.  86.  80. 101.  47.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 247\n",
      "Death at frame 459\n",
      "Death at frame 606\n",
      "Epoch: 1, frames: 607, score: 210.0, average loss: 0.11986857366005292\n",
      "actions trained:           [158. 267.  90.  44.  15.  32.]\n",
      "actions out while training:[136. 267.  94.  54.  20.  35.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 138\n",
      "Death at frame 311\n",
      "Death at frame 459\n",
      "Epoch: 2, frames: 460, score: 155.0, average loss: 0.1392513610755508\n",
      "actions trained:           [ 61. 196. 161.  16.  16.   9.]\n",
      "actions out while training:[ 59. 194. 161.  18.  16.  11.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 189\n",
      "Death at frame 826\n",
      "Death at frame 975\n",
      "Epoch: 3, frames: 976, score: 265.0, average loss: 0.12666243957312778\n",
      "actions trained:           [130. 303. 160.  89. 127. 166.]\n",
      "actions out while training:[113. 314. 140.  94. 149. 165.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 214\n",
      "Death at frame 570\n",
      "Death at frame 873\n",
      "Epoch: 4, frames: 874, score: 300.0, average loss: 0.13652223728428572\n",
      "actions trained:           [ 32. 270. 199.  97. 133. 142.]\n",
      "actions out while training:[ 35. 255. 212. 105. 123. 143.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 1008\n",
      "Death at frame 1130\n",
      "Death at frame 1524\n",
      "Epoch: 5, frames: 1525, score: 240.0, average loss: 0.15715112355709968\n",
      "actions trained:           [141. 924. 129. 111. 139.  80.]\n",
      "actions out while training:[189. 767. 141. 166. 179.  82.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 207\n",
      "Death at frame 298\n",
      "Death at frame 399\n",
      "Epoch: 6, frames: 400, score: 80.0, average loss: 0.11830034463555109\n",
      "actions trained:           [104. 144.  40.  50.  46.  15.]\n",
      "actions out while training:[ 88. 149.  40.  58.  50.  14.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 262\n",
      "Death at frame 364\n",
      "Death at frame 467\n",
      "Epoch: 7, frames: 468, score: 125.0, average loss: 0.13179491647472627\n",
      "actions trained:           [ 69. 187.  84.  25.  58.  44.]\n",
      "actions out while training:[ 61. 186.  86.  28.  58.  48.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 385\n",
      "Death at frame 486\n",
      "Death at frame 709\n",
      "Epoch: 8, frames: 710, score: 210.0, average loss: 0.11386038874748429\n",
      "actions trained:           [ 39. 248.  76. 128. 144.  74.]\n",
      "actions out while training:[ 49. 258.  95. 112. 118.  77.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 189\n",
      "Death at frame 462\n",
      "Death at frame 706\n",
      "Epoch: 9, frames: 707, score: 155.0, average loss: 0.12305435656652064\n",
      "actions trained:           [ 98. 229. 111. 116.  83.  69.]\n",
      "actions out while training:[ 93. 241. 109. 102.  72.  89.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 211\n",
      "Death at frame 1188\n",
      "Death at frame 1556\n",
      "Saving model at epoch 10: ./models/space_invaders_rl\n",
      "Epoch: 10, frames: 1557, score: 265.0, average loss: 0.1533648536788087\n",
      "actions trained:           [202. 705. 134. 131. 179. 205.]\n",
      "actions out while training:[207. 631. 134. 167. 209. 208.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 232\n",
      "Death at frame 560\n",
      "Death at frame 663\n",
      "Epoch: 11, frames: 664, score: 180.0, average loss: 0.12161367357373935\n",
      "actions trained:           [ 40. 152. 135.  82.  96. 158.]\n",
      "actions out while training:[ 44. 159. 132.  98.  86. 144.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 381\n",
      "Death at frame 856\n",
      "Death at frame 1031\n",
      "Epoch: 12, frames: 1032, score: 245.0, average loss: 0.14565592706574373\n",
      "actions trained:           [140. 565. 126.  58.  67.  75.]\n",
      "actions out while training:[165. 497. 150.  69.  75.  75.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 323\n",
      "Death at frame 902\n",
      "Death at frame 1063\n",
      "Epoch: 13, frames: 1064, score: 290.0, average loss: 0.13327846934013926\n",
      "actions trained:           [120. 302. 187. 211. 165.  78.]\n",
      "actions out while training:[106. 307. 187. 205. 173.  85.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 384\n",
      "Death at frame 602\n",
      "Death at frame 715\n",
      "Epoch: 14, frames: 716, score: 180.0, average loss: 0.11241569918250391\n",
      "actions trained:           [ 41. 243.  40. 131. 192.  68.]\n",
      "actions out while training:[ 60. 260.  53. 123. 129.  90.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 202\n",
      "Death at frame 288\n",
      "Death at frame 589\n",
      "Epoch: 15, frames: 590, score: 210.0, average loss: 0.11265918586135411\n",
      "actions trained:           [ 74. 208.  59.  55.  70. 123.]\n",
      "actions out while training:[ 52. 224.  67.  66.  69. 111.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 449\n",
      "Death at frame 676\n",
      "Death at frame 888\n",
      "Epoch: 16, frames: 889, score: 210.0, average loss: 0.12014022382916589\n",
      "actions trained:           [ 52. 296. 123. 187. 121. 109.]\n",
      "actions out while training:[ 52. 343. 111. 155. 110. 117.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 181\n",
      "Death at frame 423\n",
      "Death at frame 709\n",
      "Epoch: 17, frames: 710, score: 210.0, average loss: 0.11938873215169064\n",
      "actions trained:           [ 65. 364. 152.  27.  44.  57.]\n",
      "actions out while training:[ 67. 352. 134.  41.  50.  65.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 478\n",
      "Death at frame 570\n",
      "Death at frame 941\n",
      "Epoch: 18, frames: 942, score: 285.0, average loss: 0.1366388926698424\n",
      "actions trained:           [108. 477.  64.  63.  98. 131.]\n",
      "actions out while training:[116. 464.  72.  66. 104. 119.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 207\n",
      "Death at frame 699\n",
      "Death at frame 978\n",
      "Epoch: 19, frames: 979, score: 240.0, average loss: 0.1302675382667715\n",
      "actions trained:           [250. 354.  89.  56.  79. 150.]\n",
      "actions out while training:[189. 376.  94.  64.  94. 161.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 153\n",
      "Death at frame 390\n",
      "Death at frame 478\n",
      "Saving model at epoch 20: ./models/space_invaders_rl\n",
      "Epoch: 20, frames: 479, score: 145.0, average loss: 0.13063365594694284\n",
      "actions trained:           [  7. 142. 144.  39. 104.  42.]\n",
      "actions out while training:[  6. 143. 130.  61.  92.  46.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 371\n",
      "Death at frame 461\n",
      "Death at frame 923\n",
      "Epoch: 21, frames: 924, score: 215.0, average loss: 0.13297129688965698\n",
      "actions trained:           [122. 480.  85.  97.  73.  66.]\n",
      "actions out while training:[115. 458.  84. 106.  93.  67.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 184\n",
      "Death at frame 271\n",
      "Death at frame 614\n",
      "Epoch: 22, frames: 615, score: 210.0, average loss: 0.11620199054022505\n",
      "actions trained:           [ 37. 235. 100.  88.  45. 109.]\n",
      "actions out while training:[ 31. 244. 106.  82.  52.  99.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 198\n",
      "Death at frame 417\n",
      "Death at frame 717\n",
      "Epoch: 23, frames: 718, score: 210.0, average loss: 0.1224520381627267\n",
      "actions trained:           [ 48. 257.  91. 155. 118.  48.]\n",
      "actions out while training:[ 48. 275.  91. 144. 110.  49.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 312\n",
      "Death at frame 459\n",
      "Death at frame 733\n",
      "Epoch: 24, frames: 734, score: 210.0, average loss: 0.12974034811798343\n",
      "actions trained:           [ 78. 228.  74.  82. 142. 129.]\n",
      "actions out while training:[ 88. 237.  88.  79. 126. 115.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 156\n",
      "Death at frame 309\n",
      "Death at frame 477\n",
      "Epoch: 25, frames: 478, score: 180.0, average loss: 0.1166820234071899\n",
      "actions trained:           [ 39. 235. 139.  28.  24.  12.]\n",
      "actions out while training:[ 42. 234. 125.  30.  33.  13.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 232\n",
      "Death at frame 469\n",
      "Death at frame 737\n",
      "Epoch: 26, frames: 738, score: 210.0, average loss: 0.11853919420586902\n",
      "actions trained:           [139. 240.  93. 175.  60.  30.]\n",
      "actions out while training:[108. 261. 105. 141.  85.  37.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 217\n",
      "Death at frame 307\n",
      "Death at frame 518\n",
      "Epoch: 27, frames: 519, score: 135.0, average loss: 0.10561663181454875\n",
      "actions trained:           [  3. 396.  36.   5.  32.  46.]\n",
      "actions out while training:[  2. 386.  41.   8.  38.  43.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 208\n",
      "Death at frame 296\n",
      "Death at frame 676\n",
      "Epoch: 28, frames: 677, score: 210.0, average loss: 0.1217991434305051\n",
      "actions trained:           [ 62. 322.  48.  41. 111.  92.]\n",
      "actions out while training:[ 68. 320.  51.  43. 104.  90.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 574\n",
      "Death at frame 868\n",
      "Death at frame 1030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, frames: 1031, score: 340.0, average loss: 0.13932423998524066\n",
      "actions trained:           [126. 474.  79.  97. 138. 116.]\n",
      "actions out while training:[134. 461.  76.  97. 146. 116.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 169\n",
      "Death at frame 297\n",
      "Death at frame 620\n",
      "Saving model at epoch 30: ./models/space_invaders_rl\n",
      "Epoch: 30, frames: 621, score: 155.0, average loss: 0.12397126111826456\n",
      "actions trained:           [ 65. 249.  88.  98.  60.  60.]\n",
      "actions out while training:[ 56. 255.  87.  88.  61.  73.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 263\n",
      "Death at frame 766\n",
      "Death at frame 1174\n",
      "Epoch: 31, frames: 1175, score: 440.0, average loss: 0.14375702955413544\n",
      "actions trained:           [124. 570.  83. 121. 190.  86.]\n",
      "actions out while training:[138. 527.  81. 138. 197.  93.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 215\n",
      "Death at frame 301\n",
      "Death at frame 403\n",
      "Epoch: 32, frames: 404, score: 125.0, average loss: 0.11367942419128534\n",
      "actions trained:           [ 40. 185.  29.  23.  20. 106.]\n",
      "actions out while training:[ 33. 182.  29.  33.  36.  90.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 167\n",
      "Death at frame 249\n",
      "Death at frame 344\n",
      "Epoch: 33, frames: 345, score: 80.0, average loss: 0.10427516311844469\n",
      "actions trained:           [ 31. 144.  13.  48.  77.  31.]\n",
      "actions out while training:[ 32. 143.  17.  50.  72.  30.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 349\n",
      "Death at frame 662\n",
      "Death at frame 799\n",
      "Epoch: 34, frames: 800, score: 210.0, average loss: 0.13490168940161695\n",
      "actions trained:           [118. 318.  60. 191.  60.  52.]\n",
      "actions out while training:[ 97. 320.  72. 164.  81.  65.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 319\n",
      "Death at frame 504\n",
      "Death at frame 629\n",
      "Epoch: 35, frames: 630, score: 130.0, average loss: 0.13297002088325502\n",
      "actions trained:           [ 70. 190. 119.  70.  91.  89.]\n",
      "actions out while training:[ 74. 165. 144.  76. 106.  64.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 186\n",
      "Death at frame 283\n",
      "Death at frame 492\n",
      "Epoch: 36, frames: 493, score: 170.0, average loss: 0.11822161065886881\n",
      "actions trained:           [ 12. 172. 130.  29. 102.  47.]\n",
      "actions out while training:[ 25. 174. 112.  37.  97.  47.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 165\n",
      "Death at frame 333\n",
      "Death at frame 719\n",
      "Epoch: 37, frames: 720, score: 210.0, average loss: 0.13316665123243138\n",
      "actions trained:           [101. 183. 113.  98. 156.  68.]\n",
      "actions out while training:[103. 190. 113.  96. 137.  80.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 431\n",
      "Death at frame 1347\n",
      "Death at frame 1505\n",
      "Epoch: 38, frames: 1506, score: 210.0, average loss: 0.14723834355597185\n",
      "actions trained:           [163. 763. 133. 115. 234.  97.]\n",
      "actions out while training:[175. 738. 125. 136. 231. 100.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 275\n",
      "Death at frame 484\n",
      "Death at frame 634\n",
      "Epoch: 39, frames: 635, score: 110.0, average loss: 0.11520501078464347\n",
      "actions trained:           [ 63. 372.  37.  92.  48.  22.]\n",
      "actions out while training:[ 90. 330.  41.  92.  58.  23.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 696\n",
      "Death at frame 833\n",
      "Death at frame 1022\n",
      "Saving model at epoch 40: ./models/space_invaders_rl\n",
      "Epoch: 40, frames: 1023, score: 525.0, average loss: 0.12801021955830574\n",
      "actions trained:           [181. 229. 266. 107.  59. 180.]\n",
      "actions out while training:[155. 243. 261. 110.  65. 188.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 144\n",
      "Death at frame 281\n",
      "Death at frame 682\n",
      "Epoch: 41, frames: 683, score: 155.0, average loss: 0.1260057542413364\n",
      "actions trained:           [ 96. 179.  92. 100.  74. 141.]\n",
      "actions out while training:[ 91. 178. 103. 103.  72. 135.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 196\n",
      "Death at frame 275\n",
      "Death at frame 372\n",
      "Epoch: 42, frames: 373, score: 45.0, average loss: 0.06813427815208002\n",
      "actions trained:           [ 16. 238.  17.  62.  10.  29.]\n",
      "actions out while training:[ 12. 243.  18.  57.  10.  32.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 447\n",
      "Death at frame 650\n",
      "Death at frame 837\n",
      "Epoch: 43, frames: 838, score: 260.0, average loss: 0.1255251818796883\n",
      "actions trained:           [ 33. 547. 130.  43.  32.  52.]\n",
      "actions out while training:[ 57. 496. 137.  54.  40.  53.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 211\n",
      "Death at frame 315\n",
      "Death at frame 434\n",
      "Epoch: 44, frames: 435, score: 80.0, average loss: 0.13890848198114736\n",
      "actions trained:           [ 40. 162. 111.  33.  28.  60.]\n",
      "actions out while training:[ 52. 165. 104.  40.  26.  47.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 226\n",
      "Death at frame 807\n",
      "Death at frame 1007\n",
      "Epoch: 45, frames: 1008, score: 330.0, average loss: 0.13625518612099446\n",
      "actions trained:           [174. 234. 155. 207. 109. 128.]\n",
      "actions out while training:[156. 234. 157. 190. 127. 143.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 144\n",
      "Death at frame 280\n",
      "Death at frame 508\n",
      "Epoch: 46, frames: 509, score: 155.0, average loss: 0.12107257716648746\n",
      "actions trained:           [ 35. 233.  30.  57.  81.  72.]\n",
      "actions out while training:[ 39. 230.  31.  60.  80.  68.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 259\n",
      "Death at frame 384\n",
      "Death at frame 494\n",
      "Epoch: 47, frames: 495, score: 105.0, average loss: 0.1329143322414147\n",
      "actions trained:           [ 73. 235.  29.  98.  21.  38.]\n",
      "actions out while training:[ 74. 228.  35.  92.  24.  41.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 165\n",
      "Death at frame 773\n",
      "Death at frame 996\n",
      "Epoch: 48, frames: 997, score: 450.0, average loss: 0.13639660510099283\n",
      "actions trained:           [246. 155. 119. 130. 156. 190.]\n",
      "actions out while training:[191. 168. 166. 115. 164. 192.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 202\n",
      "Death at frame 286\n",
      "Death at frame 383\n",
      "Epoch: 49, frames: 384, score: 45.0, average loss: 0.0734989006655024\n",
      "actions trained:           [ 25. 242.  32.  29.   4.  51.]\n",
      "actions out while training:[ 19. 257.  24.  32.   5.  46.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 230\n",
      "Death at frame 882\n",
      "Death at frame 987\n",
      "Saving model at epoch 50: ./models/space_invaders_rl\n",
      "Epoch: 50, frames: 988, score: 255.0, average loss: 0.12880430809740942\n",
      "actions trained:           [ 79. 360. 209. 107.  76. 156.]\n",
      "actions out while training:[ 87. 373. 182. 108.  84. 153.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 566\n",
      "Death at frame 756\n",
      "Death at frame 859\n",
      "Epoch: 51, frames: 860, score: 260.0, average loss: 0.13180511978777543\n",
      "actions trained:           [ 54. 226. 265.  28. 176. 110.]\n",
      "actions out while training:[ 67. 232. 238.  34. 173. 115.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 199\n",
      "Death at frame 332\n",
      "Death at frame 906\n",
      "Epoch: 52, frames: 907, score: 230.0, average loss: 0.13614980199679952\n",
      "actions trained:           [102. 569.  49.  52.  58.  76.]\n",
      "actions out while training:[114. 525.  53.  64.  70.  80.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 422\n",
      "Death at frame 591\n",
      "Death at frame 781\n",
      "Epoch: 53, frames: 782, score: 210.0, average loss: 0.1221027390715455\n",
      "actions trained:           [133. 185.  49. 139. 154. 121.]\n",
      "actions out while training:[126. 202.  63. 136. 138. 116.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 550\n",
      "Death at frame 913\n",
      "Death at frame 994\n",
      "Epoch: 54, frames: 995, score: 340.0, average loss: 0.13236127633233127\n",
      "actions trained:           [106. 420. 145. 113. 116.  94.]\n",
      "actions out while training:[103. 416. 146. 112. 122.  95.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 800\n",
      "Death at frame 1085\n",
      "Epoch: 55, frames: 1086, score: 240.0, average loss: 0.13430525621556968\n",
      "actions trained:           [127. 303. 145. 146. 189. 175.]\n",
      "actions out while training:[ 98. 345. 169. 130. 179. 164.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 305\n",
      "Death at frame 463\n",
      "Death at frame 620\n",
      "Epoch: 56, frames: 621, score: 205.0, average loss: 0.13994120085296452\n",
      "actions trained:           [ 67. 295.  74.  36.  64.  84.]\n",
      "actions out while training:[ 77. 289.  70.  39.  62.  83.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 368\n",
      "Death at frame 639\n",
      "Death at frame 807\n",
      "Epoch: 57, frames: 808, score: 210.0, average loss: 0.1439736019225295\n",
      "actions trained:           [ 41. 624.  22.  69.  33.  18.]\n",
      "actions out while training:[ 76. 558.  29.  82.  42.  20.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 263\n",
      "Death at frame 574\n",
      "Death at frame 796\n",
      "Epoch: 58, frames: 797, score: 210.0, average loss: 0.1324763998460559\n",
      "actions trained:           [ 81. 437.  69.  84.  73.  52.]\n",
      "actions out while training:[102. 411.  67.  87.  77.  52.]\n",
      "\n",
      "Using strict AI actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death at frame 237\n",
      "Death at frame 317\n",
      "Death at frame 496\n",
      "Epoch: 59, frames: 497, score: 65.0, average loss: 0.09133874955248297\n",
      "actions trained:           [179. 209.  61.  11.  21.  15.]\n",
      "actions out while training:[130. 218.  75.  20.  36.  17.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 408\n",
      "Death at frame 804\n",
      "Death at frame 953\n",
      "Saving model at epoch 60: ./models/space_invaders_rl\n",
      "Epoch: 60, frames: 954, score: 185.0, average loss: 0.13455074130166383\n",
      "actions trained:           [ 29. 554.  76.  28. 193.  73.]\n",
      "actions out while training:[ 57. 512.  76.  39. 188.  81.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 265\n",
      "Death at frame 601\n",
      "Death at frame 769\n",
      "Epoch: 61, frames: 770, score: 135.0, average loss: 0.13578620646441603\n",
      "actions trained:           [146. 384.  64.  43. 104.  28.]\n",
      "actions out while training:[126. 382.  63.  62. 108.  28.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 219\n",
      "Death at frame 436\n",
      "Death at frame 543\n",
      "Epoch: 62, frames: 544, score: 155.0, average loss: 0.12349084550689317\n",
      "actions trained:           [114. 131.  56. 146.  71.  25.]\n",
      "actions out while training:[ 82. 136.  68. 138.  85.  34.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 194\n",
      "Death at frame 313\n",
      "Death at frame 440\n",
      "Epoch: 63, frames: 441, score: 150.0, average loss: 0.12712837382288733\n",
      "actions trained:           [ 41. 181.  68.  10.  78.  62.]\n",
      "actions out while training:[ 39. 181.  71.  14.  79.  56.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 262\n",
      "Death at frame 699\n",
      "Death at frame 951\n",
      "Epoch: 64, frames: 952, score: 255.0, average loss: 0.12790358217173778\n",
      "actions trained:           [172. 292. 113.  85. 157. 132.]\n",
      "actions out while training:[126. 307. 120.  93. 152. 153.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 308\n",
      "Death at frame 753\n",
      "Death at frame 831\n",
      "Epoch: 65, frames: 832, score: 210.0, average loss: 0.13976035589835134\n",
      "actions trained:           [ 74. 287.  74. 116. 143. 137.]\n",
      "actions out while training:[ 83. 287.  97. 102. 124. 138.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 236\n",
      "Death at frame 349\n",
      "Death at frame 676\n",
      "Epoch: 66, frames: 677, score: 180.0, average loss: 0.12858075030458277\n",
      "actions trained:           [ 31. 152. 103.  40. 134. 216.]\n",
      "actions out while training:[ 46. 171. 113.  42. 112. 192.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 463\n",
      "Death at frame 1000\n",
      "Death at frame 1317\n",
      "Epoch: 67, frames: 1318, score: 480.0, average loss: 0.13360696577322698\n",
      "actions trained:           [147. 421. 311. 195. 127. 116.]\n",
      "actions out while training:[132. 455. 245. 188. 144. 153.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 247\n",
      "Death at frame 673\n",
      "Death at frame 805\n",
      "Epoch: 68, frames: 806, score: 225.0, average loss: 0.1254649952525357\n",
      "actions trained:           [ 51. 237.  82.  35. 153. 247.]\n",
      "actions out while training:[ 62. 249.  99.  40. 138. 217.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 437\n",
      "Death at frame 733\n",
      "Death at frame 1051\n",
      "Epoch: 69, frames: 1052, score: 490.0, average loss: 0.13589457134871868\n",
      "actions trained:           [115. 331. 144. 107. 196. 158.]\n",
      "actions out while training:[127. 347. 176. 102. 149. 150.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 340\n",
      "Death at frame 753\n",
      "Death at frame 869\n",
      "Saving model at epoch 70: ./models/space_invaders_rl\n",
      "Epoch: 70, frames: 870, score: 210.0, average loss: 0.1303928512421954\n",
      "actions trained:           [ 54. 635.  82.  56.   8.  34.]\n",
      "actions out while training:[ 63. 566.  99.  74.  21.  46.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 223\n",
      "Death at frame 326\n",
      "Death at frame 477\n",
      "Epoch: 71, frames: 478, score: 90.0, average loss: 0.1107907489610249\n",
      "actions trained:           [ 27. 262.  52.  60.  63.  13.]\n",
      "actions out while training:[ 29. 256.  51.  59.  67.  15.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 507\n",
      "Death at frame 634\n",
      "Death at frame 904\n",
      "Epoch: 72, frames: 905, score: 230.0, average loss: 0.1294712347799108\n",
      "actions trained:           [181. 404. 139.  60.  59.  61.]\n",
      "actions out while training:[161. 389. 123.  78.  84.  69.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 581\n",
      "Death at frame 1039\n",
      "Death at frame 1392\n",
      "Epoch: 73, frames: 1393, score: 620.0, average loss: 0.1386705394115724\n",
      "actions trained:           [205. 553. 234. 153. 162.  85.]\n",
      "actions out while training:[220. 540. 240. 140. 154.  98.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 434\n",
      "Death at frame 514\n",
      "Death at frame 860\n",
      "Epoch: 74, frames: 861, score: 330.0, average loss: 0.12126646530357042\n",
      "actions trained:           [139. 219. 158.  74. 110. 160.]\n",
      "actions out while training:[120. 230. 157.  73. 122. 158.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 417\n",
      "Death at frame 644\n",
      "Death at frame 877\n",
      "Epoch: 75, frames: 878, score: 285.0, average loss: 0.14329110381900967\n",
      "actions trained:           [ 79. 445.  97.  51. 140.  65.]\n",
      "actions out while training:[ 94. 421. 104.  60. 122.  76.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 219\n",
      "Death at frame 482\n",
      "Death at frame 690\n",
      "Epoch: 76, frames: 691, score: 270.0, average loss: 0.13051016153149841\n",
      "actions trained:           [ 53. 293. 107.  77. 111.  49.]\n",
      "actions out while training:[ 52. 294. 103.  93. 106.  42.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 166\n",
      "Death at frame 287\n",
      "Death at frame 760\n",
      "Epoch: 77, frames: 761, score: 240.0, average loss: 0.13479498101282578\n",
      "actions trained:           [105. 296.  82. 110.  75.  92.]\n",
      "actions out while training:[116. 287.  82. 100.  83.  92.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 396\n",
      "Death at frame 755\n",
      "Death at frame 881\n",
      "Epoch: 78, frames: 882, score: 215.0, average loss: 0.11607791808374142\n",
      "actions trained:           [109. 340.  71.  71. 130. 160.]\n",
      "actions out while training:[ 73. 403.  77.  86. 115. 127.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 266\n",
      "Death at frame 624\n",
      "Death at frame 944\n",
      "Epoch: 79, frames: 945, score: 155.0, average loss: 0.14004179682000875\n",
      "actions trained:           [ 44. 706.  77.  45.  36.  36.]\n",
      "actions out while training:[ 88. 568.  96.  83.  64.  45.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 168\n",
      "Death at frame 312\n",
      "Death at frame 460\n",
      "Saving model at epoch 80: ./models/space_invaders_rl\n",
      "Epoch: 80, frames: 461, score: 180.0, average loss: 0.11456586382815348\n",
      "actions trained:           [ 78. 179. 139.  32.  17.  15.]\n",
      "actions out while training:[ 52. 185. 133.  49.  25.  16.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 231\n",
      "Death at frame 841\n",
      "Death at frame 1315\n",
      "Epoch: 81, frames: 1316, score: 420.0, average loss: 0.13533112798887284\n",
      "actions trained:           [229. 361. 115. 194. 207. 209.]\n",
      "actions out while training:[224. 373. 122. 191. 202. 203.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 239\n",
      "Death at frame 541\n",
      "Death at frame 673\n",
      "Epoch: 82, frames: 674, score: 155.0, average loss: 0.11380719698572063\n",
      "actions trained:           [ 76. 141.  80. 147. 147.  82.]\n",
      "actions out while training:[ 70. 158.  81. 141. 121. 102.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 249\n",
      "Death at frame 783\n",
      "Death at frame 933\n",
      "Epoch: 83, frames: 934, score: 410.0, average loss: 0.12959910502367275\n",
      "actions trained:           [172. 212.  67. 216. 151. 115.]\n",
      "actions out while training:[129. 273. 110. 135. 140. 146.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 196\n",
      "Death at frame 323\n",
      "Death at frame 476\n",
      "Epoch: 84, frames: 477, score: 90.0, average loss: 0.1332529286875382\n",
      "actions trained:           [ 31. 364.  32.  26.   8.  15.]\n",
      "actions out while training:[ 38. 332.  35.  27.  20.  24.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 392\n",
      "Death at frame 746\n",
      "Death at frame 992\n",
      "Epoch: 85, frames: 993, score: 290.0, average loss: 0.13127664908356973\n",
      "actions trained:           [ 82. 316. 105. 129. 198. 162.]\n",
      "actions out while training:[ 78. 343. 109. 124. 174. 164.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 192\n",
      "Death at frame 276\n",
      "Death at frame 669\n",
      "Epoch: 86, frames: 670, score: 180.0, average loss: 0.15446219843391054\n",
      "actions trained:           [ 35. 527.  39.  14.  15.  39.]\n",
      "actions out while training:[ 51. 468.  62.  23.  22.  43.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 178\n",
      "Death at frame 283\n",
      "Death at frame 672\n",
      "Epoch: 87, frames: 673, score: 210.0, average loss: 0.12114567823001066\n",
      "actions trained:           [ 62. 371. 121.  39.  30.  49.]\n",
      "actions out while training:[ 55. 370. 102.  48.  38.  59.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 230\n",
      "Death at frame 676\n",
      "Death at frame 871\n",
      "Epoch: 88, frames: 872, score: 240.0, average loss: 0.1362508288158223\n",
      "actions trained:           [179. 144.  89.  84. 168. 207.]\n",
      "actions out while training:[165. 155.  98.  89. 165. 199.]\n",
      "\n",
      "Using strict AI actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death at frame 378\n",
      "Death at frame 728\n",
      "Death at frame 1029\n",
      "Epoch: 89, frames: 1030, score: 265.0, average loss: 0.1354535523417426\n",
      "actions trained:           [ 72. 539. 142. 100. 119.  57.]\n",
      "actions out while training:[ 88. 518. 139. 110. 110.  64.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 739\n",
      "Death at frame 819\n",
      "Death at frame 1086\n",
      "Saving model at epoch 90: ./models/space_invaders_rl\n",
      "Epoch: 90, frames: 1087, score: 390.0, average loss: 0.13470495034007104\n",
      "actions trained:           [118. 474.  97. 100.  99. 198.]\n",
      "actions out while training:[117. 478.  98. 110. 109. 174.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 694\n",
      "Death at frame 787\n",
      "Death at frame 942\n",
      "Epoch: 91, frames: 943, score: 285.0, average loss: 0.13375661125141983\n",
      "actions trained:           [ 87. 412. 113. 146.  65. 119.]\n",
      "actions out while training:[ 94. 405. 123. 139.  65. 116.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 162\n",
      "Death at frame 252\n",
      "Death at frame 772\n",
      "Epoch: 92, frames: 773, score: 215.0, average loss: 0.1321591038420747\n",
      "actions trained:           [133. 146. 111.  93. 151. 138.]\n",
      "actions out while training:[114. 160. 135.  81. 139. 143.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 725\n",
      "Death at frame 831\n",
      "Death at frame 1077\n",
      "Epoch: 93, frames: 1078, score: 605.0, average loss: 0.1352305629019911\n",
      "actions trained:           [130. 275. 213. 173. 189.  97.]\n",
      "actions out while training:[125. 286. 215. 165. 188.  98.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 182\n",
      "Death at frame 819\n",
      "Death at frame 1421\n",
      "Epoch: 94, frames: 1422, score: 650.0, average loss: 0.13580585281832439\n",
      "actions trained:           [166. 658. 189. 139.  93. 176.]\n",
      "actions out while training:[180. 602. 215. 135. 109. 180.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 168\n",
      "Death at frame 292\n",
      "Death at frame 431\n",
      "Epoch: 95, frames: 432, score: 100.0, average loss: 0.1335804477423299\n",
      "actions trained:           [ 16. 260. 108.   1.  17.  29.]\n",
      "actions out while training:[ 25. 250.  97.   6.  20.  33.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 166\n",
      "Death at frame 300\n",
      "Death at frame 464\n",
      "Epoch: 96, frames: 465, score: 180.0, average loss: 0.13115184168584815\n",
      "actions trained:           [ 41. 148.  58.  32.  50. 135.]\n",
      "actions out while training:[ 48. 146.  65.  34.  50. 121.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 178\n",
      "Death at frame 292\n",
      "Death at frame 458\n",
      "Epoch: 97, frames: 459, score: 100.0, average loss: 0.1114812726410484\n",
      "actions trained:           [ 27. 186. 148.   7.  84.   6.]\n",
      "actions out while training:[ 25. 188. 145.  17.  75.   8.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 159\n",
      "Death at frame 649\n",
      "Death at frame 1153\n",
      "Epoch: 98, frames: 1154, score: 240.0, average loss: 0.14081621285639315\n",
      "actions trained:           [271. 266. 304. 209.  46.  57.]\n",
      "actions out while training:[228. 289. 301. 187.  68.  80.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 540\n",
      "Death at frame 654\n",
      "Death at frame 914\n",
      "Epoch: 99, frames: 915, score: 245.0, average loss: 0.13769226587217287\n",
      "actions trained:           [ 43. 642.  99.  45.  67.  18.]\n",
      "actions out while training:[ 49. 596.  89.  63.  94.  23.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 207\n",
      "Death at frame 357\n",
      "Death at frame 465\n",
      "Saving model at epoch 100: ./models/space_invaders_rl\n",
      "Epoch: 100, frames: 466, score: 135.0, average loss: 0.13758210912295346\n",
      "actions trained:           [ 40. 263.  31.  27.  43.  61.]\n",
      "actions out while training:[ 62. 256.  29.  27.  47.  44.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 464\n",
      "Death at frame 840\n",
      "Death at frame 1025\n",
      "Epoch: 101, frames: 1026, score: 300.0, average loss: 0.1277534905898774\n",
      "actions trained:           [159. 218. 205. 243.  93. 107.]\n",
      "actions out while training:[100. 244. 233. 199. 119. 130.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 242\n",
      "Death at frame 329\n",
      "Death at frame 438\n",
      "Epoch: 102, frames: 439, score: 35.0, average loss: 0.08580382963432584\n",
      "actions trained:           [ 33. 240.  38.  27.  75.  25.]\n",
      "actions out while training:[ 24. 243.  38.  32.  78.  23.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 174\n",
      "Death at frame 320\n",
      "Death at frame 842\n",
      "Epoch: 103, frames: 843, score: 280.0, average loss: 0.12615101749341467\n",
      "actions trained:           [ 70. 379.  77.  59. 160.  97.]\n",
      "actions out while training:[ 72. 364.  92.  69. 147.  98.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 300\n",
      "Death at frame 397\n",
      "Death at frame 608\n",
      "Epoch: 104, frames: 609, score: 180.0, average loss: 0.128287166600933\n",
      "actions trained:           [ 21. 347.  36.  83.  80.  41.]\n",
      "actions out while training:[ 35. 334.  38.  88.  78.  35.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 208\n",
      "Death at frame 321\n",
      "Death at frame 641\n",
      "Epoch: 105, frames: 642, score: 210.0, average loss: 0.11161886306945983\n",
      "actions trained:           [109. 170. 136.  66.  54. 106.]\n",
      "actions out while training:[ 83. 175. 141.  76.  58. 108.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 263\n",
      "Death at frame 969\n",
      "Death at frame 1180\n",
      "Epoch: 106, frames: 1181, score: 355.0, average loss: 0.13840644629995874\n",
      "actions trained:           [ 48. 743.  98.  87. 135.  69.]\n",
      "actions out while training:[ 91. 638. 122.  92. 170.  67.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 244\n",
      "Death at frame 405\n",
      "Death at frame 539\n",
      "Epoch: 107, frames: 540, score: 120.0, average loss: 0.11935568431418009\n",
      "actions trained:           [155. 169.  35.  92.  51.  37.]\n",
      "actions out while training:[ 79. 181.  41. 112.  79.  47.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 239\n",
      "Death at frame 330\n",
      "Death at frame 502\n",
      "Epoch: 108, frames: 503, score: 80.0, average loss: 0.10024826675800458\n",
      "actions trained:           [ 20. 155.  62.  37. 138.  90.]\n",
      "actions out while training:[ 22. 158.  61.  44. 122.  95.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 424\n",
      "Death at frame 589\n",
      "Death at frame 699\n",
      "Epoch: 109, frames: 700, score: 195.0, average loss: 0.1277876118409304\n",
      "actions trained:           [118. 294.  26.  67. 101.  93.]\n",
      "actions out while training:[102. 298.  27.  69. 109.  94.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 210\n",
      "Death at frame 294\n",
      "Death at frame 398\n",
      "Saving model at epoch 110: ./models/space_invaders_rl\n",
      "Epoch: 110, frames: 399, score: 60.0, average loss: 0.0732921647949142\n",
      "actions trained:           [  9. 173.  53.  47.  77.  39.]\n",
      "actions out while training:[  9. 177.  60.  42.  73.  37.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 215\n",
      "Death at frame 408\n",
      "Death at frame 573\n",
      "Epoch: 111, frames: 574, score: 155.0, average loss: 0.12061840479068836\n",
      "actions trained:           [ 17. 231.  38.  43. 103. 141.]\n",
      "actions out while training:[ 24. 238.  43.  53. 101. 114.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 455\n",
      "Death at frame 764\n",
      "Death at frame 864\n",
      "Epoch: 112, frames: 865, score: 440.0, average loss: 0.12400255831740885\n",
      "actions trained:           [125. 307.  71. 149. 133.  79.]\n",
      "actions out while training:[107. 320.  98. 124. 132.  83.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 211\n",
      "Death at frame 480\n",
      "Death at frame 848\n",
      "Epoch: 113, frames: 849, score: 270.0, average loss: 0.13949237876759266\n",
      "actions trained:           [ 62. 235. 245.  28.  90. 188.]\n",
      "actions out while training:[ 66. 243. 225.  35.  86. 193.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 250\n",
      "Death at frame 400\n",
      "Death at frame 530\n",
      "Epoch: 114, frames: 531, score: 135.0, average loss: 0.11521420366798737\n",
      "actions trained:           [ 32. 205.  63. 149.  54.  27.]\n",
      "actions out while training:[ 30. 213.  72. 122.  56.  37.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 196\n",
      "Death at frame 276\n",
      "Death at frame 372\n",
      "Epoch: 115, frames: 373, score: 45.0, average loss: 0.06805236503383458\n",
      "actions trained:           [  8. 279.  36.  32.   8.   9.]\n",
      "actions out while training:[  9. 277.  35.  33.   9.   9.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 278\n",
      "Death at frame 586\n",
      "Death at frame 1046\n",
      "Epoch: 116, frames: 1047, score: 265.0, average loss: 0.14485532824707148\n",
      "actions trained:           [115. 480. 100.  74. 180.  97.]\n",
      "actions out while training:[129. 457.  88.  91. 182.  99.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 592\n",
      "Death at frame 1057\n",
      "Death at frame 1199\n",
      "Epoch: 117, frames: 1200, score: 715.0, average loss: 0.13630224025273183\n",
      "actions trained:           [178. 328. 231. 174. 126. 162.]\n",
      "actions out while training:[165. 339. 246. 164. 124. 161.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 246\n",
      "Death at frame 641\n",
      "Death at frame 823\n",
      "Epoch: 118, frames: 824, score: 270.0, average loss: 0.1302814987422819\n",
      "actions trained:           [101. 273.  92. 100. 178.  79.]\n",
      "actions out while training:[105. 283.  88.  96. 159.  92.]\n",
      "\n",
      "Using strict AI actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death at frame 267\n",
      "Death at frame 466\n",
      "Death at frame 811\n",
      "Epoch: 119, frames: 812, score: 290.0, average loss: 0.133941110823817\n",
      "actions trained:           [111. 345. 140.  56.  69.  90.]\n",
      "actions out while training:[104. 334. 154.  65.  70.  84.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 174\n",
      "Death at frame 317\n",
      "Death at frame 747\n",
      "Saving model at epoch 120: ./models/space_invaders_rl\n",
      "Epoch: 120, frames: 748, score: 210.0, average loss: 0.12290250064522282\n",
      "actions trained:           [ 85. 433.  47.  29.  62.  91.]\n",
      "actions out while training:[ 65. 428.  63.  42.  75.  74.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 386\n",
      "Death at frame 1053\n",
      "Death at frame 1135\n",
      "Epoch: 121, frames: 1136, score: 315.0, average loss: 0.1241256126873117\n",
      "actions trained:           [110. 371. 154. 227. 115. 158.]\n",
      "actions out while training:[101. 393. 161. 205. 112. 163.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 321\n",
      "Death at frame 679\n",
      "Death at frame 990\n",
      "Epoch: 122, frames: 991, score: 305.0, average loss: 0.1381154704275069\n",
      "actions trained:           [ 47. 535. 146.  50.  94. 118.]\n",
      "actions out while training:[ 60. 495. 156.  59. 108. 112.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 180\n",
      "Death at frame 737\n",
      "Death at frame 896\n",
      "Epoch: 123, frames: 897, score: 550.0, average loss: 0.1291988952755076\n",
      "actions trained:           [136. 244. 149.  88. 164. 115.]\n",
      "actions out while training:[117. 256. 185.  87. 146. 105.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 182\n",
      "Death at frame 296\n",
      "Death at frame 623\n",
      "Epoch: 124, frames: 624, score: 225.0, average loss: 0.1291906783276193\n",
      "actions trained:           [ 80. 212. 188.  23.  81.  39.]\n",
      "actions out while training:[ 78. 209. 188.  26.  82.  40.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 166\n",
      "Death at frame 581\n",
      "Death at frame 715\n",
      "Epoch: 125, frames: 716, score: 235.0, average loss: 0.1385741629659652\n",
      "actions trained:           [ 58. 109. 220.  51. 156. 121.]\n",
      "actions out while training:[ 71. 114. 201.  66. 136. 127.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 358\n",
      "Death at frame 775\n",
      "Death at frame 946\n",
      "Epoch: 126, frames: 947, score: 290.0, average loss: 0.13928895991713533\n",
      "actions trained:           [182. 489. 116.  78.  43.  38.]\n",
      "actions out while training:[168. 472. 118.  88.  54.  46.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 464\n",
      "Death at frame 669\n",
      "Death at frame 866\n",
      "Epoch: 127, frames: 867, score: 240.0, average loss: 0.12624360622170774\n",
      "actions trained:           [ 50. 222. 209.  57.  85. 243.]\n",
      "actions out while training:[ 55. 240. 203.  72.  84. 212.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 532\n",
      "Death at frame 911\n",
      "Death at frame 1101\n",
      "Epoch: 128, frames: 1102, score: 340.0, average loss: 0.13160445486709152\n",
      "actions trained:           [181. 393. 222.  85. 100. 120.]\n",
      "actions out while training:[180. 398. 189.  94. 116. 124.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 168\n",
      "Death at frame 793\n",
      "Death at frame 1037\n",
      "Epoch: 129, frames: 1038, score: 250.0, average loss: 0.13817250560656053\n",
      "actions trained:           [107. 266. 259. 139. 116. 150.]\n",
      "actions out while training:[105. 262. 253. 136. 130. 151.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 412\n",
      "Death at frame 568\n",
      "Death at frame 745\n",
      "Saving model at epoch 130: ./models/space_invaders_rl\n",
      "Epoch: 130, frames: 746, score: 170.0, average loss: 0.12153723054890413\n",
      "actions trained:           [ 38. 454.  40.  86.  79.  48.]\n",
      "actions out while training:[ 45. 409.  43.  94.  95.  59.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 256\n",
      "Death at frame 476\n",
      "Death at frame 719\n",
      "Epoch: 131, frames: 720, score: 185.0, average loss: 0.12187840712333445\n",
      "actions trained:           [ 52. 339.  41. 136.  94.  57.]\n",
      "actions out while training:[ 62. 346.  44. 118.  88.  61.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 530\n",
      "Death at frame 736\n",
      "Death at frame 830\n",
      "Epoch: 132, frames: 831, score: 300.0, average loss: 0.1293793595743319\n",
      "actions trained:           [ 67. 245. 178.  66. 143. 131.]\n",
      "actions out while training:[ 67. 249. 181.  75. 140. 118.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 582\n",
      "Death at frame 1106\n",
      "Death at frame 1592\n",
      "Epoch: 133, frames: 1593, score: 725.0, average loss: 0.15007165448190704\n",
      "actions trained:           [129. 386. 259. 133. 176. 509.]\n",
      "actions out while training:[154. 417. 262. 145. 168. 446.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 371\n",
      "Death at frame 1167\n",
      "Death at frame 1430\n",
      "Epoch: 134, frames: 1431, score: 265.0, average loss: 0.14485678683339187\n",
      "actions trained:           [194. 692. 156. 184. 144.  60.]\n",
      "actions out while training:[186. 664. 148. 207. 155.  70.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 231\n",
      "Death at frame 560\n",
      "Death at frame 664\n",
      "Epoch: 135, frames: 665, score: 170.0, average loss: 0.12089484555098039\n",
      "actions trained:           [ 60. 324.  93.  29. 114.  44.]\n",
      "actions out while training:[ 63. 312.  90.  39. 109.  51.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 287\n",
      "Death at frame 508\n",
      "Death at frame 615\n",
      "Epoch: 136, frames: 616, score: 155.0, average loss: 0.1227909182809659\n",
      "actions trained:           [104. 316.  52.  47.  63.  33.]\n",
      "actions out while training:[ 90. 309.  51.  52.  74.  39.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 553\n",
      "Death at frame 1017\n",
      "Death at frame 1131\n",
      "Epoch: 137, frames: 1132, score: 415.0, average loss: 0.1327356541111304\n",
      "actions trained:           [184. 512.  80. 110. 199.  46.]\n",
      "actions out while training:[162. 520.  98. 110. 173.  68.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 264\n",
      "Death at frame 581\n",
      "Death at frame 686\n",
      "Epoch: 138, frames: 687, score: 180.0, average loss: 0.11887950737893754\n",
      "actions trained:           [ 31. 352.  46. 145.  53.  59.]\n",
      "actions out while training:[ 36. 342.  53. 130.  61.  64.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 223\n",
      "Death at frame 311\n",
      "Death at frame 607\n",
      "Epoch: 139, frames: 608, score: 155.0, average loss: 0.11091874626724625\n",
      "actions trained:           [ 38. 281. 135.  91.  28.  34.]\n",
      "actions out while training:[ 40. 273. 131.  91.  32.  40.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 548\n",
      "Death at frame 857\n",
      "Death at frame 1063\n",
      "Saving model at epoch 140: ./models/space_invaders_rl\n",
      "Epoch: 140, frames: 1064, score: 415.0, average loss: 0.13513834789813675\n",
      "actions trained:           [140. 273. 225. 135. 161. 129.]\n",
      "actions out while training:[132. 279. 216. 144. 161. 131.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 542\n",
      "Death at frame 690\n",
      "Death at frame 976\n",
      "Epoch: 141, frames: 977, score: 380.0, average loss: 0.13342488762951496\n",
      "actions trained:           [ 50. 259. 190.  58. 214. 205.]\n",
      "actions out while training:[ 57. 278. 189.  67. 174. 211.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 201\n",
      "Death at frame 283\n",
      "Death at frame 376\n",
      "Epoch: 142, frames: 377, score: 55.0, average loss: 0.08170037398599603\n",
      "actions trained:           [ 12. 337.   8.   8.   3.   8.]\n",
      "actions out while training:[ 11. 328.  11.  10.   5.  11.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 201\n",
      "Death at frame 285\n",
      "Death at frame 378\n",
      "Epoch: 143, frames: 379, score: 45.0, average loss: 0.06556079651041737\n",
      "actions trained:           [ 23. 281.  11.  33.   5.  25.]\n",
      "actions out while training:[ 21. 279.  12.  37.   5.  24.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 202\n",
      "Death at frame 288\n",
      "Death at frame 384\n",
      "Epoch: 144, frames: 385, score: 45.0, average loss: 0.0675488595805351\n",
      "actions trained:           [ 20. 287.  10.  40.   4.  23.]\n",
      "actions out while training:[ 20. 283.  12.  40.   4.  25.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 263\n",
      "Death at frame 641\n",
      "Death at frame 736\n",
      "Epoch: 145, frames: 737, score: 195.0, average loss: 0.11806561582711146\n",
      "actions trained:           [ 25. 359. 224.  12.  36.  80.]\n",
      "actions out while training:[ 17. 384. 159.  29.  35. 112.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 376\n",
      "Death at frame 840\n",
      "Death at frame 941\n",
      "Epoch: 146, frames: 942, score: 460.0, average loss: 0.12478200048452921\n",
      "actions trained:           [ 86. 573.  94.  66.  57.  65.]\n",
      "actions out while training:[121. 503.  91.  94.  70.  62.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 223\n",
      "Death at frame 599\n",
      "Death at frame 771\n",
      "Epoch: 147, frames: 772, score: 210.0, average loss: 0.12675483210183144\n",
      "actions trained:           [175. 418.  62.  29.  40.  47.]\n",
      "actions out while training:[127. 419.  65.  43.  48.  69.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 370\n",
      "Death at frame 1021\n",
      "Death at frame 1106\n",
      "Epoch: 148, frames: 1107, score: 415.0, average loss: 0.1438272727429358\n",
      "actions trained:           [111. 736.  63.  50. 120.  26.]\n",
      "actions out while training:[151. 674.  65.  83. 107.  26.]\n",
      "\n",
      "Using strict AI actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death at frame 170\n",
      "Death at frame 327\n",
      "Death at frame 813\n",
      "Epoch: 149, frames: 814, score: 210.0, average loss: 0.12947479230124076\n",
      "actions trained:           [ 78. 270.  65. 125. 183.  92.]\n",
      "actions out while training:[ 77. 279.  65. 118. 173. 101.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 324\n",
      "Death at frame 461\n",
      "Death at frame 635\n",
      "Saving model at epoch 150: ./models/space_invaders_rl\n",
      "Epoch: 150, frames: 636, score: 150.0, average loss: 0.12327079642868913\n",
      "actions trained:           [ 34. 397.  57.  71.  41.  35.]\n",
      "actions out while training:[ 35. 372.  66.  74.  51.  37.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 463\n",
      "Death at frame 613\n",
      "Death at frame 804\n",
      "Epoch: 151, frames: 805, score: 320.0, average loss: 0.13865674182807372\n",
      "actions trained:           [ 78. 283. 129.  50. 135. 129.]\n",
      "actions out while training:[ 95. 282. 127.  50. 137. 113.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 167\n",
      "Death at frame 324\n",
      "Death at frame 526\n",
      "Epoch: 152, frames: 527, score: 105.0, average loss: 0.11867570363809904\n",
      "actions trained:           [108. 175.  43. 123.  48.  29.]\n",
      "actions out while training:[ 86. 176.  45. 113.  63.  43.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 162\n",
      "Death at frame 714\n",
      "Death at frame 863\n",
      "Epoch: 153, frames: 864, score: 210.0, average loss: 0.1381354125554671\n",
      "actions trained:           [ 55.  98.  85. 125. 265. 235.]\n",
      "actions out while training:[ 76. 109. 105. 116. 227. 230.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 519\n",
      "Death at frame 643\n",
      "Death at frame 964\n",
      "Epoch: 154, frames: 965, score: 345.0, average loss: 0.1282686686337403\n",
      "actions trained:           [163. 195. 127.  63. 136. 280.]\n",
      "actions out while training:[150. 208. 158.  72. 138. 238.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 164\n",
      "Death at frame 630\n",
      "Death at frame 804\n",
      "Epoch: 155, frames: 805, score: 200.0, average loss: 0.13953386843841617\n",
      "actions trained:           [ 81. 302. 123.  86. 135.  77.]\n",
      "actions out while training:[ 81. 297. 125.  81. 140.  80.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 201\n",
      "Death at frame 397\n",
      "Death at frame 593\n",
      "Epoch: 156, frames: 594, score: 180.0, average loss: 0.11855116698599624\n",
      "actions trained:           [ 21. 294.  94.  83.  44.  57.]\n",
      "actions out while training:[ 36. 291.  81.  86.  45.  54.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 242\n",
      "Death at frame 640\n",
      "Death at frame 1038\n",
      "Epoch: 157, frames: 1039, score: 235.0, average loss: 0.13328776089021516\n",
      "actions trained:           [ 67. 615.  49. 104. 101. 102.]\n",
      "actions out while training:[ 96. 589.  55. 112. 105.  81.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 171\n",
      "Death at frame 281\n",
      "Death at frame 542\n",
      "Epoch: 158, frames: 543, score: 225.0, average loss: 0.13449230453475544\n",
      "actions trained:           [ 76. 270.  41.  85.  49.  21.]\n",
      "actions out while training:[ 61. 265.  44.  94.  55.  23.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 245\n",
      "Death at frame 810\n",
      "Death at frame 1007\n",
      "Epoch: 159, frames: 1008, score: 245.0, average loss: 0.12476317574583227\n",
      "actions trained:           [142. 298.  97. 159. 179. 132.]\n",
      "actions out while training:[117. 328. 124. 146. 155. 137.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 251\n",
      "Death at frame 436\n",
      "Death at frame 939\n",
      "Saving model at epoch 160: ./models/space_invaders_rl\n",
      "Epoch: 160, frames: 940, score: 440.0, average loss: 0.1323081027493983\n",
      "actions trained:           [168. 530.  78.  48.  61.  54.]\n",
      "actions out while training:[170. 514.  73.  60.  59.  63.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 472\n",
      "Death at frame 672\n",
      "Death at frame 815\n",
      "Epoch: 161, frames: 816, score: 210.0, average loss: 0.12454908569424523\n",
      "actions trained:           [ 57. 303. 139.  61.  67. 188.]\n",
      "actions out while training:[ 65. 311. 144.  60.  70. 165.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 264\n",
      "Death at frame 409\n",
      "Death at frame 520\n",
      "Epoch: 162, frames: 521, score: 130.0, average loss: 0.13170219928895255\n",
      "actions trained:           [ 61. 300.  59.  50.  30.  20.]\n",
      "actions out while training:[ 58. 292.  66.  51.  35.  18.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 162\n",
      "Death at frame 686\n",
      "Death at frame 807\n",
      "Epoch: 163, frames: 808, score: 215.0, average loss: 0.13278364059582512\n",
      "actions trained:           [ 98. 139. 140. 105. 201. 124.]\n",
      "actions out while training:[101. 147. 153. 100. 168. 138.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 180\n",
      "Death at frame 328\n",
      "Death at frame 506\n",
      "Epoch: 164, frames: 507, score: 155.0, average loss: 0.11293735291421633\n",
      "actions trained:           [ 43. 177. 157.  23.  26.  80.]\n",
      "actions out while training:[ 42. 177. 161.  25.  23.  78.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 505\n",
      "Death at frame 642\n",
      "Death at frame 800\n",
      "Epoch: 165, frames: 801, score: 410.0, average loss: 0.1265314376292973\n",
      "actions trained:           [104. 237. 211.  92.   7. 149.]\n",
      "actions out while training:[ 76. 258. 194. 100.  13. 159.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 153\n",
      "Death at frame 437\n",
      "Death at frame 644\n",
      "Epoch: 166, frames: 645, score: 220.0, average loss: 0.1388199663995356\n",
      "actions trained:           [ 18. 155. 183.  51.  84. 153.]\n",
      "actions out while training:[ 20. 166. 181.  58.  84. 135.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 911\n",
      "Epoch: 167, frames: 912, score: 255.0, average loss: 0.1445608676036035\n",
      "actions trained:           [ 52. 571.  94.  69.  67.  58.]\n",
      "actions out while training:[ 62. 535. 109.  74.  71.  60.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 149\n",
      "Death at frame 403\n",
      "Death at frame 645\n",
      "Epoch: 168, frames: 646, score: 165.0, average loss: 0.14331011892909928\n",
      "actions trained:           [ 24. 199. 210.  11.  65. 136.]\n",
      "actions out while training:[ 29. 195. 216.  14.  65. 126.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 262\n",
      "Death at frame 479\n",
      "Death at frame 715\n",
      "Epoch: 169, frames: 716, score: 210.0, average loss: 0.122531138750361\n",
      "actions trained:           [140. 264. 121. 109.  40.  41.]\n",
      "actions out while training:[ 88. 280. 125.  95.  73.  54.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 392\n",
      "Death at frame 589\n",
      "Death at frame 698\n",
      "Saving model at epoch 170: ./models/space_invaders_rl\n",
      "Epoch: 170, frames: 699, score: 215.0, average loss: 0.1266332329323694\n",
      "actions trained:           [ 11. 436.  61.  77.  95.  18.]\n",
      "actions out while training:[ 11. 432.  60.  80.  97.  18.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 233\n",
      "Death at frame 467\n",
      "Death at frame 624\n",
      "Epoch: 171, frames: 625, score: 195.0, average loss: 0.1212108561057593\n",
      "actions trained:           [ 59. 318.  93.  25.  54.  75.]\n",
      "actions out while training:[ 58. 323.  95.  29.  57.  62.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 147\n",
      "Death at frame 274\n",
      "Death at frame 761\n",
      "Epoch: 172, frames: 762, score: 240.0, average loss: 0.13456192679748885\n",
      "actions trained:           [ 37. 371. 213.  74.  47.  19.]\n",
      "actions out while training:[ 44. 378. 177.  84.  52.  26.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 568\n",
      "Death at frame 746\n",
      "Death at frame 992\n",
      "Epoch: 173, frames: 993, score: 305.0, average loss: 0.13091063777566836\n",
      "actions trained:           [156. 243. 180. 130. 208.  75.]\n",
      "actions out while training:[128. 264. 187. 123. 207.  83.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 142\n",
      "Death at frame 482\n",
      "Death at frame 934\n",
      "Epoch: 174, frames: 935, score: 315.0, average loss: 0.14650574273779202\n",
      "actions trained:           [101. 222. 236.  82. 182. 111.]\n",
      "actions out while training:[107. 221. 236.  84. 174. 112.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 416\n",
      "Death at frame 833\n",
      "Death at frame 935\n",
      "Epoch: 175, frames: 936, score: 485.0, average loss: 0.12741125114587762\n",
      "actions trained:           [120. 417. 156. 119.  39.  84.]\n",
      "actions out while training:[101. 428. 153. 118.  45.  90.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 855\n",
      "Death at frame 932\n",
      "Death at frame 1177\n",
      "Epoch: 176, frames: 1178, score: 395.0, average loss: 0.13623129679357251\n",
      "actions trained:           [116. 538. 216.  51. 131. 125.]\n",
      "actions out while training:[131. 552. 181.  58. 126. 129.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 437\n",
      "Death at frame 567\n",
      "Death at frame 728\n",
      "Epoch: 177, frames: 729, score: 255.0, average loss: 0.12637896892555087\n",
      "actions trained:           [ 42. 453.  62.   7.  99.  65.]\n",
      "actions out while training:[ 67. 431.  71.   8.  90.  61.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 198\n",
      "Death at frame 360\n",
      "Death at frame 518\n",
      "Epoch: 178, frames: 519, score: 110.0, average loss: 0.16000678932486304\n",
      "actions trained:           [ 74. 315.  45.  62.  15.   7.]\n",
      "actions out while training:[ 63. 307.  49.  68.  22.   9.]\n",
      "\n",
      "Using strict AI actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death at frame 426\n",
      "Death at frame 704\n",
      "Death at frame 1098\n",
      "Epoch: 179, frames: 1099, score: 310.0, average loss: 0.12809966058666558\n",
      "actions trained:           [232. 272. 157. 148. 166. 123.]\n",
      "actions out while training:[188. 311. 156. 135. 177. 131.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 178\n",
      "Death at frame 293\n",
      "Death at frame 478\n",
      "Saving model at epoch 180: ./models/space_invaders_rl\n",
      "Epoch: 180, frames: 479, score: 210.0, average loss: 0.11867430373985885\n",
      "actions trained:           [ 47. 186.  74.  48.  75.  48.]\n",
      "actions out while training:[ 46. 190.  74.  51.  73.  44.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 182\n",
      "Death at frame 463\n",
      "Death at frame 688\n",
      "Epoch: 181, frames: 689, score: 215.0, average loss: 0.13325432102483833\n",
      "actions trained:           [148. 183. 105.  69. 126.  57.]\n",
      "actions out while training:[121. 191. 115.  77. 126.  58.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 142\n",
      "Death at frame 256\n",
      "Death at frame 743\n",
      "Epoch: 182, frames: 744, score: 285.0, average loss: 0.1437866196556079\n",
      "actions trained:           [ 80. 225. 127. 139.  61. 111.]\n",
      "actions out while training:[ 81. 219. 126. 137.  69. 111.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 180\n",
      "Death at frame 630\n",
      "Death at frame 954\n",
      "Epoch: 183, frames: 955, score: 245.0, average loss: 0.12742583005330022\n",
      "actions trained:           [ 37. 480. 175.  98.  70.  94.]\n",
      "actions out while training:[ 51. 471. 173. 104.  67.  88.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 228\n",
      "Death at frame 340\n",
      "Death at frame 553\n",
      "Epoch: 184, frames: 554, score: 150.0, average loss: 0.12410194053516964\n",
      "actions trained:           [ 26. 353.  99.  11.  23.  41.]\n",
      "actions out while training:[ 44. 329.  90.  21.  28.  41.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 162\n",
      "Death at frame 305\n",
      "Death at frame 461\n",
      "Epoch: 185, frames: 462, score: 80.0, average loss: 0.1084437426325813\n",
      "actions trained:           [ 22. 364.  16.  15.  41.   3.]\n",
      "actions out while training:[ 29. 346.  24.  17.  41.   4.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 166\n",
      "Death at frame 669\n",
      "Death at frame 828\n",
      "Epoch: 186, frames: 829, score: 410.0, average loss: 0.12872929993190368\n",
      "actions trained:           [263. 174. 110. 108. 132.  41.]\n",
      "actions out while training:[189. 194. 125. 108. 150.  62.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 370\n",
      "Death at frame 453\n",
      "Death at frame 706\n",
      "Epoch: 187, frames: 707, score: 140.0, average loss: 0.13241142132727007\n",
      "actions trained:           [ 33. 447.  89.  15.  75.  47.]\n",
      "actions out while training:[ 42. 424.  69.  35.  88.  48.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 178\n",
      "Death at frame 685\n",
      "Death at frame 1152\n",
      "Epoch: 188, frames: 1153, score: 225.0, average loss: 0.1503390531607499\n",
      "actions trained:           [ 94. 719. 120.  47.  90.  82.]\n",
      "actions out while training:[131. 635. 127.  55. 109.  95.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 502\n",
      "Death at frame 771\n",
      "Death at frame 877\n",
      "Epoch: 189, frames: 878, score: 165.0, average loss: 0.14161561969959488\n",
      "actions trained:           [226. 234.  55. 142. 194.  26.]\n",
      "actions out while training:[164. 240.  61. 151. 228.  33.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 992\n",
      "Death at frame 1195\n",
      "Saving model at epoch 190: ./models/space_invaders_rl\n",
      "Epoch: 190, frames: 1196, score: 585.0, average loss: 0.1430514381035538\n",
      "actions trained:           [147. 330. 191. 157. 212. 158.]\n",
      "actions out while training:[140. 341. 201. 141. 204. 168.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 585\n",
      "Death at frame 755\n",
      "Death at frame 1033\n",
      "Epoch: 191, frames: 1034, score: 550.0, average loss: 0.1374588211531746\n",
      "actions trained:           [ 78. 296. 168. 167. 145. 179.]\n",
      "actions out while training:[ 77. 303. 170. 151. 149. 183.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 215\n",
      "Death at frame 490\n",
      "Death at frame 671\n",
      "Epoch: 192, frames: 672, score: 235.0, average loss: 0.11744218247714501\n",
      "actions trained:           [159. 257.  93.  38.  82.  42.]\n",
      "actions out while training:[120. 274.  84.  56.  87.  50.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 363\n",
      "Death at frame 469\n",
      "Death at frame 877\n",
      "Epoch: 193, frames: 878, score: 230.0, average loss: 0.12835081707447016\n",
      "actions trained:           [ 26. 329.  88. 109. 128. 197.]\n",
      "actions out while training:[ 36. 334. 100. 129. 121. 157.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 170\n",
      "Death at frame 282\n",
      "Death at frame 756\n",
      "Epoch: 194, frames: 757, score: 510.0, average loss: 0.13692447772047375\n",
      "actions trained:           [ 86. 212.  77. 211. 120.  50.]\n",
      "actions out while training:[ 95. 213.  85. 181. 125.  57.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 215\n",
      "Death at frame 407\n",
      "Death at frame 678\n",
      "Epoch: 195, frames: 679, score: 135.0, average loss: 0.12371068817385768\n",
      "actions trained:           [ 61. 278. 141.  91.  26.  81.]\n",
      "actions out while training:[ 58. 276. 135.  92.  38.  79.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 179\n",
      "Death at frame 295\n",
      "Death at frame 570\n",
      "Epoch: 196, frames: 571, score: 180.0, average loss: 0.11696305210985915\n",
      "actions trained:           [ 31. 228. 132.  48.  96.  35.]\n",
      "actions out while training:[ 30. 231. 130.  54.  89.  36.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 231\n",
      "Death at frame 395\n",
      "Death at frame 539\n",
      "Epoch: 197, frames: 540, score: 135.0, average loss: 0.12094861064618886\n",
      "actions trained:           [ 38. 211.  26.  78. 136.  50.]\n",
      "actions out while training:[ 38. 213.  29.  87. 114.  58.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 164\n",
      "Death at frame 648\n",
      "Death at frame 832\n",
      "Epoch: 198, frames: 833, score: 210.0, average loss: 0.12830578721722485\n",
      "actions trained:           [133. 255.  96. 135. 123.  90.]\n",
      "actions out while training:[109. 281. 103. 116. 128.  95.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 289\n",
      "Death at frame 550\n",
      "Death at frame 801\n",
      "Epoch: 199, frames: 802, score: 210.0, average loss: 0.14894341647047699\n",
      "actions trained:           [ 14. 643.  24.  11.  74.  35.]\n",
      "actions out while training:[ 58. 573.  25.  17.  86.  42.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 422\n",
      "Death at frame 722\n",
      "Death at frame 815\n",
      "Saving model at epoch 200: ./models/space_invaders_rl\n",
      "Epoch: 200, frames: 816, score: 225.0, average loss: 0.13475681202815298\n",
      "actions trained:           [187. 281. 159.  73.  50.  65.]\n",
      "actions out while training:[136. 298. 153.  85.  63.  80.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 550\n",
      "Death at frame 716\n",
      "Death at frame 846\n",
      "Epoch: 201, frames: 847, score: 250.0, average loss: 0.13005272279282462\n",
      "actions trained:           [ 88. 278. 218. 105.  93.  64.]\n",
      "actions out while training:[ 89. 278. 206. 107.  94.  72.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 651\n",
      "Death at frame 810\n",
      "Death at frame 906\n",
      "Epoch: 202, frames: 907, score: 285.0, average loss: 0.13161004832565995\n",
      "actions trained:           [172. 156. 118.  65. 132. 263.]\n",
      "actions out while training:[154. 180. 127.  75. 150. 220.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 387\n",
      "Death at frame 613\n",
      "Death at frame 894\n",
      "Epoch: 203, frames: 895, score: 210.0, average loss: 0.1237893539356554\n",
      "actions trained:           [ 52. 451.  80. 182.  82.  47.]\n",
      "actions out while training:[ 57. 461. 101. 143.  76.  56.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 250\n",
      "Death at frame 330\n",
      "Death at frame 545\n",
      "Epoch: 204, frames: 546, score: 105.0, average loss: 0.12384264118780638\n",
      "actions trained:           [ 12. 338.  81.  55.  38.  21.]\n",
      "actions out while training:[ 14. 305.  90.  69.  45.  22.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 409\n",
      "Death at frame 650\n",
      "Death at frame 804\n",
      "Epoch: 205, frames: 805, score: 255.0, average loss: 0.13216372212296534\n",
      "actions trained:           [ 25. 401. 198.  12.  79.  89.]\n",
      "actions out while training:[ 28. 396. 189.  21.  83.  87.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 424\n",
      "Death at frame 512\n",
      "Death at frame 714\n",
      "Epoch: 206, frames: 715, score: 225.0, average loss: 0.13888036227335646\n",
      "actions trained:           [ 78. 294.  91.  93. 122.  36.]\n",
      "actions out while training:[ 76. 291.  88.  95. 123.  41.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 181\n",
      "Death at frame 1120\n",
      "Death at frame 1568\n",
      "Epoch: 207, frames: 1569, score: 465.0, average loss: 0.135620357134714\n",
      "actions trained:           [236. 423. 210. 148. 235. 316.]\n",
      "actions out while training:[236. 446. 257. 138. 222. 269.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 161\n",
      "Death at frame 466\n",
      "Death at frame 780\n",
      "Epoch: 208, frames: 781, score: 315.0, average loss: 0.12983446850436692\n",
      "actions trained:           [180. 308.  64.  44.  94.  90.]\n",
      "actions out while training:[200. 290.  71.  52.  86.  81.]\n",
      "\n",
      "Using strict AI actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death at frame 166\n",
      "Death at frame 386\n",
      "Death at frame 485\n",
      "Epoch: 209, frames: 486, score: 135.0, average loss: 0.1396102371655128\n",
      "actions trained:           [124. 207.  65.  33.  44.  12.]\n",
      "actions out while training:[117. 196.  69.  43.  50.  10.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 247\n",
      "Death at frame 484\n",
      "Death at frame 1027\n",
      "Saving model at epoch 210: ./models/space_invaders_rl\n",
      "Epoch: 210, frames: 1028, score: 375.0, average loss: 0.12123021351271897\n",
      "actions trained:           [209. 261. 222.  69. 117. 149.]\n",
      "actions out while training:[151. 276. 214.  84. 145. 157.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 228\n",
      "Death at frame 309\n",
      "Death at frame 535\n",
      "Epoch: 211, frames: 536, score: 135.0, average loss: 0.11901768466714821\n",
      "actions trained:           [ 30. 330.  31.  91.  33.  20.]\n",
      "actions out while training:[ 34. 310.  40.  79.  40.  32.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 219\n",
      "Death at frame 735\n",
      "Death at frame 942\n",
      "Epoch: 212, frames: 943, score: 485.0, average loss: 0.15293215159043\n",
      "actions trained:           [ 76. 324. 147.  66. 139. 190.]\n",
      "actions out while training:[102. 323. 145.  76. 136. 160.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 569\n",
      "Death at frame 993\n",
      "Death at frame 1276\n",
      "Epoch: 213, frames: 1277, score: 200.0, average loss: 0.1393099566981285\n",
      "actions trained:           [190. 537. 105. 219. 176.  49.]\n",
      "actions out while training:[166. 545. 102. 223. 180.  60.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 365\n",
      "Death at frame 517\n",
      "Death at frame 936\n",
      "Epoch: 214, frames: 937, score: 275.0, average loss: 0.12992331851907424\n",
      "actions trained:           [ 77. 563.  82.  66.  80.  68.]\n",
      "actions out while training:[115. 476. 102.  75.  99.  69.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 192\n",
      "Death at frame 981\n",
      "Death at frame 1478\n",
      "Epoch: 215, frames: 1479, score: 360.0, average loss: 0.13351154164723064\n",
      "actions trained:           [261. 476. 124. 148. 254. 215.]\n",
      "actions out while training:[218. 527. 132. 158. 228. 215.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 239\n",
      "Death at frame 326\n",
      "Death at frame 612\n",
      "Epoch: 216, frames: 613, score: 135.0, average loss: 0.14689779082183416\n",
      "actions trained:           [ 12. 429.  40.  60.  35.  36.]\n",
      "actions out while training:[ 44. 400.  33.  71.  38.  26.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 230\n",
      "Death at frame 586\n",
      "Death at frame 703\n",
      "Epoch: 217, frames: 704, score: 210.0, average loss: 0.12277952875761579\n",
      "actions trained:           [ 94. 210. 114. 146. 100.  39.]\n",
      "actions out while training:[ 83. 219. 115. 137.  91.  58.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 293\n",
      "Death at frame 399\n",
      "Death at frame 565\n",
      "Epoch: 218, frames: 566, score: 125.0, average loss: 0.12694446676886895\n",
      "actions trained:           [ 27. 305.  66.  26.  66.  75.]\n",
      "actions out while training:[ 34. 285.  75.  30.  72.  69.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 187\n",
      "Death at frame 548\n",
      "Death at frame 752\n",
      "Epoch: 219, frames: 753, score: 180.0, average loss: 0.11956671535753334\n",
      "actions trained:           [113. 227. 133.  87.  86. 106.]\n",
      "actions out while training:[ 90. 241. 130. 101.  96.  94.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 154\n",
      "Death at frame 349\n",
      "Death at frame 710\n",
      "Saving model at epoch 220: ./models/space_invaders_rl\n",
      "Epoch: 220, frames: 711, score: 155.0, average loss: 0.134921705561277\n",
      "actions trained:           [ 74. 249.  76.  77. 106. 128.]\n",
      "actions out while training:[ 68. 259.  75.  82. 103. 123.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 819\n",
      "Death at frame 911\n",
      "Death at frame 1346\n",
      "Epoch: 221, frames: 1347, score: 335.0, average loss: 0.13113788049664007\n",
      "actions trained:           [166. 586. 134. 149. 108. 203.]\n",
      "actions out while training:[147. 593. 146. 147. 119. 194.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 236\n",
      "Death at frame 439\n",
      "Death at frame 779\n",
      "Epoch: 222, frames: 780, score: 230.0, average loss: 0.1281237726702488\n",
      "actions trained:           [107. 303.  72.  86. 133.  78.]\n",
      "actions out while training:[103. 302.  81.  87. 128.  78.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 201\n",
      "Death at frame 341\n",
      "Death at frame 551\n",
      "Epoch: 223, frames: 552, score: 135.0, average loss: 0.13113217968757243\n",
      "actions trained:           [  4. 421.  56.  11.  41.  18.]\n",
      "actions out while training:[ 12. 401.  52.  25.  49.  12.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 271\n",
      "Death at frame 405\n",
      "Death at frame 555\n",
      "Epoch: 224, frames: 556, score: 155.0, average loss: 0.1309410286469224\n",
      "actions trained:           [ 96. 252.  40. 102.  46.  19.]\n",
      "actions out while training:[ 73. 251.  49.  98.  58.  26.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 442\n",
      "Death at frame 740\n",
      "Death at frame 1016\n",
      "Epoch: 225, frames: 1017, score: 420.0, average loss: 0.12493129125303863\n",
      "actions trained:           [156. 324. 102. 123. 152. 159.]\n",
      "actions out while training:[151. 327. 108. 118. 154. 158.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 217\n",
      "Death at frame 692\n",
      "Death at frame 791\n",
      "Epoch: 226, frames: 792, score: 215.0, average loss: 0.12284297932158725\n",
      "actions trained:           [159. 208. 125.  87. 104. 108.]\n",
      "actions out while training:[108. 231. 137.  70. 112. 133.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 205\n",
      "Death at frame 292\n",
      "Death at frame 431\n",
      "Epoch: 227, frames: 432, score: 135.0, average loss: 0.12169779468558285\n",
      "actions trained:           [  3. 250. 101.   5.  35.  37.]\n",
      "actions out while training:[  6. 245.  93.  12.  40.  35.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 239\n",
      "Death at frame 897\n",
      "Death at frame 1329\n",
      "Epoch: 228, frames: 1330, score: 330.0, average loss: 0.13529750181707406\n",
      "actions trained:           [116. 435. 141. 200. 129. 308.]\n",
      "actions out while training:[124. 470. 161. 170. 131. 273.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 243\n",
      "Death at frame 342\n",
      "Death at frame 457\n",
      "Epoch: 229, frames: 458, score: 60.0, average loss: 0.10749383898468141\n",
      "actions trained:           [ 57. 282.  44.  28.  23.  23.]\n",
      "actions out while training:[ 45. 273.  47.  29.  36.  27.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 231\n",
      "Death at frame 464\n",
      "Death at frame 878\n",
      "Saving model at epoch 230: ./models/space_invaders_rl\n",
      "Epoch: 230, frames: 879, score: 290.0, average loss: 0.12809570572065326\n",
      "actions trained:           [ 76. 355. 131.  55. 121. 140.]\n",
      "actions out while training:[ 80. 354. 124.  66. 122. 132.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 357\n",
      "Death at frame 650\n",
      "Death at frame 770\n",
      "Epoch: 231, frames: 771, score: 210.0, average loss: 0.12732389172962538\n",
      "actions trained:           [ 66. 192. 141.  50.  93. 228.]\n",
      "actions out while training:[ 67. 225. 157.  71.  89. 161.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 201\n",
      "Death at frame 333\n",
      "Death at frame 488\n",
      "Epoch: 232, frames: 489, score: 65.0, average loss: 0.11012013145121624\n",
      "actions trained:           [ 25. 355.  18.  29.  54.   7.]\n",
      "actions out while training:[ 30. 345.  19.  34.  53.   7.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 401\n",
      "Death at frame 1016\n",
      "Epoch: 233, frames: 1017, score: 315.0, average loss: 0.13651627794148596\n",
      "actions trained:           [118. 443.  91. 121. 177.  66.]\n",
      "actions out while training:[133. 430. 100. 124. 162.  67.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 168\n",
      "Death at frame 416\n",
      "Death at frame 785\n",
      "Epoch: 234, frames: 786, score: 210.0, average loss: 0.12255849833501192\n",
      "actions trained:           [152. 366. 102.  97.  42.  26.]\n",
      "actions out while training:[100. 389. 100. 105.  58.  33.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 203\n",
      "Death at frame 292\n",
      "Death at frame 462\n",
      "Epoch: 235, frames: 463, score: 105.0, average loss: 0.09486373858097606\n",
      "actions trained:           [ 22. 215. 131.  42.  29.  23.]\n",
      "actions out while training:[ 22. 215. 108.  52.  40.  25.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 229\n",
      "Death at frame 919\n",
      "Epoch: 236, frames: 920, score: 465.0, average loss: 0.13743491047925524\n",
      "actions trained:           [ 91. 515. 119.  61.  79.  54.]\n",
      "actions out while training:[ 94. 496. 123.  64.  84.  58.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 419\n",
      "Death at frame 796\n",
      "Death at frame 895\n",
      "Epoch: 237, frames: 896, score: 260.0, average loss: 0.1372947618349771\n",
      "actions trained:           [154. 496.  73.  75.  58.  39.]\n",
      "actions out while training:[139. 473.  75.  87.  77.  44.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 678\n",
      "Death at frame 924\n",
      "Death at frame 1067\n",
      "Epoch: 238, frames: 1068, score: 490.0, average loss: 0.12433014145971694\n",
      "actions trained:           [187. 270. 117. 106. 196. 191.]\n",
      "actions out while training:[155. 300. 137.  92. 187. 196.]\n",
      "\n",
      "Using strict AI actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death at frame 280\n",
      "Death at frame 417\n",
      "Death at frame 513\n",
      "Epoch: 239, frames: 514, score: 120.0, average loss: 0.11860726835155742\n",
      "actions trained:           [ 24. 216.  96.  28. 104.  45.]\n",
      "actions out while training:[ 27. 219.  89.  47.  90.  41.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 179\n",
      "Death at frame 510\n",
      "Death at frame 874\n",
      "Saving model at epoch 240: ./models/space_invaders_rl\n",
      "Epoch: 240, frames: 875, score: 265.0, average loss: 0.13343416524087892\n",
      "actions trained:           [ 15. 570. 109.  57. 101.  22.]\n",
      "actions out while training:[ 20. 536. 114.  71. 107.  26.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 150\n",
      "Death at frame 625\n",
      "Death at frame 845\n",
      "Epoch: 241, frames: 846, score: 150.0, average loss: 0.14435689442625171\n",
      "actions trained:           [182. 332.  84. 111. 101.  35.]\n",
      "actions out while training:[171. 309.  92. 122. 109.  42.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 186\n",
      "Death at frame 648\n",
      "Death at frame 847\n",
      "Epoch: 242, frames: 848, score: 260.0, average loss: 0.12621620688825766\n",
      "actions trained:           [120. 310. 155. 110.  76.  76.]\n",
      "actions out while training:[ 96. 326. 143. 105.  90.  87.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 486\n",
      "Death at frame 921\n",
      "Death at frame 1408\n",
      "Epoch: 243, frames: 1409, score: 965.0, average loss: 0.1357763060018718\n",
      "actions trained:           [ 67. 367. 252. 210. 183. 329.]\n",
      "actions out while training:[ 82. 405. 290. 211. 159. 261.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 356\n",
      "Death at frame 696\n",
      "Death at frame 1084\n",
      "Epoch: 244, frames: 1085, score: 235.0, average loss: 0.14728864801856714\n",
      "actions trained:           [269. 555. 100.  61.  73.  26.]\n",
      "actions out while training:[278. 474. 112.  78. 111.  31.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 176\n",
      "Death at frame 791\n",
      "Death at frame 1093\n",
      "Epoch: 245, frames: 1094, score: 310.0, average loss: 0.1281686897329518\n",
      "actions trained:           [ 61. 382. 123. 322. 133.  72.]\n",
      "actions out while training:[ 63. 424. 151. 227. 134.  94.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 205\n",
      "Death at frame 465\n",
      "Death at frame 788\n",
      "Epoch: 246, frames: 789, score: 240.0, average loss: 0.12538147648665443\n",
      "actions trained:           [ 48. 270. 107. 100. 113. 150.]\n",
      "actions out while training:[ 57. 277. 120.  94. 110. 130.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 401\n",
      "Death at frame 725\n",
      "Death at frame 1083\n",
      "Epoch: 247, frames: 1084, score: 295.0, average loss: 0.1377604150068848\n",
      "actions trained:           [164. 530.  89.  46. 187.  67.]\n",
      "actions out while training:[169. 511.  90.  53. 189.  71.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 166\n",
      "Death at frame 327\n",
      "Death at frame 614\n",
      "Epoch: 248, frames: 615, score: 210.0, average loss: 0.13116420576839843\n",
      "actions trained:           [ 26. 157. 110.  97. 142.  82.]\n",
      "actions out while training:[ 26. 169. 111.  94. 136.  78.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 137\n",
      "Death at frame 473\n",
      "Death at frame 673\n",
      "Epoch: 249, frames: 674, score: 260.0, average loss: 0.1412962728151426\n",
      "actions trained:           [ 21. 407.  97.  89.  27.  32.]\n",
      "actions out while training:[ 31. 389. 101.  93.  27.  32.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 166\n",
      "Death at frame 948\n",
      "Death at frame 1407\n",
      "Saving model at epoch 250: ./models/space_invaders_rl\n",
      "Epoch: 250, frames: 1408, score: 330.0, average loss: 0.14231786533336777\n",
      "actions trained:           [175. 340. 222.  65. 182. 423.]\n",
      "actions out while training:[204. 366. 231.  83. 200. 323.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 166\n",
      "Death at frame 318\n",
      "Death at frame 464\n",
      "Epoch: 251, frames: 465, score: 155.0, average loss: 0.13674971409005163\n",
      "actions trained:           [ 73. 215.  38.  65.  50.  23.]\n",
      "actions out while training:[ 63. 212.  41.  68.  53.  27.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 412\n",
      "Death at frame 759\n",
      "Death at frame 865\n",
      "Epoch: 252, frames: 866, score: 260.0, average loss: 0.13277286465016774\n",
      "actions trained:           [ 91. 437. 155.  38.  70.  74.]\n",
      "actions out while training:[ 88. 435. 145.  45.  76.  76.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 388\n",
      "Death at frame 824\n",
      "Death at frame 1083\n",
      "Epoch: 253, frames: 1084, score: 290.0, average loss: 0.13230730289340767\n",
      "actions trained:           [ 71. 636. 128. 115.  58.  75.]\n",
      "actions out while training:[ 84. 582. 138. 124.  72.  83.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 169\n",
      "Death at frame 307\n",
      "Death at frame 703\n",
      "Epoch: 254, frames: 704, score: 410.0, average loss: 0.13231877258123362\n",
      "actions trained:           [166. 329.  39.  43.  96.  30.]\n",
      "actions out while training:[138. 327.  42.  51. 111.  34.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 182\n",
      "Death at frame 268\n",
      "Death at frame 363\n",
      "Epoch: 255, frames: 364, score: 155.0, average loss: 0.12019064146947338\n",
      "actions trained:           [  7. 121.  52.  28.  62.  93.]\n",
      "actions out while training:[ 15. 122.  62.  27.  53.  84.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 172\n",
      "Death at frame 287\n",
      "Death at frame 937\n",
      "Epoch: 256, frames: 938, score: 515.0, average loss: 0.13477556172873417\n",
      "actions trained:           [171. 303.  76. 197. 135.  55.]\n",
      "actions out while training:[161. 308.  88. 182. 138.  60.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 166\n",
      "Death at frame 786\n",
      "Epoch: 257, frames: 787, score: 405.0, average loss: 0.1391360196515714\n",
      "actions trained:           [ 64. 141. 172.  75. 155. 179.]\n",
      "actions out while training:[ 64. 142. 179.  76. 144. 181.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 182\n",
      "Death at frame 902\n",
      "Death at frame 1010\n",
      "Epoch: 258, frames: 1011, score: 260.0, average loss: 0.12868489023175209\n",
      "actions trained:           [116. 394. 188. 125.  64. 123.]\n",
      "actions out while training:[100. 403. 176. 119.  84. 128.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 147\n",
      "Death at frame 756\n",
      "Death at frame 945\n",
      "Epoch: 259, frames: 946, score: 440.0, average loss: 0.1447681293379808\n",
      "actions trained:           [298. 236. 137.  86. 109.  79.]\n",
      "actions out while training:[267. 245. 138.  94. 114.  87.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 255\n",
      "Death at frame 592\n",
      "Death at frame 817\n",
      "Saving model at epoch 260: ./models/space_invaders_rl\n",
      "Epoch: 260, frames: 818, score: 260.0, average loss: 0.12794304358553119\n",
      "actions trained:           [ 21. 294. 164.  99.  89. 150.]\n",
      "actions out while training:[ 22. 295. 172.  97.  91. 140.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 329\n",
      "Death at frame 436\n",
      "Death at frame 1088\n",
      "Epoch: 261, frames: 1089, score: 280.0, average loss: 0.14094525470920624\n",
      "actions trained:           [ 71. 634. 143.  79.  89.  72.]\n",
      "actions out while training:[106. 571. 142.  90.  95.  84.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 235\n",
      "Death at frame 322\n",
      "Death at frame 621\n",
      "Epoch: 262, frames: 622, score: 155.0, average loss: 0.13923765866898807\n",
      "actions trained:           [ 14. 377.  33.  69.  71.  57.]\n",
      "actions out while training:[ 38. 363.  31.  75.  62.  52.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 172\n",
      "Death at frame 279\n",
      "Death at frame 513\n",
      "Epoch: 263, frames: 514, score: 170.0, average loss: 0.130952059061139\n",
      "actions trained:           [ 70. 226.  39.  50.  86.  42.]\n",
      "actions out while training:[ 60. 224.  44.  62.  80.  43.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 211\n",
      "Death at frame 443\n",
      "Death at frame 573\n",
      "Epoch: 264, frames: 574, score: 185.0, average loss: 0.12080722642626114\n",
      "actions trained:           [ 66. 137.  70.  68. 105. 127.]\n",
      "actions out while training:[ 64. 144.  74.  70.  97. 124.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 169\n",
      "Death at frame 310\n",
      "Death at frame 489\n",
      "Epoch: 265, frames: 490, score: 100.0, average loss: 0.11975569332456214\n",
      "actions trained:           [ 18. 329.  38.  22.  31.  51.]\n",
      "actions out while training:[ 30. 310.  35.  32.  43.  39.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 160\n",
      "Death at frame 272\n",
      "Death at frame 357\n",
      "Epoch: 266, frames: 358, score: 105.0, average loss: 0.11803792748359786\n",
      "actions trained:           [ 41. 156.  49.  30.  54.  27.]\n",
      "actions out while training:[ 40. 154.  52.  29.  55.  27.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 258\n",
      "Death at frame 373\n",
      "Death at frame 479\n",
      "Epoch: 267, frames: 480, score: 85.0, average loss: 0.1380250126305013\n",
      "actions trained:           [ 71. 207.  74.  31.  65.  31.]\n",
      "actions out while training:[ 60. 205.  63.  41.  75.  35.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 173\n",
      "Death at frame 296\n",
      "Death at frame 701\n",
      "Epoch: 268, frames: 702, score: 245.0, average loss: 0.13554525108538237\n",
      "actions trained:           [ 90. 257. 101. 103. 119.  31.]\n",
      "actions out while training:[ 84. 255. 110.  98. 119.  35.]\n",
      "\n",
      "Using strict AI actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death at frame 569\n",
      "Death at frame 769\n",
      "Death at frame 872\n",
      "Epoch: 269, frames: 873, score: 315.0, average loss: 0.12196653134613385\n",
      "actions trained:           [ 39. 288.  87. 193. 189.  76.]\n",
      "actions out while training:[ 49. 299.  98. 174. 157.  95.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 170\n",
      "Death at frame 537\n",
      "Death at frame 727\n",
      "Saving model at epoch 270: ./models/space_invaders_rl\n",
      "Epoch: 270, frames: 728, score: 180.0, average loss: 0.1201167636034349\n",
      "actions trained:           [104. 413.  68.  36.  75.  31.]\n",
      "actions out while training:[ 85. 396.  65.  52.  91.  38.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 351\n",
      "Death at frame 480\n",
      "Death at frame 640\n",
      "Epoch: 271, frames: 641, score: 210.0, average loss: 0.12245687573562429\n",
      "actions trained:           [ 33. 318.  81.  22.  84. 102.]\n",
      "actions out while training:[ 42. 306.  83.  38.  80.  91.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 160\n",
      "Death at frame 660\n",
      "Death at frame 817\n",
      "Epoch: 272, frames: 818, score: 210.0, average loss: 0.12803179308780427\n",
      "actions trained:           [ 92. 181. 143. 112.  84. 205.]\n",
      "actions out while training:[ 86. 192. 156. 117.  86. 180.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 221\n",
      "Death at frame 305\n",
      "Death at frame 944\n",
      "Epoch: 273, frames: 945, score: 240.0, average loss: 0.1282995780694562\n",
      "actions trained:           [103. 440. 129. 141.  78.  53.]\n",
      "actions out while training:[ 85. 451. 116. 123.  94.  75.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 665\n",
      "Death at frame 843\n",
      "Death at frame 964\n",
      "Epoch: 274, frames: 965, score: 345.0, average loss: 0.12615902917776203\n",
      "actions trained:           [232. 308. 107.  42.  98. 177.]\n",
      "actions out while training:[194. 328. 115.  43. 118. 166.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 320\n",
      "Death at frame 510\n",
      "Death at frame 645\n",
      "Epoch: 275, frames: 646, score: 210.0, average loss: 0.13081582221863616\n",
      "actions trained:           [ 25. 437.  22.  23.  93.  45.]\n",
      "actions out while training:[ 38. 393.  35.  48.  92.  39.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 268\n",
      "Death at frame 368\n",
      "Death at frame 604\n",
      "Epoch: 276, frames: 605, score: 155.0, average loss: 0.1348722919919418\n",
      "actions trained:           [ 65. 411.  34.  51.  27.  16.]\n",
      "actions out while training:[ 58. 398.  34.  55.  42.  17.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 168\n",
      "Death at frame 292\n",
      "Death at frame 653\n",
      "Epoch: 277, frames: 654, score: 210.0, average loss: 0.1288767055946245\n",
      "actions trained:           [ 93. 129. 105.  80. 100. 146.]\n",
      "actions out while training:[ 77. 130. 137.  72.  98. 139.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 259\n",
      "Death at frame 608\n",
      "Death at frame 703\n",
      "Epoch: 278, frames: 704, score: 235.0, average loss: 0.11722157424836335\n",
      "actions trained:           [ 85. 219. 169.  70.  81.  79.]\n",
      "actions out while training:[ 81. 232. 148.  78.  83.  81.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 241\n",
      "Death at frame 501\n",
      "Death at frame 664\n",
      "Epoch: 279, frames: 665, score: 215.0, average loss: 0.11827981002301631\n",
      "actions trained:           [ 22. 236. 230. 104.  25.  47.]\n",
      "actions out while training:[ 24. 245. 201. 101.  35.  58.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 185\n",
      "Death at frame 891\n",
      "Death at frame 1196\n",
      "Saving model at epoch 280: ./models/space_invaders_rl\n",
      "Epoch: 280, frames: 1197, score: 330.0, average loss: 0.13000033651575033\n",
      "actions trained:           [138. 572.  97. 160. 147.  82.]\n",
      "actions out while training:[137. 558. 104. 152. 152.  93.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 367\n",
      "Death at frame 603\n",
      "Death at frame 778\n",
      "Epoch: 281, frames: 779, score: 245.0, average loss: 0.12217662941340807\n",
      "actions trained:           [ 42. 212. 230.  37. 153. 104.]\n",
      "actions out while training:[ 49. 221. 195.  42. 136. 135.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 893\n",
      "Death at frame 1397\n",
      "Death at frame 1488\n",
      "Epoch: 282, frames: 1489, score: 450.0, average loss: 0.1375225880186854\n",
      "actions trained:           [111. 549. 191. 187. 162. 288.]\n",
      "actions out while training:[127. 574. 188. 196. 159. 244.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 278\n",
      "Death at frame 446\n",
      "Death at frame 559\n",
      "Epoch: 283, frames: 560, score: 125.0, average loss: 0.11970521462466296\n",
      "actions trained:           [ 54. 409.  21.  17.  44.  14.]\n",
      "actions out while training:[ 76. 363.  26.  27.  54.  13.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 313\n",
      "Death at frame 477\n",
      "Death at frame 661\n",
      "Epoch: 284, frames: 662, score: 180.0, average loss: 0.12905411258049662\n",
      "actions trained:           [ 69. 459.  41.  34.  22.  36.]\n",
      "actions out while training:[ 56. 433.  51.  50.  35.  36.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 174\n",
      "Death at frame 291\n",
      "Death at frame 638\n",
      "Epoch: 285, frames: 639, score: 155.0, average loss: 0.12136670790930695\n",
      "actions trained:           [101. 259.  54. 140.  66.  18.]\n",
      "actions out while training:[ 79. 267.  62. 123.  83.  24.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 259\n",
      "Death at frame 377\n",
      "Death at frame 497\n",
      "Epoch: 286, frames: 498, score: 130.0, average loss: 0.12439828246485493\n",
      "actions trained:           [ 12. 206.  62.  48.  78.  91.]\n",
      "actions out while training:[ 20. 203.  63.  53.  72.  86.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 165\n",
      "Death at frame 289\n",
      "Death at frame 504\n",
      "Epoch: 287, frames: 505, score: 155.0, average loss: 0.12107068475098791\n",
      "actions trained:           [ 74. 282.  23.  56.  38.  31.]\n",
      "actions out while training:[ 74. 266.  28.  50.  50.  36.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 230\n",
      "Death at frame 325\n",
      "Death at frame 452\n",
      "Epoch: 288, frames: 453, score: 80.0, average loss: 0.11162203489482538\n",
      "actions trained:           [ 70. 203.  42.  39.  50.  48.]\n",
      "actions out while training:[ 70. 202.  42.  39.  49.  50.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 165\n",
      "Death at frame 296\n",
      "Death at frame 668\n",
      "Epoch: 289, frames: 669, score: 210.0, average loss: 0.1414095756427874\n",
      "actions trained:           [ 92. 137.  83.  75. 104. 177.]\n",
      "actions out while training:[ 76. 150.  94.  78. 118. 152.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 402\n",
      "Death at frame 647\n",
      "Death at frame 780\n",
      "Saving model at epoch 290: ./models/space_invaders_rl\n",
      "Epoch: 290, frames: 781, score: 225.0, average loss: 0.12450618379631159\n",
      "actions trained:           [153. 213.  83. 114. 112. 105.]\n",
      "actions out while training:[138. 228. 105.  99.  97. 113.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 285\n",
      "Death at frame 659\n",
      "Death at frame 801\n",
      "Epoch: 291, frames: 802, score: 225.0, average loss: 0.14406288952255697\n",
      "actions trained:           [ 60. 597.  47.  37.  40.  20.]\n",
      "actions out while training:[ 79. 542.  58.  44.  57.  21.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 179\n",
      "Death at frame 460\n",
      "Death at frame 781\n",
      "Epoch: 292, frames: 782, score: 235.0, average loss: 0.13057885142133838\n",
      "actions trained:           [ 86. 360. 104.  84. 101.  46.]\n",
      "actions out while training:[ 94. 343. 116.  83.  98.  47.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 204\n",
      "Death at frame 456\n",
      "Death at frame 542\n",
      "Epoch: 293, frames: 543, score: 135.0, average loss: 0.12048803198101568\n",
      "actions trained:           [ 44. 147. 159.  45.  83.  64.]\n",
      "actions out while training:[ 64. 153. 126.  58.  79.  62.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 554\n",
      "Death at frame 1008\n",
      "Death at frame 1143\n",
      "Epoch: 294, frames: 1144, score: 390.0, average loss: 0.13276758289805707\n",
      "actions trained:           [287. 334. 151. 127. 152.  92.]\n",
      "actions out while training:[245. 358. 151. 136. 147. 106.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 356\n",
      "Death at frame 498\n",
      "Death at frame 817\n",
      "Epoch: 295, frames: 818, score: 210.0, average loss: 0.12887630280140075\n",
      "actions trained:           [ 15. 468.  62.  37.  52. 183.]\n",
      "actions out while training:[ 25. 439.  80.  68.  65. 140.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 172\n",
      "Death at frame 421\n",
      "Death at frame 608\n",
      "Epoch: 296, frames: 609, score: 210.0, average loss: 0.1274873870096608\n",
      "actions trained:           [ 46. 266.  57. 158.  67.  14.]\n",
      "actions out while training:[ 43. 268.  56. 146.  80.  15.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 176\n",
      "Death at frame 289\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name)\n",
    "global_step = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if load_model:\n",
    "        print(\"Loading existing model before training: {}\".format(saver_file))\n",
    "        saver.restore(sess, saver_file)\n",
    "    else:\n",
    "        print(\"Creating new model before training: {}\".format(saver_file))\n",
    "        sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):  #play n_epochs games\n",
    "        observation = env.reset()\n",
    "        temp_lives = 3\n",
    "        score = 0.0\n",
    "        frames = np.empty([height, width, 0])\n",
    "        actions = np.empty([0])\n",
    "        rewards = np.empty([0])\n",
    "        all_logits = []\n",
    "        punish_frames = []\n",
    "        \n",
    "        #get the first frame\n",
    "        input_value = 0  #set an initial input value\n",
    "        #observation, reward_float, done_bool, info_dict = env.step(input_value)\n",
    "        obs_greyscale = preprocess_observation(observation)\n",
    "        obs_greyscale_reshape = np.reshape(obs_greyscale, (height,width,1))\n",
    "        frames = np.append(frames, obs_greyscale_reshape, axis=2)\n",
    "        actions = np.append(actions, input_value)\n",
    "        last_action_step = 0\n",
    "        decision_step_counter = 0\n",
    "    \n",
    "        \n",
    "        game_step_counter = 0\n",
    "        action_from_ai_epsilon_greedy = 0\n",
    "        \n",
    "        #play the game until the first death, recording the frames along the way\n",
    "        gc.disable()\n",
    "        while True:\n",
    "            #this_frame = frames[:,:,np.ma.size(frames, axis=2)-1]\n",
    "            #this_flattened_frame = this_frame.flatten()\n",
    "            #this_flattened_frame_reshaped = np.reshape(this_flattened_frame, (1, this_flattened_frame.size))\n",
    "            \n",
    "            num_frames = np.ma.size(frames, axis=2)\n",
    "                       \n",
    "            concatenated_frames = frames[:,:,num_frames-1]\n",
    "            \n",
    "            for j in range(1,frames_captured,1):\n",
    "                if num_frames-j <= 1:\n",
    "                    this_frame = frames[:,:,0]\n",
    "                elif j >= num_frames:\n",
    "                    this_frame = frames[:,:,num_frames-1]\n",
    "                else:\n",
    "                    this_frame = frames[:,:,num_frames-1-j]\n",
    "                concatenated_frames = np.append(concatenated_frames, this_frame, axis=0)\n",
    "            \n",
    "            #concatenated_frames_reshaped = np.reshape(concatenated_frames, (1, 210*5*160))\n",
    "            concatenated_frames_reshaped = np.reshape(concatenated_frames, (1, height*frames_captured, width, 1))\n",
    "            \n",
    "            #todo - write a function to build the concatenated_frames\n",
    "            \n",
    "            #use the input value from the AI\n",
    "            #temp_input = [0,0,0,0,0,0]\n",
    "            temp_input = np.zeros(discrete_actions) #[0, 0, 0, ...]\n",
    "            temp_input[action_from_ai_epsilon_greedy] = 1\n",
    "            temp_input_reshaped = np.reshape(temp_input, (1, len(temp_input)))\n",
    "            \n",
    "            #tf_dropout_keep_prob: 1.0,\n",
    "            feed_dict = {tf_input_frame : concatenated_frames_reshaped, \n",
    "                         tf_input_value : temp_input_reshaped,\n",
    "                         tf_input_learning_rate: 0.0,\n",
    "                         tf_dropout_keep_prob: 1.0,\n",
    "                         tf_reward: 0.0}\n",
    "            logits_out = sess.run([logits], feed_dict=feed_dict)\n",
    "            #all_gradients.append(gradients_out)\n",
    "            #all_variables.append(variables_out)\n",
    "\n",
    "            all_logits.append(logits_out[0])\n",
    "            \n",
    "            #if global_step % 5 == 0:  #only allow a change of direction every 5 steps.  \n",
    "            action_from_ai_logits_argmax = np.argmax(logits_out[0])\n",
    "            \n",
    "#             print(str(logits_out[0]) + \": \" + str(np.argmax(logits_out[0])))\n",
    "            \n",
    "#             print(\"output_probability_out: \" + str(output_probability_out))\n",
    "#             print(\"action_from_ai_logits_argmax: \" + str(action_from_ai_logits_argmax) + \", action_from_ai: \" + str(action_from_ai), \", numpy_choice: \" + str(numpy_choice))\n",
    "#             print(\"var: \" + str(var_out))\n",
    "             \n",
    "    \n",
    "#            print (\"logits_out[0]: {}\".format(logits_out[0]))\n",
    "    \n",
    "            positive_logits = logits_out[0] + abs(np.amin(logits_out[0]))\n",
    "            softmax_logits = softmax(positive_logits / np.amax(positive_logits))\n",
    "#            print (\"softmax_logits[0]: {}\".format(softmax_logits[0]))\n",
    "\n",
    "            try:\n",
    "                multinomial_action_array = np.random.multinomial(1, softmax_logits[0])\n",
    "                action_from_multinomial_action = np.argmax(multinomial_action_array)\n",
    "#                print (\"multinomial_action_array: {}\".format(multinomial_action_array))\n",
    "            except ValueError:\n",
    "                #I have no idea why this occassionally errors out.\n",
    "                action_from_multinomial_action = np.argmax(softmax_logits)\n",
    "                print (\"multinomial error, using action {}\".format(action_from_multinomial_action))\n",
    "                continue\n",
    "                \n",
    "#            print (\"action_from_multinomial_action: {}\".format(action_from_multinomial_action))\n",
    "        \n",
    "            # decide which action to use\n",
    "            if epoch % use_ai_every_x_epoch == 0 and epoch >= 0:\n",
    "                if game_step_counter == 0:\n",
    "                    print(\"Using strict AI actions\")\n",
    "                action_from_ai_epsilon_greedy = action_from_ai_logits_argmax  #do what the AI says\n",
    "            else:\n",
    "#                print(\"Using probability-based actions\")\n",
    "                action_from_ai_epsilon_greedy = action_from_multinomial_action  #use probability-based action\n",
    "            #action_from_ai_epsilon_greedy = action_from_ai  #do what the AI says with probabilities using tf.multinomial probabilities\n",
    "            #action_from_ai_epsilon_greedy = numpy_choice #do what the AI says with probabilities using numpy  probabilities\n",
    "                \n",
    "            #run the next step given the input from the logits\n",
    "            observation, reward_float, done_bool, info_dict = env.step(action_from_ai_epsilon_greedy)\n",
    "            \n",
    "            #add this frame to our frame buffer\n",
    "            obs_greyscale = preprocess_observation(observation)\n",
    "            obs_greyscale_reshape = np.reshape(obs_greyscale, (height,width,1))\n",
    "            frames = np.append(frames, obs_greyscale_reshape, axis=2)\n",
    "            actions = np.append(actions, action_from_ai_epsilon_greedy)\n",
    "            \n",
    "            score = score + reward_float\n",
    "            \n",
    "            if reward_float > max_score:\n",
    "                rewards = np.append(rewards, max_score)\n",
    "            else:\n",
    "                rewards = np.append(rewards, reward_float)\n",
    "            \n",
    "            lives = info_dict['ale.lives']\n",
    "\n",
    "            if done_bool:\n",
    "                punish_frames.append(len(rewards) - 40)\n",
    "                print(\"Death at frame {}\".format(len(rewards)))\n",
    "                break\n",
    "                \n",
    "            if game_step_counter > frame_limit:\n",
    "                break\n",
    "                \n",
    "            if lives != temp_lives:  #we lost a life.  consider this game over.\n",
    "                #print(\"Lost a life.  Current lives: {}\".format(lives))\n",
    "                temp_lives = lives\n",
    "                if len(rewards) > 5:\n",
    "                    punish_frames.append(len(rewards) - 40)\n",
    "                    print(\"Death at frame {}\".format(len(rewards)))\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                env.render()  #display the current frame.\n",
    "                \n",
    "            decision_step_counter += 1\n",
    "            game_step_counter += 1\n",
    "            global_step += 1\n",
    "        gc.enable()\n",
    "        if score == 0:\n",
    "            continue #causes an error and breaks the model.  just continue.\n",
    "        \n",
    "        if epoch % 10 == 0 and epoch > 0:\n",
    "            print(\"Saving model at epoch {}: {}\".format(epoch, saver_file))\n",
    "            saver.save(sess, saver_file)\n",
    "        \n",
    "        #punish death\n",
    "        for this_frame in punish_frames:\n",
    "            #rewards[this_frame] = (-1.0 * score / len(punish_frames)) / 2.0\n",
    "            rewards[this_frame] = -10\n",
    "        \n",
    "        num_frames = np.ma.size(frames, axis=2)\n",
    "        frames_to_skip_begin = 0\n",
    "        frames_to_skip_end  = 0  #number of frames between pacman being eaten and game reset\n",
    "    \n",
    "        discounted_rewards = helper_discount_rewards(rewards, discount_decay_rate, frames_to_skip_begin, num_frames) #-1-frames_to_skip_end\n",
    "        discounted_rewards_median = np.median(discounted_rewards)\n",
    "        discounted_rewards_mean = np.mean(discounted_rewards)\n",
    "        average_logits = get_average_logits(all_logits, discounted_rewards)\n",
    "        \n",
    "        display_actions = np.zeros(discrete_actions)\n",
    "        ai_actions = np.zeros(discrete_actions)\n",
    "        loss_out_sum = 0.0\n",
    "        frames_taught = 0.000000001\n",
    "        \n",
    "        skipped_frames = 0\n",
    "        temp_action = 0\n",
    "        \n",
    "        reward_frame_counter = 0\n",
    "        punish_frame_counter = 0\n",
    "            \n",
    "        frames_to_train = np.arange(num_frames-1-frames_to_skip_end)\n",
    "        np.random.shuffle(frames_to_train)\n",
    "        \n",
    "        for i in range(frames_to_skip_begin, num_frames-1-frames_to_skip_end):   #skip the first frames\n",
    "\n",
    "            this_random_frame_index = frames_to_train[i]\n",
    "\n",
    "            if i >= num_frames-1-frames_to_skip_end: \n",
    "                continue  #only train if frame is not during pacman's death throws \n",
    "            \n",
    "            if True: \n",
    "                concatenated_frames = frames[:,:,this_random_frame_index]\n",
    "                for j in range(-1,-1*frames_captured,-1):\n",
    "                    this_frame = frames[:,:,this_random_frame_index+j]\n",
    "                    this_flattened_frame = this_frame.flatten()\n",
    "                    concatenated_frames = np.append(concatenated_frames, this_frame, axis=0)\n",
    "                    \n",
    "#                concatenated_frames_reshaped = np.reshape(concatenated_frames, (1, 210*5*160))\n",
    "                concatenated_frames_reshaped = np.reshape(concatenated_frames, (1, height*frames_captured, width, 1))\n",
    "            \n",
    "#                action_taken_one_hot = action_to_one_hot(actions[i], n_outputs, rewards[i])\n",
    "                action_taken_one_hot = action_to_one_hot(actions[this_random_frame_index], n_outputs)\n",
    "                display_actions = np.add(display_actions, action_taken_one_hot)\n",
    "                \n",
    "                #reward_for_frame = discounted_rewards[i] + (-1.0 * abs(discounted_rewards_median))  # hopefully, adding the median will keep the AI from getting stuck taking one action\n",
    "                #reward_for_frame = discounted_rewards[i] - .05\n",
    "                reward_for_frame = discounted_rewards[this_random_frame_index]\n",
    "                #print(\"reward for frame {} is {}.  action: {}\".format(i, reward_for_frame, action_taken_one_hot_reshaped))\n",
    "               \n",
    "                if reward_for_frame > 0.85:\n",
    "                    reward_for_frame = 1.0\n",
    "                elif reward_for_frame < 0.0:\n",
    "                    reward_for_frame = -1.0\n",
    "                    \n",
    "                action_taken_one_hot = action_taken_one_hot * reward_for_frame\n",
    "                \n",
    "#                 if reward_for_frame != 1.0:\n",
    "#                     action_taken_one_hot = action_taken_one_hot - average_logits[int(actions[this_random_frame_index])]\n",
    "    \n",
    "                action_taken_one_hot_reshaped = np.reshape(action_taken_one_hot, (1, len(action_taken_one_hot)))\n",
    "    \n",
    "#                print(action_taken_one_hot_reshaped)\n",
    "                \n",
    "                feed_dict = {tf_input_frame : concatenated_frames_reshaped, \n",
    "                             tf_input_value : action_taken_one_hot_reshaped,\n",
    "                             tf_input_learning_rate: learning_rate,\n",
    "                             tf_dropout_keep_prob: dropout_keep_prob,\n",
    "                             tf_reward: reward_for_frame}\n",
    "                loss_out, _, logits_out = sess.run([loss, training_op, logits], feed_dict=feed_dict)\n",
    "\n",
    "                frames_taught = frames_taught + 1\n",
    "                loss_out_sum += loss_out\n",
    "\n",
    "                action_from_ai = np.argmax(logits_out[0])\n",
    "                action_from_ai_one_hot = action_to_one_hot(action_from_ai, n_outputs)\n",
    "                ai_actions = np.add(ai_actions, action_from_ai_one_hot)\n",
    "            else:\n",
    "                skipped_frames = skipped_frames + 1\n",
    "            \n",
    "            temp_action = actions[i]\n",
    "#             if epoch % 10 == 0:\n",
    "#                 print(\"ai action[\" + str(i) + \"]: \" + str(actions[i]) + \", action_taken_one_hot: \" + \n",
    "#                       str(action_taken_one_hot_reshaped) + \", loss_out: \" + str(loss_out))\n",
    "#                 print(\"logits: \" + str(logits_out[0]))\n",
    "        \n",
    "        print(\"Epoch: \" + str(epoch) + \", frames: \" + str(num_frames) + \", score: \" + str(score) + \", average loss: \" + str(loss_out_sum/frames_taught))\n",
    "        #print(\"actions trained: {}, rewards: {}, punishments: {}, median: {}, mean: {}\".format(display_actions, reward_frame_counter, punish_frame_counter, discounted_rewards_median, discounted_rewards_mean))\n",
    "        print(\"actions trained:           {}\".format(display_actions))\n",
    "        print(\"actions out while training:{}\".format(ai_actions))\n",
    "        #print(\"discounted_rewards: \" + str(discounted_rewards))\n",
    "        #TODO - should learning rate decrease over time?\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discounted rewared at frame 221: 0.7386609940220507\n",
      "0: reward: 0.0, discounted reward: -0.28489180016272203\n",
      "1: reward: 0.0, discounted reward: -0.2845605741472154\n",
      "2: reward: 0.0, discounted reward: -0.28421191518352423\n",
      "3: reward: 0.0, discounted reward: -0.2838449057480599\n",
      "4: reward: 0.0, discounted reward: -0.2834585800265184\n",
      "5: reward: 0.0, discounted reward: -0.28305192137226426\n",
      "6: reward: 0.0, discounted reward: -0.2826238596309441\n",
      "7: reward: 0.0, discounted reward: -0.2821732683242913\n",
      "8: reward: 0.0, discounted reward: -0.2816989616857094\n",
      "9: reward: 0.0, discounted reward: -0.28119969153983376\n",
      "10: reward: 0.0, discounted reward: -0.28067414401785934\n",
      "11: reward: 0.0, discounted reward: -0.2801209360999915\n",
      "12: reward: 0.0, discounted reward: -0.27953861197592017\n",
      "13: reward: 0.0, discounted reward: -0.2789256392137398\n",
      "14: reward: 0.0, discounted reward: -0.27828040472723414\n",
      "15: reward: 0.0, discounted reward: -0.27760121053091236\n",
      "16: reward: 0.0, discounted reward: -0.2768862692716263\n",
      "17: reward: 0.0, discounted reward: -0.27613369952500944\n",
      "18: reward: 0.0, discounted reward: -0.27534152084436003\n",
      "19: reward: 0.0, discounted reward: -0.27450764854893966\n",
      "20: reward: 0.0, discounted reward: -0.27362988823797085\n",
      "21: reward: 0.0, discounted reward: -0.27270593001589843\n",
      "22: reward: 0.0, discounted reward: -0.2717333424137169\n",
      "23: reward: 0.0, discounted reward: -0.27070956599036794\n",
      "24: reward: 0.0, discounted reward: -0.269631906597369\n",
      "25: reward: 0.0, discounted reward: -0.26849752828894907\n",
      "26: reward: 0.0, discounted reward: -0.26730344585903343\n",
      "27: reward: 0.0, discounted reward: -0.2660465169854379\n",
      "28: reward: 0.0, discounted reward: -0.2647234339606006\n",
      "29: reward: 0.0, discounted reward: -0.2633307149870876\n",
      "30: reward: 0.0, discounted reward: -0.2618646950149686\n",
      "31: reward: 0.0, discounted reward: -0.2603215160969487\n",
      "32: reward: 0.0, discounted reward: -0.2586971172358751\n",
      "33: reward: 0.0, discounted reward: -0.2569872236979029\n",
      "34: reward: 0.0, discounted reward: -0.2551873357631953\n",
      "35: reward: 0.0, discounted reward: -0.2532927168845557\n",
      "36: reward: 0.0, discounted reward: -0.25129838122282977\n",
      "37: reward: 0.0, discounted reward: -0.24919908052627623\n",
      "38: reward: 0.0, discounted reward: -0.24698929031937775\n",
      "39: reward: 0.0, discounted reward: -0.24466319536474776\n",
      "40: reward: 0.0, discounted reward: -0.2422146743598741\n",
      "41: reward: 0.0, discounted reward: -0.23963728382842814\n",
      "42: reward: 0.0, discounted reward: -0.23692424116374816\n",
      "43: reward: 0.0, discounted reward: -0.23406840677987453\n",
      "44: reward: 0.0, discounted reward: -0.23106226532316546\n",
      "45: reward: 0.0, discounted reward: -0.22789790589505057\n",
      "46: reward: 0.0, discounted reward: -0.22456700123387707\n",
      "47: reward: 0.0, discounted reward: -0.22106078580106284\n",
      "48: reward: 0.0, discounted reward: -0.21737003271388997\n",
      "49: reward: 0.0, discounted reward: -0.2134850294642343\n",
      "50: reward: 0.0, discounted reward: -0.2093955523593336\n",
      "51: reward: 0.0, discounted reward: -0.20509083961733288\n",
      "52: reward: 0.0, discounted reward: -0.20055956304680578\n",
      "53: reward: 0.0, discounted reward: -0.19578979823572465\n",
      "54: reward: 0.0, discounted reward: -0.19076899317142873\n",
      "55: reward: 0.0, discounted reward: -0.18548393520901194\n",
      "56: reward: 0.0, discounted reward: -0.17992071630120482\n",
      "57: reward: 0.0, discounted reward: -0.17406469639824992\n",
      "58: reward: 0.0, discounted reward: -0.16790046492145533\n",
      "59: reward: 0.0, discounted reward: -0.16141180020903995\n",
      "60: reward: 0.0, discounted reward: -0.15458162682755006\n",
      "61: reward: 0.0, discounted reward: -0.1473919706365081\n",
      "62: reward: 0.0, discounted reward: -0.13982391148804288\n",
      "63: reward: 0.0, discounted reward: -0.13185753343702683\n",
      "64: reward: 0.0, discounted reward: -0.12347187233069418\n",
      "65: reward: 0.0, discounted reward: -0.11464486063981769\n",
      "66: reward: 0.0, discounted reward: -0.10535326938626348\n",
      "67: reward: 0.0, discounted reward: -0.09557264701410118\n",
      "68: reward: 0.0, discounted reward: -0.08527725504340401\n",
      "69: reward: 0.0, discounted reward: -0.07444000033740698\n",
      "70: reward: 0.0, discounted reward: -0.06303236380477854\n",
      "71: reward: 0.0, discounted reward: -0.05102432534938018\n",
      "72: reward: 0.0, discounted reward: -0.03838428487001347\n",
      "73: reward: 0.0, discounted reward: -0.02507897910225905\n",
      "74: reward: 0.0, discounted reward: -0.011073394083570174\n",
      "75: reward: 0.0, discounted reward: 0.003669326988733898\n",
      "76: reward: 0.0, discounted reward: 0.01918798074905398\n",
      "77: reward: 0.0, discounted reward: 0.03552340575991723\n",
      "78: reward: 0.0, discounted reward: 0.05271858998187854\n",
      "79: reward: 0.0, discounted reward: 0.07081878389973253\n",
      "80: reward: 0.0, discounted reward: 0.08987161960273679\n",
      "81: reward: 0.0, discounted reward: 0.10992723613221492\n",
      "82: reward: 0.0, discounted reward: 0.13103841142640246\n",
      "83: reward: 0.0, discounted reward: 0.1532607012097577\n",
      "84: reward: 0.0, discounted reward: 0.17665258519223695\n",
      "85: reward: 0.0, discounted reward: 0.2012756209632677\n",
      "86: reward: 0.0, discounted reward: 0.22719460598540533\n",
      "87: reward: 0.0, discounted reward: 0.2544777481139713\n",
      "88: reward: 0.0, discounted reward: 0.2831968450914091\n",
      "89: reward: 0.0, discounted reward: 0.313427473488712\n",
      "90: reward: 0.0, discounted reward: 0.3452491875911362\n",
      "91: reward: 0.0, discounted reward: 0.37874572875158274\n",
      "92: reward: 0.0, discounted reward: 0.41400524576257913\n",
      "93: reward: 0.0, discounted reward: 0.4511205268267857\n",
      "94: reward: 0.0, discounted reward: 0.49018924373647704\n",
      "95: reward: 0.0, discounted reward: 0.531314208904573\n",
      "96: reward: 0.0, discounted reward: 0.5746036459236215\n",
      "97: reward: 0.0, discounted reward: 0.6201714743647252\n",
      "98: reward: 0.0, discounted reward: 0.668137609565887\n",
      "99: reward: 0.0, discounted reward: 0.7186282781986887\n",
      "100: reward: 0.0, discounted reward: 0.7717763504437434\n",
      "101: reward: 0.0, discounted reward: 0.827721689649064\n",
      "102: reward: 0.0, discounted reward: 0.8866115203915067\n",
      "103: reward: 0.0, discounted reward: 0.9486008159098674\n",
      "104: reward: 0.0, discounted reward: 1.0138527059291946\n",
      "105: reward: 0.0, discounted reward: 1.082538905949539\n",
      "106: reward: 0.0, discounted reward: 1.1548401691288488\n",
      "107: reward: 0.0, discounted reward: 1.2309467619491752\n",
      "108: reward: 0.0, discounted reward: 1.3110589649179396\n",
      "109: reward: 0.0, discounted reward: 1.395387599621902\n",
      "110: reward: 0.0, discounted reward: 1.4841545835208103\n",
      "111: reward: 0.0, discounted reward: 1.5775935139407131\n",
      "112: reward: 0.0, discounted reward: 1.675950282803769\n",
      "113: reward: 0.0, discounted reward: 1.779483723712249\n",
      "114: reward: 0.0, discounted reward: 1.8884662930895963\n",
      "115: reward: 0.0, discounted reward: 2.0031847871710147\n",
      "116: reward: 0.0, discounted reward: 2.1239410967304018\n",
      "117: reward: 0.0, discounted reward: 2.2510530015297574\n",
      "118: reward: 0.0, discounted reward: 2.3848550065817102\n",
      "119: reward: 0.0, discounted reward: 2.5256992224258714\n",
      "120: reward: 0.0, discounted reward: 2.6739562917355144\n",
      "121: reward: 5.0, discounted reward: 2.830016364693033\n",
      "122: reward: 0.0, discounted reward: 1.1827890301533963\n",
      "123: reward: 0.0, discounted reward: 1.260366615659225\n",
      "124: reward: 0.0, discounted reward: 1.3420272319811501\n",
      "125: reward: 0.0, discounted reward: 1.4279857754779133\n",
      "126: reward: 0.0, discounted reward: 1.5184684528429269\n",
      "127: reward: 0.0, discounted reward: 1.6137133763850466\n",
      "128: reward: 0.0, discounted reward: 1.7139711906399095\n",
      "129: reward: 0.0, discounted reward: 1.8195057319608179\n",
      "130: reward: 0.0, discounted reward: 1.9305947228249318\n",
      "131: reward: 5.0, discounted reward: 2.047530502681894\n",
      "132: reward: 0.0, discounted reward: 0.35911970172061797\n",
      "133: reward: 0.0, discounted reward: 0.393346269940511\n",
      "134: reward: 0.0, discounted reward: 0.4293742364877667\n",
      "135: reward: 0.0, discounted reward: 0.4672984118006675\n",
      "136: reward: 0.0, discounted reward: 0.507218596340563\n",
      "137: reward: 0.0, discounted reward: 0.5492398432246636\n",
      "138: reward: 0.0, discounted reward: 0.5934727346816115\n",
      "139: reward: 0.0, discounted reward: 0.6400336730573462\n",
      "140: reward: 0.0, discounted reward: 0.689045187137067\n",
      "141: reward: 0.0, discounted reward: 0.7406362545894045\n",
      "142: reward: 0.0, discounted reward: 0.7949426413813389\n",
      "143: reward: 0.0, discounted reward: 0.8521072590570593\n",
      "144: reward: 0.0, discounted reward: 0.9122805408209755\n",
      "145: reward: 0.0, discounted reward: 0.9756208374145715\n",
      "146: reward: 0.0, discounted reward: 1.042294833828883\n",
      "147: reward: 0.0, discounted reward: 1.112477987949211\n",
      "148: reward: 0.0, discounted reward: 1.1863549922863985\n",
      "149: reward: 0.0, discounted reward: 1.2641202600097536\n",
      "150: reward: 0.0, discounted reward: 1.3459784365606537\n",
      "151: reward: 0.0, discounted reward: 1.4321449381931803\n",
      "152: reward: 0.0, discounted reward: 1.5228465188589975\n",
      "153: reward: 0.0, discounted reward: 1.618321866928279\n",
      "154: reward: 0.0, discounted reward: 1.7188222333169962\n",
      "155: reward: 0.0, discounted reward: 1.8246120926735405\n",
      "156: reward: 0.0, discounted reward: 1.93596983936464\n",
      "157: reward: 0.0, discounted reward: 2.053188520092113\n",
      "158: reward: 0.0, discounted reward: 2.1765766050684006\n",
      "159: reward: 0.0, discounted reward: 2.3064587997802826\n",
      "160: reward: 0.0, discounted reward: 2.443176899477\n",
      "161: reward: 0.0, discounted reward: 2.5870906886314393\n",
      "162: reward: 0.0, discounted reward: 2.738578887741376\n",
      "163: reward: 0.0, discounted reward: 2.898040149962362\n",
      "164: reward: 5.0, discounted reward: 3.0658941101949786\n",
      "165: reward: 0.0, discounted reward: 1.4310813938396547\n",
      "166: reward: 0.0, discounted reward: 1.5217269984868655\n",
      "167: reward: 0.0, discounted reward: 1.6171434244312977\n",
      "168: reward: 0.0, discounted reward: 1.7175817675307004\n",
      "169: reward: 0.0, discounted reward: 1.8233063392142819\n",
      "170: reward: 0.0, discounted reward: 1.9345953620391045\n",
      "171: reward: 0.0, discounted reward: 2.0517417018547075\n",
      "172: reward: 0.0, discounted reward: 2.1750536385027104\n",
      "173: reward: 0.0, discounted reward: 2.304855677079556\n",
      "174: reward: 0.0, discounted reward: 2.4414894018972877\n",
      "175: reward: 5.0, discounted reward: 2.585314375389637\n",
      "176: reward: 0.0, discounted reward: 0.9252079887814002\n",
      "177: reward: 0.0, discounted reward: 0.9892286773729133\n",
      "178: reward: 0.0, discounted reward: 1.0566188758902957\n",
      "179: reward: 0.0, discounted reward: 1.1275559269612243\n",
      "180: reward: 0.0, discounted reward: 1.202226507035886\n",
      "181: reward: 0.0, discounted reward: 1.280827117640793\n",
      "182: reward: 0.0, discounted reward: 1.3635646024880637\n",
      "183: reward: 0.0, discounted reward: 1.4506566918009802\n",
      "184: reward: 0.0, discounted reward: 1.5423325752882608\n",
      "185: reward: 0.0, discounted reward: 1.6388335052748717\n",
      "186: reward: 0.0, discounted reward: 1.7404134315765676\n",
      "187: reward: 0.0, discounted reward: 1.847339669788879\n",
      "188: reward: 0.0, discounted reward: 1.959893604749207\n",
      "189: reward: 0.0, discounted reward: 2.078371431023236\n",
      "190: reward: 0.0, discounted reward: 2.2030849323643196\n",
      "191: reward: 5.0, discounted reward: 2.3343623021970386\n",
      "192: reward: 0.0, discounted reward: 0.6610479117365595\n",
      "193: reward: 0.0, discounted reward: 0.7111654383783441\n",
      "194: reward: 0.0, discounted reward: 0.7639207295802226\n",
      "195: reward: 0.0, discounted reward: 0.8194526150558842\n",
      "196: reward: 0.0, discounted reward: 0.8779072313460544\n",
      "197: reward: 0.0, discounted reward: 0.9394384063883388\n",
      "198: reward: 0.0, discounted reward: 1.0042080643275855\n",
      "199: reward: 0.0, discounted reward: 1.0723866516320557\n",
      "200: reward: 0.0, discounted reward: 1.1441535856367613\n",
      "201: reward: 0.0, discounted reward: 1.219697726694346\n",
      "202: reward: 0.0, discounted reward: 1.2992178751760142\n",
      "203: reward: 0.0, discounted reward: 1.3829232946304018\n",
      "204: reward: 0.0, discounted reward: 1.4710342624771255\n",
      "205: reward: 0.0, discounted reward: 1.5637826496842029\n",
      "206: reward: 0.0, discounted reward: 1.661412530954811\n",
      "207: reward: 0.0, discounted reward: 1.764180827029135\n",
      "208: reward: 0.0, discounted reward: 1.8723579807915818\n",
      "209: reward: 5.0, discounted reward: 1.9862286689625779\n",
      "210: reward: 0.0, discounted reward: 0.29459145570028533\n",
      "211: reward: 0.0, discounted reward: 0.3254218004454239\n",
      "212: reward: 0.0, discounted reward: 0.3578747949139908\n",
      "213: reward: 0.0, discounted reward: 0.39203584172300876\n",
      "214: reward: 0.0, discounted reward: 0.4279948383640801\n",
      "215: reward: 0.0, discounted reward: 0.4658464137757342\n",
      "216: reward: 0.0, discounted reward: 0.505690177366949\n",
      "217: reward: 0.0, discounted reward: 0.5476309811471751\n",
      "218: reward: 0.0, discounted reward: 0.5917791956526763\n",
      "219: reward: 0.0, discounted reward: 0.6382510003953092\n",
      "220: reward: 0.0, discounted reward: 0.6871686895980808\n",
      "221: reward: 0.0, discounted reward: 0.7386609940220507\n",
      "222: reward: 0.0, discounted reward: 0.7928634197314927\n",
      "223: reward: 0.0, discounted reward: 0.8499186046888001\n",
      "224: reward: 0.0, discounted reward: 0.9099766941175447\n",
      "225: reward: 0.0, discounted reward: 0.9731957356214865\n",
      "226: reward: 0.0, discounted reward: 1.03974209509932\n",
      "227: reward: 0.0, discounted reward: 1.1097908945496708\n",
      "228: reward: 0.0, discounted reward: 1.1835264729184611\n",
      "229: reward: 0.0, discounted reward: 1.2611428712013988\n",
      "230: reward: 0.0, discounted reward: 1.342844343078175\n",
      "231: reward: 0.0, discounted reward: 1.4288458924221499\n",
      "232: reward: 5.0, discounted reward: 1.5193738391000182\n",
      "233: reward: 0.0, discounted reward: -0.1968346809971461\n",
      "234: reward: 0.0, discounted reward: -0.19186886976239861\n",
      "235: reward: 0.0, discounted reward: -0.18664170004161185\n",
      "236: reward: 0.0, discounted reward: -0.18113941612499418\n",
      "237: reward: 0.0, discounted reward: -0.1753475383180282\n",
      "238: reward: 0.0, discounted reward: -0.16925082483701143\n",
      "239: reward: 0.0, discounted reward: -0.162833231699099\n",
      "240: reward: 0.0, discounted reward: -0.15607787050129646\n",
      "241: reward: 0.0, discounted reward: -0.14896696397729378\n",
      "242: reward: 0.0, discounted reward: -0.1414817992151857\n",
      "243: reward: 0.0, discounted reward: -0.13360267841296666\n",
      "244: reward: 0.0, discounted reward: -0.12530886704220975\n",
      "245: reward: 0.0, discounted reward: -0.1165785392835183\n",
      "246: reward: 0.0, discounted reward: -0.10738872059015886\n",
      "247: reward: 0.0, discounted reward: -0.09771522722872789\n",
      "248: reward: 0.0, discounted reward: -0.0875326026377479\n",
      "249: reward: 0.0, discounted reward: -0.07681405043671632\n",
      "250: reward: 0.0, discounted reward: -0.06553136390931467\n",
      "251: reward: 0.0, discounted reward: -0.05365485177520769\n",
      "252: reward: 0.0, discounted reward: -0.04115326005509506\n",
      "253: reward: 0.0, discounted reward: -0.027993689823397554\n",
      "254: reward: 0.0, discounted reward: -0.014141510632137026\n",
      "255: reward: 0.0, discounted reward: 0.00043973062182141326\n",
      "256: reward: 0.0, discounted reward: 0.01578840562598819\n",
      "257: reward: 0.0, discounted reward: 0.031944905630374304\n",
      "258: reward: 0.0, discounted reward: 0.04895174774025442\n",
      "259: reward: 0.0, discounted reward: 0.0668536868032861\n",
      "260: reward: 0.0, discounted reward: 0.08569783318542472\n",
      "261: reward: 0.0, discounted reward: 0.10553377674557064\n",
      "262: reward: 0.0, discounted reward: 0.12641371733519793\n",
      "263: reward: 0.0, discounted reward: 0.1483926021663845\n",
      "264: reward: 0.0, discounted reward: 0.17152827040973886\n",
      "265: reward: 0.0, discounted reward: 0.19588160540274338\n",
      "266: reward: 0.0, discounted reward: 0.22151669486906397\n",
      "267: reward: 0.0, discounted reward: 0.24850099957045407\n",
      "268: reward: 0.0, discounted reward: 0.27690553083507524\n",
      "269: reward: 0.0, discounted reward: 0.30680503742941334\n",
      "270: reward: 0.0, discounted reward: 0.33827820226555866\n",
      "271: reward: 0.0, discounted reward: 0.37140784946150107\n",
      "272: reward: 0.0, discounted reward: 0.40628116229933525\n",
      "273: reward: 0.0, discounted reward: 0.4429899126549501\n",
      "274: reward: 0.0, discounted reward: 0.4816307025029658\n",
      "275: reward: 0.0, discounted reward: 0.5223052181324561\n",
      "276: reward: 0.0, discounted reward: 0.5651204977424458\n",
      "277: reward: 0.0, discounted reward: 0.6101892131213823\n",
      "278: reward: 0.0, discounted reward: 0.6576299661518418\n",
      "279: reward: 0.0, discounted reward: 0.7075676009207466\n",
      "280: reward: 0.0, discounted reward: 0.7601335322564359\n",
      "281: reward: 0.0, discounted reward: 0.8154660915571613\n",
      "282: reward: 0.0, discounted reward: 0.8737108908210829\n",
      "283: reward: 0.0, discounted reward: 0.9350212058357372\n",
      "284: reward: 0.0, discounted reward: 0.9995583795353734\n",
      "285: reward: 0.0, discounted reward: 1.067492246587622\n",
      "286: reward: 0.0, discounted reward: 1.139001580326831\n",
      "287: reward: 0.0, discounted reward: 1.2142745632102088\n",
      "288: reward: 0.0, discounted reward: 1.293509282034817\n",
      "289: reward: 0.0, discounted reward: 1.3769142492186153\n",
      "290: reward: 5.0, discounted reward: 1.4647089515173501\n",
      "291: reward: 0.0, discounted reward: -0.2543766679262703\n",
      "292: reward: 0.0, discounted reward: -0.25243938231937146\n",
      "293: reward: 0.0, discounted reward: -0.25040013431210956\n",
      "294: reward: 0.0, discounted reward: -0.24825355746236025\n",
      "295: reward: 0.0, discounted reward: -0.24599400288367668\n",
      "296: reward: 0.0, discounted reward: -0.24361552437979928\n",
      "297: reward: 0.0, discounted reward: -0.24111186279677044\n",
      "298: reward: 0.0, discounted reward: -0.2384764295514769\n",
      "299: reward: 0.0, discounted reward: -0.23570228929327322\n",
      "300: reward: 0.0, discounted reward: -0.23278214165305877\n",
      "301: reward: 0.0, discounted reward: -0.22970830203178044\n",
      "302: reward: 0.0, discounted reward: -0.22647268137780321\n",
      "303: reward: 0.0, discounted reward: -0.2230667648999325\n",
      "304: reward: 0.0, discounted reward: -0.2194815896600685\n",
      "305: reward: 0.0, discounted reward: -0.21570772098652752\n",
      "306: reward: 0.0, discounted reward: -0.21173522764595806\n",
      "307: reward: 0.0, discounted reward: -0.20755365570851653\n",
      "308: reward: 0.0, discounted reward: -0.2031520010375254\n",
      "309: reward: 0.0, discounted reward: -0.19851868033121897\n",
      "310: reward: 0.0, discounted reward: -0.19364150064037008\n",
      "311: reward: 0.0, discounted reward: -0.1885076272815818\n",
      "312: reward: 0.0, discounted reward: -0.18310355006180468\n",
      "313: reward: 0.0, discounted reward: -0.17741504772519712\n",
      "314: reward: 0.0, discounted reward: -0.17142715052876817\n",
      "315: reward: 0.0, discounted reward: -0.16512410084831664\n",
      "316: reward: 0.0, discounted reward: -0.15848931171099923\n",
      "317: reward: 0.0, discounted reward: -0.15150532314540194\n",
      "318: reward: 0.0, discounted reward: -0.14415375623424692\n",
      "319: reward: 0.0, discounted reward: -0.13641526474882057\n",
      "320: reward: 0.0, discounted reward: -0.12826948423784548\n",
      "321: reward: 0.0, discounted reward: -0.11969497843681905\n",
      "322: reward: 0.0, discounted reward: -0.11066918285679123\n",
      "323: reward: 0.0, discounted reward: -0.10116834540413037\n",
      "324: reward: 0.0, discounted reward: -0.09116746387501368\n",
      "325: reward: 0.0, discounted reward: -0.080640220160154\n",
      "326: reward: 0.0, discounted reward: -0.06955891098661748\n",
      "327: reward: 0.0, discounted reward: -0.057894375014473816\n",
      "328: reward: 0.0, discounted reward: -0.04561591609642781\n",
      "329: reward: 0.0, discounted reward: -0.032691222498484676\n",
      "330: reward: 0.0, discounted reward: -0.019086281869070857\n",
      "331: reward: 0.0, discounted reward: -0.004765291732845772\n",
      "332: reward: 0.0, discounted reward: 0.010309434726338542\n",
      "333: reward: 0.0, discounted reward: 0.026177567841269414\n",
      "334: reward: 0.0, discounted reward: 0.04288086585698609\n",
      "335: reward: 0.0, discounted reward: 0.0604632848208984\n",
      "336: reward: 0.0, discounted reward: 0.07897109425659557\n",
      "337: reward: 0.0, discounted reward: 0.09845299892575046\n",
      "338: reward: 0.0, discounted reward: 0.1189602669985451\n",
      "339: reward: 0.0, discounted reward: 0.1405468649699079\n",
      "340: reward: 0.0, discounted reward: 0.16326959967660554\n",
      "341: reward: 0.0, discounted reward: 0.1871882677889189\n",
      "342: reward: 0.0, discounted reward: 0.21236581317030134\n",
      "343: reward: 0.0, discounted reward: 0.23886849251912495\n",
      "344: reward: 0.0, discounted reward: 0.266766049728413\n",
      "345: reward: 0.0, discounted reward: 0.29613189942240037\n",
      "346: reward: 0.0, discounted reward: 0.32704332015291343\n",
      "347: reward: 0.0, discounted reward: 0.3595816577639798\n",
      "348: reward: 0.0, discounted reward: 0.3938325394598392\n",
      "349: reward: 0.0, discounted reward: 0.42988609913969117\n",
      "350: reward: 0.0, discounted reward: 0.4678372145921669\n",
      "351: reward: 0.0, discounted reward: 0.5077857571737203\n",
      "352: reward: 0.0, discounted reward: 0.549836854627987\n",
      "353: reward: 0.0, discounted reward: 0.5941011677377415\n",
      "354: reward: 0.0, discounted reward: 0.6406951815374831\n",
      "355: reward: 0.0, discounted reward: 0.6897415118530005\n",
      "356: reward: 0.0, discounted reward: 0.7413692279745977\n",
      "357: reward: 0.0, discounted reward: 0.7957141923131211\n",
      "358: reward: 0.0, discounted reward: 0.8529194179326195\n",
      "359: reward: 0.0, discounted reward: 0.9131354449005126\n",
      "360: reward: 0.0, discounted reward: 0.9765207364456631\n",
      "361: reward: 0.0, discounted reward: 1.0432420959668742\n",
      "362: reward: 0.0, discounted reward: 1.1134751059892016\n",
      "363: reward: 0.0, discounted reward: 1.1874045902232309\n",
      "364: reward: 0.0, discounted reward: 1.2652250999432615\n",
      "365: reward: 0.0, discounted reward: 1.3471414259643462\n",
      "366: reward: 5.0, discounted reward: 1.433369137565488\n",
      "367: reward: 0.0, discounted reward: -0.28736594577033603\n",
      "368: reward: 0.0, discounted reward: -0.2871649379447038\n",
      "369: reward: 0.0, discounted reward: -0.28695335075982786\n",
      "370: reward: 0.0, discounted reward: -0.28673062740732685\n",
      "371: reward: 0.0, discounted reward: -0.28649618177311525\n",
      "372: reward: 0.0, discounted reward: -0.2862493968949978\n",
      "373: reward: 0.0, discounted reward: -0.2859896233390846\n",
      "374: reward: 0.0, discounted reward: -0.285716177490755\n",
      "375: reward: 0.0, discounted reward: -0.2854283397556712\n",
      "376: reward: 0.0, discounted reward: -0.2851253526661093\n",
      "377: reward: 0.0, discounted reward: -0.2848064188876231\n",
      "378: reward: 0.0, discounted reward: -0.2844706991207955\n",
      "379: reward: 0.0, discounted reward: -0.2841173098925559\n",
      "380: reward: 0.0, discounted reward: -0.28374532123125107\n",
      "381: reward: 0.0, discounted reward: -0.2833537542193513\n",
      "382: reward: 0.0, discounted reward: -0.2829415784173515\n",
      "383: reward: 0.0, discounted reward: -0.28250770915208856\n",
      "384: reward: 0.0, discounted reward: -0.2820510046623381\n",
      "385: reward: 0.0, discounted reward: -0.28157026309417976\n",
      "386: reward: 0.0, discounted reward: -0.28106421933822356\n",
      "387: reward: 0.0, discounted reward: -0.28053154170037486\n",
      "388: reward: 0.0, discounted reward: -0.27997082839737636\n",
      "389: reward: 0.0, discounted reward: -0.2793806038679042\n",
      "390: reward: 0.0, discounted reward: -0.27875931488951244\n",
      "391: reward: 0.0, discounted reward: -0.27810532649120534\n",
      "392: reward: 0.0, discounted reward: -0.27741691765088206\n",
      "393: reward: 0.0, discounted reward: -0.2766922767663313\n",
      "394: reward: 0.0, discounted reward: -0.27592949688785673\n",
      "395: reward: 0.0, discounted reward: -0.2751265706999888\n",
      "396: reward: 0.0, discounted reward: -0.2742813852390752\n",
      "397: reward: 0.0, discounted reward: -0.27339171633285037\n",
      "398: reward: 0.0, discounted reward: -0.2724552227473505\n",
      "399: reward: 0.0, discounted reward: -0.2714694400257717\n",
      "400: reward: 0.0, discounted reward: -0.27043177400305723\n",
      "401: reward: 0.0, discounted reward: -0.2693394939791472\n",
      "402: reward: 0.0, discounted reward: -0.26818972553292614\n",
      "403: reward: 0.0, discounted reward: -0.2669794429579566\n",
      "404: reward: 0.0, discounted reward: -0.2657054613000939\n",
      "405: reward: 0.0, discounted reward: -0.264364427976028\n",
      "406: reward: 0.0, discounted reward: -0.26295281395069536\n",
      "407: reward: 0.0, discounted reward: -0.26146690445034526\n",
      "408: reward: 0.0, discounted reward: -0.2599027891868188\n",
      "409: reward: 0.0, discounted reward: -0.25825635206731734\n",
      "410: reward: 0.0, discounted reward: -0.2565232603625789\n",
      "411: reward: 0.0, discounted reward: -0.2546989533049595\n",
      "412: reward: 0.0, discounted reward: -0.25277863008641277\n",
      "413: reward: 0.0, discounted reward: -0.25075723722478466\n",
      "414: reward: 0.0, discounted reward: -0.24862945526517607\n",
      "415: reward: 0.0, discounted reward: -0.24638968478137757\n",
      "416: reward: 0.0, discounted reward: -0.24403203164053708\n",
      "417: reward: 0.0, discounted reward: -0.24155029149228388\n",
      "418: reward: 0.0, discounted reward: -0.23893793344149108\n",
      "419: reward: 0.0, discounted reward: -0.23618808286170914\n",
      "420: reward: 0.0, discounted reward: -0.233293503304044\n",
      "421: reward: 0.0, discounted reward: -0.23024657745387012\n",
      "422: reward: 0.0, discounted reward: -0.22703928708526602\n",
      "423: reward: 0.0, discounted reward: -0.22366319196041967\n",
      "424: reward: 0.0, discounted reward: -0.2201094076184761\n",
      "425: reward: 0.0, discounted reward: -0.2163685819953776\n",
      "426: reward: 0.0, discounted reward: -0.21243087081316864\n",
      "427: reward: 0.0, discounted reward: -0.20828591167400137\n",
      "428: reward: 0.0, discounted reward: -0.20392279679066733\n",
      "429: reward: 0.0, discounted reward: -0.1993300442818947\n",
      "430: reward: 0.0, discounted reward: -0.19449556795687092\n",
      "431: reward: 0.0, discounted reward: -0.1894066455094774\n",
      "432: reward: 0.0, discounted reward: -0.18404988503853686\n",
      "433: reward: 0.0, discounted reward: -0.1784111898059679\n",
      "434: reward: 0.0, discounted reward: -0.17247572114010581\n",
      "435: reward: 0.0, discounted reward: -0.16622785938656678\n",
      "436: reward: 0.0, discounted reward: -0.1596511628038941\n",
      "437: reward: 0.0, discounted reward: -0.15272832429581762\n",
      "438: reward: 0.0, discounted reward: -0.1454411258662634\n",
      "439: reward: 0.0, discounted reward: -0.13777039067725896\n",
      "440: reward: 0.0, discounted reward: -0.1296959325835701\n",
      "441: reward: 0.0, discounted reward: -0.12119650301126601\n",
      "442: reward: 0.0, discounted reward: -0.11224973504041962\n",
      "443: reward: 0.0, discounted reward: -0.10283208454479183\n",
      "444: reward: 0.0, discounted reward: -0.09291876823360469\n",
      "445: reward: 0.0, discounted reward: -0.08248369843235506\n",
      "446: reward: 0.0, discounted reward: -0.07149941443103966\n",
      "447: reward: 0.0, discounted reward: -0.059937010219128735\n",
      "448: reward: 0.0, discounted reward: -0.04776605841711724\n",
      "449: reward: 0.0, discounted reward: -0.03495453020447353\n",
      "450: reward: 0.0, discounted reward: -0.02146871103326963\n",
      "451: reward: 0.0, discounted reward: -0.0072731119056865775\n",
      "452: reward: 0.0, discounted reward: 0.007669624018085071\n",
      "453: reward: 0.0, discounted reward: 0.023398819727318353\n",
      "454: reward: 0.0, discounted reward: 0.039955867842300756\n",
      "455: reward: 0.0, discounted reward: 0.05738433954228226\n",
      "456: reward: 0.0, discounted reward: 0.0757300992264733\n",
      "457: reward: 0.0, discounted reward: 0.09504142520983233\n",
      "458: reward: 0.0, discounted reward: 0.11536913677126286\n",
      "459: reward: 0.0, discounted reward: 0.13676672788855812\n",
      "460: reward: 0.0, discounted reward: 0.15929050801202685\n",
      "461: reward: 0.0, discounted reward: 0.1829997502472571\n",
      "462: reward: 0.0, discounted reward: 0.20795684733697312\n",
      "463: reward: 0.0, discounted reward: 0.23422747585246373\n",
      "464: reward: 0.0, discounted reward: 0.2618807690266643\n",
      "465: reward: 0.0, discounted reward: 0.29098949868371754\n",
      "466: reward: 0.0, discounted reward: 0.32163026674377365\n",
      "467: reward: 0.0, discounted reward: 0.3538837068069906\n",
      "468: reward: 0.0, discounted reward: 0.38783469634721884\n",
      "469: reward: 0.0, discounted reward: 0.4235725800737751\n",
      "470: reward: 0.0, discounted reward: 0.4611914050490974\n",
      "471: reward: 0.0, discounted reward: 0.5007901681810155\n",
      "472: reward: 0.0, discounted reward: 0.5424730767409294\n",
      "473: reward: 0.0, discounted reward: 0.5863498225934703\n",
      "474: reward: 0.0, discounted reward: 0.632535870859303\n",
      "475: reward: 0.0, discounted reward: 0.6811527637707057\n",
      "476: reward: 0.0, discounted reward: 0.7323284405195505\n",
      "477: reward: 0.0, discounted reward: 0.7861975739393873\n",
      "478: reward: 0.0, discounted reward: 0.8429019249076365\n",
      "479: reward: 0.0, discounted reward: 0.9025907154005305\n",
      "480: reward: 0.0, discounted reward: 0.9654210211825242\n",
      "481: reward: 0.0, discounted reward: 1.0315581851635702\n",
      "482: reward: 0.0, discounted reward: 1.1011762525120394\n",
      "483: reward: 0.0, discounted reward: 1.174458428668323\n",
      "484: reward: 0.0, discounted reward: 1.251597561464411\n",
      "485: reward: 0.0, discounted reward: 1.332796648618188\n",
      "486: reward: 0.0, discounted reward: 1.418269371937953\n",
      "487: reward: 0.0, discounted reward: 1.508240659642969\n",
      "488: reward: 0.0, discounted reward: 1.6029472782798277\n",
      "489: reward: 0.0, discounted reward: 1.7026384557923107\n",
      "490: reward: 0.0, discounted reward: 1.8075765373843982\n",
      "491: reward: 0.0, discounted reward: 1.9180376759023847\n",
      "492: reward: 0.0, discounted reward: 2.0343125585528967\n",
      "493: reward: 5.0, discounted reward: 2.1567071718692254\n",
      "494: reward: 0.0, discounted reward: 0.47404251139149317\n",
      "495: reward: 0.0, discounted reward: 0.5143176485414321\n",
      "496: reward: 0.0, discounted reward: 0.5567125297518942\n",
      "497: reward: 0.0, discounted reward: 0.6013387204997491\n",
      "498: reward: 0.0, discounted reward: 0.64831365812907\n",
      "499: reward: 0.0, discounted reward: 0.6977609608967763\n",
      "500: reward: 0.0, discounted reward: 0.7498107532838355\n",
      "501: reward: 0.0, discounted reward: 0.8046000084281083\n",
      "502: reward: 0.0, discounted reward: 0.8622729085799744\n",
      "503: reward: 0.0, discounted reward: 0.9229812245293073\n",
      "504: reward: 0.0, discounted reward: 0.9868847150022891\n",
      "505: reward: 0.0, discounted reward: 1.0541515470791123\n",
      "506: reward: 0.0, discounted reward: 1.124958738738926\n",
      "507: reward: 0.0, discounted reward: 1.1994926246966247\n",
      "508: reward: 0.0, discounted reward: 1.27794934675736\n",
      "509: reward: 0.0, discounted reward: 1.3605353699791867\n",
      "510: reward: 5.0, discounted reward: 1.4474680260021622\n",
      "511: reward: 0.0, discounted reward: -0.27252501057383677\n",
      "512: reward: 0.0, discounted reward: -0.27154290089575733\n",
      "513: reward: 0.0, discounted reward: -0.27050910123462096\n",
      "514: reward: 0.0, discounted reward: -0.26942089106500383\n",
      "515: reward: 0.0, discounted reward: -0.2682754066759331\n",
      "516: reward: 0.0, discounted reward: -0.26706963363480607\n",
      "517: reward: 0.0, discounted reward: -0.26580039885467227\n",
      "518: reward: 0.0, discounted reward: -0.26446436224400516\n",
      "519: reward: 0.0, discounted reward: -0.26305800791698714\n",
      "520: reward: 0.0, discounted reward: -0.26157763494117875\n",
      "521: reward: 0.0, discounted reward: -0.26001934759822243\n",
      "522: reward: 0.0, discounted reward: -0.25837904513195276\n",
      "523: reward: 0.0, discounted reward: -0.256652410956932\n",
      "524: reward: 0.0, discounted reward: -0.25483490129901537\n",
      "525: reward: 0.0, discounted reward: -0.2529217332380505\n",
      "526: reward: 0.0, discounted reward: -0.25090787212124543\n",
      "527: reward: 0.0, discounted reward: -0.24878801831408215\n",
      "528: reward: 0.0, discounted reward: -0.2465565932539103\n",
      "529: reward: 0.0, discounted reward: -0.24420772476951885\n",
      "530: reward: 0.0, discounted reward: -0.24173523162805421\n",
      "531: reward: 5.0, discounted reward: -0.23913260726861774\n",
      "532: reward: 0.0, discounted reward: -2.0478940982272893\n",
      "533: reward: 0.0, discounted reward: -2.14035246684676\n",
      "534: reward: 0.0, discounted reward: -2.237677065393571\n",
      "535: reward: 0.0, discounted reward: -2.34012401123232\n",
      "536: reward: 0.0, discounted reward: -2.4479629015888973\n",
      "537: reward: 0.0, discounted reward: -2.561477523016874\n",
      "538: reward: 0.0, discounted reward: -2.680966598204217\n",
      "539: reward: 0.0, discounted reward: -2.806744572085631\n",
      "540: reward: -10.0, discounted reward: -2.9391424393292254\n",
      "541: reward: 0.0, discounted reward: 0.5444935757199904\n",
      "542: reward: 0.0, discounted reward: 0.5884766636240608\n",
      "543: reward: 0.0, discounted reward: 0.6347746508915034\n",
      "544: reward: 0.0, discounted reward: 0.6835093743309166\n",
      "545: reward: 0.0, discounted reward: 0.7348090832145097\n",
      "546: reward: 0.0, discounted reward: 0.7888087767761863\n",
      "547: reward: 0.0, discounted reward: 0.8456505594726881\n",
      "548: reward: 0.0, discounted reward: 0.9054840149426899\n",
      "549: reward: 0.0, discounted reward: 0.9684665996479551\n",
      "550: reward: 0.0, discounted reward: 1.0347640572324448\n",
      "551: reward: 0.0, discounted reward: 1.1045508546898024\n",
      "552: reward: 0.0, discounted reward: 1.1780106414870208\n",
      "553: reward: 0.0, discounted reward: 1.255336732852514\n",
      "554: reward: 0.0, discounted reward: 1.3367326185004014\n",
      "555: reward: 5.0, discounted reward: 1.4224124981297566\n",
      "556: reward: 0.0, discounted reward: -0.29889925043952675\n",
      "557: reward: 0.0, discounted reward: -0.29930525864911517\n",
      "558: reward: 0.0, discounted reward: -0.2997326357118398\n",
      "559: reward: 0.0, discounted reward: -0.3001825063041815\n",
      "560: reward: 0.0, discounted reward: -0.3006560542961201\n",
      "561: reward: 0.0, discounted reward: -0.3011545258665818\n",
      "562: reward: 0.0, discounted reward: -0.3016792327828573\n",
      "563: reward: 0.0, discounted reward: -0.302231555852621\n",
      "564: reward: 0.0, discounted reward: -0.3028129485576354\n",
      "565: reward: 0.0, discounted reward: -0.3034249408787032\n",
      "566: reward: 0.0, discounted reward: -0.30406914332193247\n",
      "567: reward: 0.0, discounted reward: -0.30474725115691065\n",
      "568: reward: 0.0, discounted reward: -0.3054610488779403\n",
      "569: reward: 0.0, discounted reward: -0.3062124149000767\n",
      "570: reward: 0.0, discounted reward: -0.30700332650232565\n",
      "571: reward: 0.0, discounted reward: -0.30783586503100874\n",
      "572: reward: 0.0, discounted reward: -0.3087122213769909\n",
      "573: reward: 0.0, discounted reward: -0.3096347017411827\n",
      "574: reward: 0.0, discounted reward: -0.3106057337034898\n",
      "575: reward: 0.0, discounted reward: -0.31162787261118147\n",
      "576: reward: 0.0, discounted reward: -0.31270380830348854\n",
      "577: reward: 0.0, discounted reward: -0.31383637219012755\n",
      "578: reward: 0.0, discounted reward: -0.3150285447023792\n",
      "579: reward: 0.0, discounted reward: -0.3162834631363282\n",
      "580: reward: 0.0, discounted reward: -0.31760442990890614\n",
      "581: reward: 0.0, discounted reward: -0.31899492124846185\n",
      "582: reward: 0.0, discounted reward: -0.32045859634273105\n",
      "583: reward: 0.0, discounted reward: -0.32199930696827755\n",
      "584: reward: 0.0, discounted reward: -0.32362110762674756\n",
      "585: reward: 0.0, discounted reward: -0.32532826621461075\n",
      "586: reward: 0.0, discounted reward: -0.3271252752544667\n",
      "587: reward: 0.0, discounted reward: -0.32901686371747296\n",
      "588: reward: 0.0, discounted reward: -0.3310080094680059\n",
      "589: reward: 0.0, discounted reward: -0.33310395236330376\n",
      "590: reward: 0.0, discounted reward: -0.3353102080425646\n",
      "591: reward: 0.0, discounted reward: -0.33763258244178657\n",
      "592: reward: 0.0, discounted reward: -0.3400771870725465\n",
      "593: reward: 0.0, discounted reward: -0.3426504551049255\n",
      "594: reward: 0.0, discounted reward: -0.3453591582969032\n",
      "595: reward: 0.0, discounted reward: -0.34821042481477466\n",
      "596: reward: 0.0, discounted reward: -0.3512117579914813\n",
      "597: reward: 0.0, discounted reward: -0.3543710560722252\n",
      "598: reward: 0.0, discounted reward: -0.3576966329993241\n",
      "599: reward: 0.0, discounted reward: -0.36119724029100697\n",
      "600: reward: 0.0, discounted reward: -0.36488209007172595\n",
      "601: reward: 0.0, discounted reward: -0.36876087931458795\n",
      "602: reward: 0.0, discounted reward: -0.3728438153597059\n",
      "603: reward: 0.0, discounted reward: -0.37714164277561957\n",
      "604: reward: 0.0, discounted reward: -0.381665671634476\n",
      "605: reward: 0.0, discounted reward: -0.38642780727537746\n",
      "606: reward: 0.0, discounted reward: -0.39144058163422113\n",
      "607: reward: 0.0, discounted reward: -0.3967171862224777\n",
      "608: reward: 0.0, discounted reward: -0.4022715068416951\n",
      "609: reward: 0.0, discounted reward: -0.4081181601250818\n",
      "610: reward: 0.0, discounted reward: -0.41427253200233094\n",
      "611: reward: 0.0, discounted reward: -0.4207508181889091\n",
      "612: reward: 0.0, discounted reward: -0.4275700668063597\n",
      "613: reward: 0.0, discounted reward: -0.43474822324578144\n",
      "614: reward: 0.0, discounted reward: -0.4423041773925411\n",
      "615: reward: 0.0, discounted reward: -0.45025781333649867\n",
      "616: reward: 0.0, discounted reward: -0.45863006169855924\n",
      "617: reward: 0.0, discounted reward: -0.4674429547112547\n",
      "618: reward: 0.0, discounted reward: -0.4767196841983024\n",
      "619: reward: 0.0, discounted reward: -0.4864846626057211\n",
      "620: reward: 0.0, discounted reward: -0.4967635872451092\n",
      "621: reward: 0.0, discounted reward: -0.5075835079181493\n",
      "622: reward: 0.0, discounted reward: -0.5189728981002968\n",
      "623: reward: 0.0, discounted reward: -0.5309617298709783\n",
      "624: reward: 0.0, discounted reward: -0.5435815527874853\n",
      "625: reward: 0.0, discounted reward: -0.5568655769101241\n",
      "626: reward: 0.0, discounted reward: -0.5708487601971124\n",
      "627: reward: 0.0, discounted reward: -0.5855679004992053\n",
      "628: reward: 0.0, discounted reward: -0.6010617323961452\n",
      "629: reward: 0.0, discounted reward: -0.6173710291297662\n",
      "630: reward: 0.0, discounted reward: -0.6345387099019988\n",
      "631: reward: 0.0, discounted reward: -0.6526099528201383\n",
      "632: reward: 0.0, discounted reward: -0.671632313786601\n",
      "633: reward: 0.0, discounted reward: -0.6916558516460354\n",
      "634: reward: 0.0, discounted reward: -0.7127332599191243\n",
      "635: reward: 0.0, discounted reward: -0.7349200054697441\n",
      "636: reward: 0.0, discounted reward: -0.7582744744703966\n",
      "637: reward: 0.0, discounted reward: -0.7828581260500308\n",
      "638: reward: 0.0, discounted reward: -0.8087356540285932\n",
      "639: reward: 0.0, discounted reward: -0.8359751571639219\n",
      "640: reward: 0.0, discounted reward: -0.8646483183590048\n",
      "641: reward: 0.0, discounted reward: -0.8948305933011974\n",
      "642: reward: 0.0, discounted reward: -0.926601409029821\n",
      "643: reward: 0.0, discounted reward: -0.9600443729546881\n",
      "644: reward: 0.0, discounted reward: -0.9952474928756009\n",
      "645: reward: 0.0, discounted reward: -1.0323034085818248\n",
      "646: reward: 0.0, discounted reward: -1.0713096356410077\n",
      "647: reward: 0.0, discounted reward: -1.1123688220190953\n",
      "648: reward: 0.0, discounted reward: -1.1555890182065556\n",
      "649: reward: 0.0, discounted reward: -1.2010839615617772\n",
      "650: reward: 0.0, discounted reward: -1.248973375619905\n",
      "651: reward: 0.0, discounted reward: -1.2993832851547766\n",
      "652: reward: 0.0, discounted reward: -1.3524463478230624\n",
      "653: reward: 0.0, discounted reward: -1.4083022032633632\n",
      "654: reward: 0.0, discounted reward: -1.4670978405689428\n",
      "655: reward: 0.0, discounted reward: -1.5289879851011323\n",
      "656: reward: 0.0, discounted reward: -1.5941355056613316\n",
      "657: reward: 0.0, discounted reward: -1.6627118430931203\n",
      "658: reward: 0.0, discounted reward: -1.7348974614423716\n",
      "659: reward: 0.0, discounted reward: -1.810882322862636\n",
      "660: reward: 0.0, discounted reward: -1.890866387515546\n",
      "661: reward: 0.0, discounted reward: -1.975060139781767\n",
      "662: reward: 0.0, discounted reward: -2.063685142167263\n",
      "663: reward: 0.0, discounted reward: -2.1569746183625216\n",
      "664: reward: 0.0, discounted reward: -2.2551740669891096\n",
      "665: reward: 0.0, discounted reward: -2.358541907648676\n",
      "666: reward: 0.0, discounted reward: -2.4673501609745356\n",
      "667: reward: 0.0, discounted reward: -2.5818851644754406\n",
      "668: reward: 0.0, discounted reward: -2.7024483260553405\n",
      "669: reward: 0.0, discounted reward: -2.8293569171920767\n",
      "670: reward: 0.0, discounted reward: -2.9629449078623256\n",
      "671: reward: 0.0, discounted reward: -3.1035638454099566\n",
      "672: reward: 0.0, discounted reward: -3.2515837796706206\n",
      "673: reward: 0.0, discounted reward: -3.4073942367871086\n",
      "674: reward: 0.0, discounted reward: -3.5714052442781488\n",
      "675: reward: -10.0, discounted reward: -3.7440484100581912\n",
      "676: reward: 0.0, discounted reward: -0.30277586715260524\n",
      "677: reward: 0.0, discounted reward: -0.30338590782077673\n",
      "678: reward: 0.0, discounted reward: -0.3040280558925362\n",
      "679: reward: 0.0, discounted reward: -0.3047040012312303\n",
      "680: reward: 0.0, discounted reward: -0.305415522640382\n",
      "681: reward: 0.0, discounted reward: -0.3061644925447523\n",
      "682: reward: 0.0, discounted reward: -0.3069528819177736\n",
      "683: reward: 0.0, discounted reward: -0.30778276546832234\n",
      "684: reward: 0.0, discounted reward: -0.3086563271004789\n",
      "685: reward: 0.0, discounted reward: -0.30957586566064377\n",
      "686: reward: 0.0, discounted reward: -0.310543800987133\n",
      "687: reward: 0.0, discounted reward: -0.3115626802781744\n",
      "688: reward: 0.0, discounted reward: -0.31263518479506003\n",
      "689: reward: 0.0, discounted reward: -0.3137641369180975\n",
      "690: reward: 0.0, discounted reward: -0.3149525075739265\n",
      "691: reward: 0.0, discounted reward: -0.31620342405374646\n",
      "692: reward: 0.0, discounted reward: -0.31752017824303064\n",
      "693: reward: 0.0, discounted reward: -0.3189062352843824\n",
      "694: reward: 0.0, discounted reward: -0.3203652426963316\n",
      "695: reward: 0.0, discounted reward: -0.3219010399720676\n",
      "696: reward: 0.0, discounted reward: -0.3235176686833687\n",
      "697: reward: 0.0, discounted reward: -0.32521938311631715\n",
      "698: reward: 0.0, discounted reward: -0.32701066146678925\n",
      "699: reward: 0.0, discounted reward: -0.3288962176251809\n",
      "700: reward: 0.0, discounted reward: -0.3308810135813827\n",
      "701: reward: 0.0, discounted reward: -0.3329702724826477\n",
      "702: reward: 0.0, discounted reward: -0.3351694923787162\n",
      "703: reward: 0.0, discounted reward: -0.33748446069036714\n",
      "704: reward: 0.0, discounted reward: -0.3399212694394735\n",
      "705: reward: 0.0, discounted reward: -0.342486331280638\n",
      "706: reward: 0.0, discounted reward: -0.3451863963766007\n",
      "707: reward: 0.0, discounted reward: -0.34802857016182454\n",
      "708: reward: 0.0, discounted reward: -0.3510203320410075\n",
      "709: reward: 0.0, discounted reward: -0.35416955507172654\n",
      "710: reward: 0.0, discounted reward: -0.3574845266830097\n",
      "711: reward: 0.0, discounted reward: -0.36097397048436025\n",
      "712: reward: 0.0, discounted reward: -0.36464706922262413\n",
      "713: reward: 0.0, discounted reward: -0.36851348894711233\n",
      "714: reward: 0.0, discounted reward: -0.3725834044465736\n",
      "715: reward: 0.0, discounted reward: -0.376867526024954\n",
      "716: reward: 0.0, discounted reward: -0.381377127686407\n",
      "717: reward: 0.0, discounted reward: -0.3861240768037259\n",
      "718: reward: 0.0, discounted reward: -0.3911208653482722\n",
      "719: reward: 0.0, discounted reward: -0.396380642763584\n",
      "720: reward: 0.0, discounted reward: -0.4019172505691754\n",
      "721: reward: 0.0, discounted reward: -0.40774525878558737\n",
      "722: reward: 0.0, discounted reward: -0.41388000427654736\n",
      "723: reward: 0.0, discounted reward: -0.4203376311091369\n",
      "724: reward: 0.0, discounted reward: -0.42713513303817846\n",
      "725: reward: 0.0, discounted reward: -0.4342903982266432\n",
      "726: reward: 0.0, discounted reward: -0.441822256319764\n",
      "727: reward: 0.0, discounted reward: -0.4497505279967334\n",
      "728: reward: 0.0, discounted reward: -0.45809607713038525\n",
      "729: reward: 0.0, discounted reward: -0.46688086569212406\n",
      "730: reward: 0.0, discounted reward: -0.47612801154658607\n",
      "731: reward: 0.0, discounted reward: -0.4858618492881249\n",
      "732: reward: 0.0, discounted reward: -0.4961079942792185\n",
      "733: reward: 0.0, discounted reward: -0.506893410059317\n",
      "734: reward: 0.0, discounted reward: -0.5182464793015259\n",
      "735: reward: 0.0, discounted reward: -0.5301970785038511\n",
      "736: reward: 0.0, discounted reward: -0.5427766566115618\n",
      "737: reward: 0.0, discounted reward: -0.5560183177775732\n",
      "738: reward: 0.0, discounted reward: -0.5699569084786377\n",
      "739: reward: 0.0, discounted reward: -0.5846291092166004\n",
      "740: reward: 0.0, discounted reward: -0.6000735310460348\n",
      "741: reward: 0.0, discounted reward: -0.6163308171822816\n",
      "742: reward: 0.0, discounted reward: -0.633443749957278\n",
      "743: reward: 0.0, discounted reward: -0.6514573634046429\n",
      "744: reward: 0.0, discounted reward: -0.67041906177029\n",
      "745: reward: 0.0, discounted reward: -0.6903787442604449\n",
      "746: reward: 0.0, discounted reward: -0.7113889363553447\n",
      "747: reward: 0.0, discounted reward: -0.7335049280341868\n",
      "748: reward: 0.0, discounted reward: -0.7567849192750731\n",
      "749: reward: 0.0, discounted reward: -0.7812901732128481\n",
      "750: reward: 0.0, discounted reward: -0.8070851773578744\n",
      "751: reward: 0.0, discounted reward: -0.8342378133000075\n",
      "752: reward: 0.0, discounted reward: -0.862819535344358\n",
      "753: reward: 0.0, discounted reward: -0.8929055585489376\n",
      "754: reward: 0.0, discounted reward: -0.9245750566590213\n",
      "755: reward: 0.0, discounted reward: -0.9579113704591095\n",
      "756: reward: 0.0, discounted reward: -0.9930022270907812\n",
      "757: reward: 0.0, discounted reward: -1.0299399709135935\n",
      "758: reward: 0.0, discounted reward: -1.068821806516554\n",
      "759: reward: 0.0, discounted reward: -1.1097500545196701\n",
      "760: reward: 0.0, discounted reward: -1.1528324208387397\n",
      "761: reward: 0.0, discounted reward: -1.198182280121971\n",
      "762: reward: 0.0, discounted reward: -1.2459189741043197\n",
      "763: reward: 0.0, discounted reward: -1.2961681256646866\n",
      "764: reward: 0.0, discounted reward: -1.3490619694124415\n",
      "765: reward: 0.0, discounted reward: -1.404739699673236\n",
      "766: reward: 0.0, discounted reward: -1.4633478367898616\n",
      "767: reward: 0.0, discounted reward: -1.5250406127020992\n",
      "768: reward: 0.0, discounted reward: -1.589980376820244\n",
      "769: reward: 0.0, discounted reward: -1.6583380232603966\n",
      "770: reward: 0.0, discounted reward: -1.7302934405658206\n",
      "771: reward: 0.0, discounted reward: -1.8060359850978456\n",
      "772: reward: 0.0, discounted reward: -1.8857649793420825\n",
      "773: reward: 0.0, discounted reward: -1.9696902364412792\n",
      "774: reward: 0.0, discounted reward: -2.0580326123351704\n",
      "775: reward: 0.0, discounted reward: -2.151024586960319\n",
      "776: reward: 0.0, discounted reward: -2.248910876039423\n",
      "777: reward: 0.0, discounted reward: -2.351949075070059\n",
      "778: reward: 0.0, discounted reward: -2.46041033720757\n",
      "779: reward: 0.0, discounted reward: -2.574580086826003\n",
      "780: reward: 0.0, discounted reward: -2.6947587706348797\n",
      "781: reward: 0.0, discounted reward: -2.821262648328434\n",
      "782: reward: 0.0, discounted reward: -2.954424624847965\n",
      "783: reward: 0.0, discounted reward: -3.094595126447471\n",
      "784: reward: 0.0, discounted reward: -3.2421430228680044\n",
      "785: reward: 0.0, discounted reward: -3.3974565980475124\n",
      "786: reward: 0.0, discounted reward: -3.560944571920679\n",
      "787: reward: -10.0, discounted reward: -3.7330371759976964\n",
      "788: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "789: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "790: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "791: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "792: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "793: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "794: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "795: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "796: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "797: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "798: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "799: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "800: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "801: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "802: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "803: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "804: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "805: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "806: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "807: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "808: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "809: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "810: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "811: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "812: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "813: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "814: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "815: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "816: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "817: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "818: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "819: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "820: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "821: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "822: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "823: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "824: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "825: reward: 0.0, discounted reward: -0.29118509445734736\n",
      "826: reward: 0.0, discounted reward: -0.29118509445734736\n"
     ]
    }
   ],
   "source": [
    "#set above epochs to 1.  \n",
    "#create a concatenated image of the last 5 frames\n",
    "\n",
    "\n",
    "\n",
    "frame_to_view = 222\n",
    "concatenated_frames = frames[:,:,frame_to_view]\n",
    "\n",
    "\n",
    "for j in range(frame_to_view-1, frame_to_view):\n",
    "    print(\"discounted rewared at frame {}: {}\".format(j, discounted_rewards[j]))\n",
    "    this_frame = frames[:,:,j]\n",
    "    #print(\"this_frame: \" + str(this_frame.shape))\n",
    "    concatenated_frames = np.append(concatenated_frames, this_frame, axis=0)\n",
    "\n",
    "    \n",
    "# print(concatenated_frames)\n",
    "# print the image\n",
    "#show_observation(concatenated_frames)\n",
    "\n",
    "counter = 0\n",
    "for this_reward, this_discounted_reward in zip(rewards, discounted_rewards):\n",
    "    print(\"{}: reward: {}, discounted reward: {}\".format(counter, this_reward, this_discounted_reward))\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "#print(actions)\n",
    "#print(all_logits)\n",
    "\n",
    "# logit_sums = np.zeros(6)\n",
    "\n",
    "# for this_logit in all_logits:\n",
    "#     logit_sums = logit_sums + this_logit\n",
    "# #    print(this_logit)\n",
    "\n",
    "# print(logit_sums/len(all_logits))\n",
    "# print(get_average_logits(all_logits, discounted_rewards))\n",
    "\n",
    "# print(actions[5])\n",
    "# print(average_logits[int(actions[5])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_frames: 1086\n",
      "0: reward: 0.0, discounted reward: 0.0766933783174092\n",
      "1: reward: 0.0, discounted reward: 0.07899669881315724\n",
      "2: reward: 0.0, discounted reward: 0.08137125602526862\n",
      "3: reward: 0.0, discounted reward: 0.08381925315115664\n",
      "4: reward: 0.0, discounted reward: 0.0863429615283608\n",
      "5: reward: 0.0, discounted reward: 0.08894472274197332\n",
      "6: reward: 0.0, discounted reward: 0.09162695079724396\n",
      "7: reward: 0.0, discounted reward: 0.09439213435937864\n",
      "8: reward: 0.0, discounted reward: 0.09724283906261028\n",
      "9: reward: 0.0, discounted reward: 0.10018170989068412\n",
      "10: reward: 0.0, discounted reward: 0.10321147363096643\n",
      "11: reward: 0.0, discounted reward: 0.10633494140445336\n",
      "12: reward: 0.0, discounted reward: 0.1095550112740275\n",
      "13: reward: 0.0, discounted reward: 0.1128746709333823\n",
      "14: reward: 0.0, discounted reward: 0.1162970004791089\n",
      "15: reward: 0.0, discounted reward: 0.11982517526851774\n",
      "16: reward: 0.0, discounted reward: 0.12346246886584647\n",
      "17: reward: 0.0, discounted reward: 0.12721225607958742\n",
      "18: reward: 0.0, discounted reward: 0.13107801609375336\n",
      "19: reward: 0.0, discounted reward: 0.13506333569598628\n",
      "20: reward: 0.0, discounted reward: 0.13917191260550474\n",
      "21: reward: 0.0, discounted reward: 0.1434075589039774\n",
      "22: reward: 0.0, discounted reward: 0.1477742045725059\n",
      "23: reward: 0.0, discounted reward: 0.1522759011379992\n",
      "24: reward: 0.0, discounted reward: 0.15691682543232216\n",
      "25: reward: 0.0, discounted reward: 0.16170128346770668\n",
      "26: reward: 0.0, discounted reward: 0.16663371443202063\n",
      "27: reward: 0.0, discounted reward: 0.17171869480760202\n",
      "28: reward: 0.0, discounted reward: 0.17696094261747972\n",
      "29: reward: 0.0, discounted reward: 0.1823653218029207\n",
      "30: reward: 0.0, discounted reward: 0.18793684673636496\n",
      "31: reward: 0.0, discounted reward: 0.1936806868739364\n",
      "32: reward: 0.0, discounted reward: 0.1996021715518451\n",
      "33: reward: 0.0, discounted reward: 0.2057067949311324\n",
      "34: reward: 0.0, discounted reward: 0.21200022109534608\n",
      "35: reward: 0.0, discounted reward: 0.2184882893058757\n",
      "36: reward: 0.0, discounted reward: 0.22517701941982374\n",
      "37: reward: 0.0, discounted reward: 0.23207261747544028\n",
      "38: reward: 0.0, discounted reward: 0.2391814814503027\n",
      "39: reward: 0.0, discounted reward: 0.24651020719758354\n",
      "40: reward: 0.0, discounted reward: 0.2540655945659143\n",
      "41: reward: 0.0, discounted reward: 0.26185465370852334\n",
      "42: reward: 0.0, discounted reward: 0.2698846115875017\n",
      "43: reward: 0.0, discounted reward: 0.278162918679232\n",
      "44: reward: 0.0, discounted reward: 0.28669725588720135\n",
      "45: reward: 0.0, discounted reward: 0.2954955416686131\n",
      "46: reward: 0.0, discounted reward: 0.3045659393814087\n",
      "47: reward: 0.0, discounted reward: 0.31391686485851755\n",
      "48: reward: 0.0, discounted reward: 0.32355699421636175\n",
      "49: reward: 0.0, discounted reward: 0.33349527190486095\n",
      "50: reward: 0.0, discounted reward: 0.3437409190064064\n",
      "51: reward: 5.0, discounted reward: 0.3543034417915049\n",
      "52: reward: 0.0, discounted reward: 0.17776959231843614\n",
      "53: reward: 0.0, discounted reward: 0.18319898128844275\n",
      "54: reward: 0.0, discounted reward: 0.18879628950494443\n",
      "55: reward: 0.0, discounted reward: 0.19456671034669873\n",
      "56: reward: 0.0, discounted reward: 0.20051559781242478\n",
      "57: reward: 0.0, discounted reward: 0.20664847148843105\n",
      "58: reward: 0.0, discounted reward: 0.21297102166988083\n",
      "59: reward: 0.0, discounted reward: 0.21948911464044757\n",
      "60: reward: 0.0, discounted reward: 0.22620879811525865\n",
      "61: reward: 0.0, discounted reward: 0.23313630685217732\n",
      "62: reward: 0.0, discounted reward: 0.24027806843662952\n",
      "63: reward: 0.0, discounted reward: 0.24764070924534315\n",
      "64: reward: 0.0, discounted reward: 0.2552310605945325\n",
      "65: reward: 0.0, discounted reward: 0.26305616507823276\n",
      "66: reward: 0.0, discounted reward: 0.2711232831026661\n",
      "67: reward: 0.0, discounted reward: 0.27943989962270044\n",
      "68: reward: 0.0, discounted reward: 0.28801373108665335\n",
      "69: reward: 0.0, discounted reward: 0.2968527325958832\n",
      "70: reward: 0.0, discounted reward: 0.30596510528581083\n",
      "71: reward: 0.0, discounted reward: 0.3153593039352208\n",
      "72: reward: 0.0, discounted reward: 0.32504404481090116\n",
      "73: reward: 0.0, discounted reward: 0.3350283137549015\n",
      "74: reward: 0.0, discounted reward: 0.3453213745219122\n",
      "75: reward: 0.0, discounted reward: 0.3559327773745005\n",
      "76: reward: 0.0, discounted reward: 0.3668723679441792\n",
      "77: reward: 0.0, discounted reward: 0.37815029636652847\n",
      "78: reward: 0.0, discounted reward: 0.3897770266988472\n",
      "79: reward: 0.0, discounted reward: 0.4017633466290727\n",
      "80: reward: 0.0, discounted reward: 0.41412037748497527\n",
      "81: reward: 0.0, discounted reward: 0.42685958455291606\n",
      "82: reward: 0.0, discounted reward: 0.43999278771574163\n",
      "83: reward: 0.0, discounted reward: 0.4535321724196856\n",
      "84: reward: 0.0, discounted reward: 0.46749030098045247\n",
      "85: reward: 0.0, discounted reward: 0.481880124238975\n",
      "86: reward: 0.0, discounted reward: 0.4967149935776581\n",
      "87: reward: 0.0, discounted reward: 0.5120086733082592\n",
      "88: reward: 0.0, discounted reward: 0.5277753534428995\n",
      "89: reward: 0.0, discounted reward: 0.5440296628600545\n",
      "90: reward: 0.0, discounted reward: 0.56078668287774\n",
      "91: reward: 0.0, discounted reward: 0.578061961246488\n",
      "92: reward: 0.0, discounted reward: 0.5958715265750942\n",
      "93: reward: 0.0, discounted reward: 0.6142319032025233\n",
      "94: reward: 0.0, discounted reward: 0.6331601265297697\n",
      "95: reward: 0.0, discounted reward: 0.6526737588259001\n",
      "96: reward: 0.0, discounted reward: 0.6727909055229415\n",
      "97: reward: 0.0, discounted reward: 0.6935302320147371\n",
      "98: reward: 0.0, discounted reward: 0.7149109809753509\n",
      "99: reward: 0.0, discounted reward: 0.7369529902130971\n",
      "100: reward: 0.0, discounted reward: 0.759676711076753\n",
      "101: reward: 0.0, discounted reward: 0.7831032274310374\n",
      "102: reward: 0.0, discounted reward: 0.8072542752189596\n",
      "103: reward: 0.0, discounted reward: 0.8321522626291886\n",
      "104: reward: 0.0, discounted reward: 0.8578202908871566\n",
      "105: reward: 0.0, discounted reward: 0.8842821756891855\n",
      "106: reward: 0.0, discounted reward: 0.9115624692995247\n",
      "107: reward: 0.0, discounted reward: 0.9396864833308021\n",
      "108: reward: 0.0, discounted reward: 0.9686803122290263\n",
      "109: reward: 0.0, discounted reward: 0.9985708574849275\n",
      "110: reward: 0.0, discounted reward: 1.029385852594104\n",
      "111: reward: 0.0, discounted reward: 1.0611538887891312\n",
      "112: reward: 0.0, discounted reward: 1.0939044415675099\n",
      "113: reward: 0.0, discounted reward: 1.1276678980400652\n",
      "114: reward: 0.0, discounted reward: 1.1624755851251738\n",
      "115: reward: 0.0, discounted reward: 1.1983597986149763\n",
      "116: reward: 0.0, discounted reward: 1.235353833140546\n",
      "117: reward: 0.0, discounted reward: 1.2734920130638139\n",
      "118: reward: 0.0, discounted reward: 1.3128097243249146\n",
      "119: reward: 0.0, discounted reward: 1.3533434472745032\n",
      "120: reward: 0.0, discounted reward: 1.3951307905215014\n",
      "121: reward: 0.0, discounted reward: 1.4382105258276854\n",
      "122: reward: 0.0, discounted reward: 1.482622624081483\n",
      "123: reward: 0.0, discounted reward: 1.5284082923843676\n",
      "124: reward: 0.0, discounted reward: 1.5756100122842482\n",
      "125: reward: 25.0, discounted reward: 1.6242715791913418\n",
      "126: reward: 0.0, discounted reward: 0.737322901910123\n",
      "127: reward: 0.0, discounted reward: 0.7600580633417281\n",
      "128: reward: 0.0, discounted reward: 0.7834963740959602\n",
      "129: reward: 0.0, discounted reward: 0.8076595810590861\n",
      "130: reward: 0.0, discounted reward: 0.832570103701484\n",
      "131: reward: 0.0, discounted reward: 0.8582510548792137\n",
      "132: reward: 5.0, discounted reward: 0.8847262622789351\n",
      "133: reward: 0.0, discounted reward: 0.7245972423054776\n",
      "134: reward: 0.0, discounted reward: 0.7469388266359083\n",
      "135: reward: 0.0, discounted reward: 0.7699713878013006\n",
      "136: reward: 0.0, discounted reward: 0.7937162962192309\n",
      "137: reward: 0.0, discounted reward: 0.8181955832480249\n",
      "138: reward: 0.0, discounted reward: 0.843431961628225\n",
      "139: reward: 0.0, discounted reward: 0.8694488465562663\n",
      "140: reward: 0.0, discounted reward: 0.8962703774099171\n",
      "141: reward: 0.0, discounted reward: 0.9239214401456397\n",
      "142: reward: 0.0, discounted reward: 0.9524276903886527\n",
      "143: reward: 10.0, discounted reward: 0.9818155772371197\n",
      "144: reward: 0.0, discounted reward: 0.6372662713613635\n",
      "145: reward: 0.0, discounted reward: 0.6569068978275431\n",
      "146: reward: 0.0, discounted reward: 0.6771549663493778\n",
      "147: reward: 0.0, discounted reward: 0.6980292637945681\n",
      "148: reward: 0.0, discounted reward: 0.7195491580679603\n",
      "149: reward: 0.0, discounted reward: 0.7417346160817666\n",
      "150: reward: 0.0, discounted reward: 0.764606222281567\n",
      "151: reward: 0.0, discounted reward: 0.7881851977452787\n",
      "152: reward: 0.0, discounted reward: 0.8124934198728165\n",
      "153: reward: 0.0, discounted reward: 0.8375534426847112\n",
      "154: reward: 0.0, discounted reward: 0.8633885177485202\n",
      "155: reward: 0.0, discounted reward: 0.890022615752447\n",
      "156: reward: 0.0, discounted reward: 0.917480448746186\n",
      "157: reward: 0.0, discounted reward: 0.9457874930696282\n",
      "158: reward: 0.0, discounted reward: 0.9749700129907025\n",
      "159: reward: 0.0, discounted reward: 1.0050550850742843\n",
      "160: reward: 15.0, discounted reward: 1.0360706233047812\n",
      "161: reward: 0.0, discounted reward: 0.5057762603239269\n",
      "162: reward: 0.0, discounted reward: 0.5213501854178145\n",
      "163: reward: 0.0, discounted reward: 0.5374057782981112\n",
      "164: reward: 0.0, discounted reward: 0.5539579359066645\n",
      "165: reward: 0.0, discounted reward: 0.5710220159154823\n",
      "166: reward: 0.0, discounted reward: 0.5886138509761192\n",
      "167: reward: 0.0, discounted reward: 0.6067497634097655\n",
      "168: reward: 0.0, discounted reward: 0.625446580351669\n",
      "169: reward: 0.0, discounted reward: 0.6447216493639406\n",
      "170: reward: 0.0, discounted reward: 0.6645928545312307\n",
      "171: reward: 0.0, discounted reward: 0.6850786330542104\n",
      "172: reward: 0.0, discounted reward: 0.7061979923562512\n",
      "173: reward: 0.0, discounted reward: 0.7279705277191799\n",
      "174: reward: 0.0, discounted reward: 0.7504164404644672\n",
      "175: reward: 0.0, discounted reward: 0.7735565566967223\n",
      "176: reward: 0.0, discounted reward: 0.7974123466268821\n",
      "177: reward: 20.0, discounted reward: 0.8220059444930262\n",
      "178: reward: 0.0, discounted reward: 0.0976679762149266\n",
      "179: reward: 0.0, discounted reward: 0.10061999561472157\n",
      "180: reward: 0.0, discounted reward: 0.10366331458358236\n",
      "181: reward: 0.0, discounted reward: 0.10680075681952132\n",
      "182: reward: 0.0, discounted reward: 0.11003523335141716\n",
      "183: reward: 0.0, discounted reward: 0.11336974523996957\n",
      "184: reward: 0.0, discounted reward: 0.11680738636218856\n",
      "185: reward: 0.0, discounted reward: 0.12035134628200195\n",
      "186: reward: 0.0, discounted reward: 0.1240049132096446\n",
      "187: reward: 0.0, discounted reward: 0.1277714770525752\n",
      "188: reward: 0.0, discounted reward: 0.13165453256075105\n",
      "189: reward: 0.0, discounted reward: 0.13565768256917976\n",
      "190: reward: 0.0, discounted reward: 0.13978464134075574\n",
      "191: reward: 0.0, discounted reward: 0.14403923801248358\n",
      "192: reward: 0.0, discounted reward: 0.14842542014828547\n",
      "193: reward: 0.0, discounted reward: 0.1529472574016895\n",
      "194: reward: 0.0, discounted reward: 0.1576089452917967\n",
      "195: reward: 0.0, discounted reward: 0.16241480909603095\n",
      "196: reward: 0.0, discounted reward: 0.16736930786328275\n",
      "197: reward: 0.0, discounted reward: 0.17247703855117122\n",
      "198: reward: 0.0, discounted reward: 0.1777427402912624\n",
      "199: reward: 0.0, discounted reward: 0.1831712987862018\n",
      "200: reward: 0.0, discounted reward: 0.18876775084284036\n",
      "201: reward: 0.0, discounted reward: 0.19453728904556053\n",
      "202: reward: 0.0, discounted reward: 0.20048526657413798\n",
      "203: reward: 0.0, discounted reward: 0.20661720217060958\n",
      "204: reward: 0.0, discounted reward: 0.2129387852597556\n",
      "205: reward: 0.0, discounted reward: 0.21945588122794735\n",
      "206: reward: 0.0, discounted reward: 0.22617453686525843\n",
      "207: reward: 0.0, discounted reward: 0.2331009859758884\n",
      "208: reward: 0.0, discounted reward: 0.24024165516210488\n",
      "209: reward: 0.0, discounted reward: 0.24760316978707034\n",
      "210: reward: 0.0, discounted reward: 0.25519236012208624\n",
      "211: reward: 0.0, discounted reward: 0.2630162676839583\n",
      "212: reward: 0.0, discounted reward: 0.27108215176836253\n",
      "213: reward: 0.0, discounted reward: 0.2793974961852741\n",
      "214: reward: 0.0, discounted reward: 0.2879700162027087\n",
      "215: reward: 0.0, discounted reward: 0.2968076657052186\n",
      "216: reward: 0.0, discounted reward: 0.3059186445737855\n",
      "217: reward: 0.0, discounted reward: 0.31531140629395754\n",
      "218: reward: 0.0, discounted reward: 0.3249946657992896\n",
      "219: reward: 0.0, discounted reward: 0.3349774075573638\n",
      "220: reward: 0.0, discounted reward: 0.345268893905894\n",
      "221: reward: 0.0, discounted reward: 0.3558786736466467\n",
      "222: reward: 0.0, discounted reward: 0.36681659090515467\n",
      "223: reward: 0.0, discounted reward: 0.37809279426444126\n",
      "224: reward: 0.0, discounted reward: 0.38971774618123156\n",
      "225: reward: 0.0, discounted reward: 0.40170223269338645\n",
      "226: reward: 0.0, discounted reward: 0.41405737342756677\n",
      "227: reward: 0.0, discounted reward: 0.4267946319164125\n",
      "228: reward: 0.0, discounted reward: 0.43992582623481014\n",
      "229: reward: 0.0, discounted reward: 0.45346313996511695\n",
      "230: reward: 0.0, discounted reward: 0.46741913350151576\n",
      "231: reward: 0.0, discounted reward: 0.4818067557039888\n",
      "232: reward: 0.0, discounted reward: 0.49663935591272385\n",
      "233: reward: 0.0, discounted reward: 0.5119306963341002\n",
      "234: reward: 0.0, discounted reward: 0.5276949648097459\n",
      "235: reward: 0.0, discounted reward: 0.5439467879805145\n",
      "236: reward: 0.0, discounted reward: 0.5607012448575958\n",
      "237: reward: 0.0, discounted reward: 0.5779738808133497\n",
      "238: reward: 0.0, discounted reward: 0.5957807220048484\n",
      "239: reward: 0.0, discounted reward: 0.614138290243507\n",
      "240: reward: 0.0, discounted reward: 0.6330636183245983\n",
      "241: reward: 0.0, discounted reward: 0.652574265830878\n",
      "242: reward: 0.0, discounted reward: 0.6726883354249809\n",
      "243: reward: 0.0, discounted reward: 0.6934244896457052\n",
      "244: reward: 0.0, discounted reward: 0.7148019682237718\n",
      "245: reward: 0.0, discounted reward: 0.7368406059331187\n",
      "246: reward: 0.0, discounted reward: 0.759560850994301\n",
      "247: reward: 0.0, discounted reward: 0.7829837840470664\n",
      "248: reward: 0.0, discounted reward: 0.8071311377097111\n",
      "249: reward: 0.0, discounted reward: 0.8320253167433654\n",
      "250: reward: 0.0, discounted reward: 0.8576894188399162\n",
      "251: reward: 0.0, discounted reward: 0.8841472560528553\n",
      "252: reward: 0.0, discounted reward: 0.9114233768909367\n",
      "253: reward: 0.0, discounted reward: 0.9395430890951444\n",
      "254: reward: 0.0, discounted reward: 0.9685324831201008\n",
      "255: reward: 0.0, discounted reward: 0.9984184563417052\n",
      "256: reward: 0.0, discounted reward: 1.0292287380134624\n",
      "257: reward: 0.0, discounted reward: 1.0609919149946554\n",
      "258: reward: 30.0, discounted reward: 1.0937374582742359\n",
      "259: reward: 0.0, discounted reward: 0.002957460991482539\n",
      "260: reward: 0.0, discounted reward: 0.002980289198799859\n",
      "261: reward: 0.0, discounted reward: 0.003003823433147611\n",
      "262: reward: 0.0, discounted reward: 0.0030280855304133346\n",
      "263: reward: 0.0, discounted reward: 0.0030530980018212976\n",
      "264: reward: 0.0, discounted reward: 0.0030788840548191974\n",
      "265: reward: 0.0, discounted reward: 0.0031054676146108464\n",
      "266: reward: 0.0, discounted reward: 0.0031328733463548146\n",
      "267: reward: 0.0, discounted reward: 0.0031611266780496275\n",
      "268: reward: 0.0, discounted reward: 0.0031902538241267535\n",
      "269: reward: 0.0, discounted reward: 0.0032202818097732755\n",
      "270: reward: 0.0, discounted reward: 0.003251238496006804\n",
      "271: reward: 0.0, discounted reward: 0.0032831526055259043\n",
      "272: reward: 0.0, discounted reward: 0.003316053749360029\n",
      "273: reward: 0.0, discounted reward: 0.0033499724543436626\n",
      "274: reward: 0.0, discounted reward: 0.003384940191440192\n",
      "275: reward: 0.0, discounted reward: 0.003420989404941769\n",
      "276: reward: 0.0, discounted reward: 0.003458153542572261\n",
      "277: reward: 0.0, discounted reward: 0.0034964670865212212\n",
      "278: reward: 0.0, discounted reward: 0.0035359655854376756\n",
      "279: reward: 0.0, discounted reward: 0.0035766856874134014\n",
      "280: reward: 0.0, discounted reward: 0.003618665173986314\n",
      "281: reward: 0.0, discounted reward: 0.0036619429951955033\n",
      "282: reward: 0.0, discounted reward: 0.00370655930572044\n",
      "283: reward: 0.0, discounted reward: 0.003752555502137901\n",
      "284: reward: 0.0, discounted reward: 0.0037999742613311596\n",
      "285: reward: 0.0, discounted reward: 0.003848859580087096\n",
      "286: reward: 0.0, discounted reward: 0.0038992568159179584\n",
      "287: reward: 0.0, discounted reward: 0.003951212729145652\n",
      "288: reward: 0.0, discounted reward: 0.004004775526287604\n",
      "289: reward: 0.0, discounted reward: 0.004059994904784462\n",
      "290: reward: 0.0, discounted reward: 0.004116922099111119\n",
      "291: reward: 0.0, discounted reward: 0.004175609928313858\n",
      "292: reward: 0.0, discounted reward: 0.004236112845017714\n",
      "293: reward: 0.0, discounted reward: 0.004298486985949523\n",
      "294: reward: 0.0, discounted reward: 0.004362790224023554\n",
      "295: reward: 0.0, discounted reward: 0.004429082222038018\n",
      "296: reward: 0.0, discounted reward: 0.0044974244880323115\n",
      "297: reward: 0.0, discounted reward: 0.004567880432356326\n",
      "298: reward: 0.0, discounted reward: 0.004640515426504794\n",
      "299: reward: 0.0, discounted reward: 0.004715396863771254\n",
      "300: reward: 0.0, discounted reward: 0.004792594221777915\n",
      "301: reward: 0.0, discounted reward: 0.004872179126939422\n",
      "302: reward: 0.0, discounted reward: 0.004954225420920356\n",
      "303: reward: 0.0, discounted reward: 0.005038809229148123\n",
      "304: reward: 0.0, discounted reward: 0.005126009031444792\n",
      "305: reward: 0.0, discounted reward: 0.005215905734843418\n",
      "306: reward: 0.0, discounted reward: 0.005308582748656435\n",
      "307: reward: 0.0, discounted reward: 0.005404126061865732\n",
      "308: reward: 0.0, discounted reward: 0.005502624322906242\n",
      "309: reward: 0.0, discounted reward: 0.005604168921917079\n",
      "310: reward: 0.0, discounted reward: 0.005708854075536497\n",
      "311: reward: 0.0, discounted reward: 0.005816776914319403\n",
      "312: reward: 0.0, discounted reward: 0.005928037572858482\n",
      "313: reward: 0.0, discounted reward: 0.006042739282692582\n",
      "314: reward: 0.0, discounted reward: 0.006160988468088563\n",
      "315: reward: 0.0, discounted reward: 0.006282894844785451\n",
      "316: reward: 0.0, discounted reward: 0.006408571521792551\n",
      "317: reward: 0.0, discounted reward: 0.0065381351063359535\n",
      "318: reward: 0.0, discounted reward: 0.006671705812050801\n",
      "319: reward: 0.0, discounted reward: 0.0068094075705197165\n",
      "320: reward: 0.0, discounted reward: 0.006951368146260866\n",
      "321: reward: 0.0, discounted reward: 0.007097719255272361\n",
      "322: reward: 0.0, discounted reward: 0.007248596687242974\n",
      "323: reward: 0.0, discounted reward: 0.007404140431542575\n",
      "324: reward: 0.0, discounted reward: 0.007564494807109174\n",
      "325: reward: 0.0, discounted reward: 0.007729808596353091\n",
      "326: reward: 0.0, discounted reward: 0.00790023518320249\n",
      "327: reward: 0.0, discounted reward: 0.008075932695418364\n",
      "328: reward: 0.0, discounted reward: 0.008257064151311019\n",
      "329: reward: 0.0, discounted reward: 0.008443797610994168\n",
      "330: reward: 0.0, discounted reward: 0.008636306332317002\n",
      "331: reward: 0.0, discounted reward: 0.008834768931618891\n",
      "332: reward: 0.0, discounted reward: 0.009039369549455892\n",
      "333: reward: 0.0, discounted reward: 0.0092502980214528\n",
      "334: reward: 0.0, discounted reward: 0.009467750054439302\n",
      "335: reward: 0.0, discounted reward: 0.009691927408033636\n",
      "336: reward: 0.0, discounted reward: 0.009923038081842226\n",
      "337: reward: 0.0, discounted reward: 0.010161296508449021\n",
      "338: reward: 0.0, discounted reward: 0.01040692375237355\n",
      "339: reward: 0.0, discounted reward: 0.010660147715182346\n",
      "340: reward: 0.0, discounted reward: 0.01092120334694399\n",
      "341: reward: 0.0, discounted reward: 0.011190332864224035\n",
      "342: reward: 0.0, discounted reward: 0.01146778597482202\n",
      "343: reward: 0.0, discounted reward: 0.011753820109459119\n",
      "344: reward: 0.0, discounted reward: 0.012048700660631382\n",
      "345: reward: 0.0, discounted reward: 0.012352701228850215\n",
      "346: reward: 0.0, discounted reward: 0.012666103876498492\n",
      "347: reward: 0.0, discounted reward: 0.012989199389537957\n",
      "348: reward: 0.0, discounted reward: 0.013322287547310598\n",
      "349: reward: 0.0, discounted reward: 0.013665677400684457\n",
      "350: reward: 0.0, discounted reward: 0.014019687558801836\n",
      "351: reward: 0.0, discounted reward: 0.014384646484696041\n",
      "352: reward: 0.0, discounted reward: 0.014760892800050891\n",
      "353: reward: 0.0, discounted reward: 0.015148775599385789\n",
      "354: reward: 0.0, discounted reward: 0.015548654773957848\n",
      "355: reward: 0.0, discounted reward: 0.015960901345681622\n",
      "356: reward: 0.0, discounted reward: 0.016385897811376237\n",
      "357: reward: 0.0, discounted reward: 0.01682403849765934\n",
      "358: reward: 0.0, discounted reward: 0.01727572992681718\n",
      "359: reward: 0.0, discounted reward: 0.017741391193990208\n",
      "360: reward: 0.0, discounted reward: 0.01822145435602426\n",
      "361: reward: 0.0, discounted reward: 0.018716364832348023\n",
      "362: reward: 0.0, discounted reward: 0.019226581818248815\n",
      "363: reward: 0.0, discounted reward: 0.01975257871093004\n",
      "364: reward: 0.0, discounted reward: 0.020294843548745736\n",
      "365: reward: 0.0, discounted reward: 0.020853879464019648\n",
      "366: reward: 0.0, discounted reward: 0.021430205149869045\n",
      "367: reward: 0.0, discounted reward: 0.022024355341466358\n",
      "368: reward: 0.0, discounted reward: 0.02263688131218524\n",
      "369: reward: 0.0, discounted reward: 0.023268351385091306\n",
      "370: reward: 0.0, discounted reward: 0.023919351460252196\n",
      "371: reward: 0.0, discounted reward: 0.024590485558356207\n",
      "372: reward: 0.0, discounted reward: 0.025282376381143848\n",
      "373: reward: 0.0, discounted reward: 0.02599566588917234\n",
      "374: reward: 0.0, discounted reward: 0.02673101589744914\n",
      "375: reward: 0.0, discounted reward: 0.027489108689487077\n",
      "376: reward: 0.0, discounted reward: 0.02827064765035093\n",
      "377: reward: 0.0, discounted reward: 0.029076357919282734\n",
      "378: reward: 0.0, discounted reward: 0.0299069870625114\n",
      "379: reward: 0.0, discounted reward: 0.03076330576687085\n",
      "380: reward: 0.0, discounted reward: 0.03164610855487028\n",
      "381: reward: 0.0, discounted reward: 0.032556214521880004\n",
      "382: reward: 0.0, discounted reward: 0.033494468096116836\n",
      "383: reward: 0.0, discounted reward: 0.034461739822134185\n",
      "384: reward: 0.0, discounted reward: 0.035458927168543826\n",
      "385: reward: 0.0, discounted reward: 0.03648695536071871\n",
      "386: reward: 0.0, discounted reward: 0.03754677823924952\n",
      "387: reward: 0.0, discounted reward: 0.038639379144951386\n",
      "388: reward: 0.0, discounted reward: 0.03976577183124197\n",
      "389: reward: 0.0, discounted reward: 0.040927001404737415\n",
      "390: reward: 0.0, discounted reward: 0.0421241452949389\n",
      "391: reward: 0.0, discounted reward: 0.04335831425390951\n",
      "392: reward: 0.0, discounted reward: 0.044630653386868904\n",
      "393: reward: 0.0, discounted reward: 0.0459423432146621\n",
      "394: reward: 0.0, discounted reward: 0.04729460076908806\n",
      "395: reward: 0.0, discounted reward: 0.04868868072210453\n",
      "396: reward: 0.0, discounted reward: 0.050125876549956545\n",
      "397: reward: 0.0, discounted reward: 0.051607521733309146\n",
      "398: reward: 0.0, discounted reward: 0.05313499099449739\n",
      "399: reward: 0.0, discounted reward: 0.054709701573041974\n",
      "400: reward: 0.0, discounted reward: 0.05633311454061372\n",
      "401: reward: 0.0, discounted reward: 0.05800673615666705\n",
      "402: reward: 0.0, discounted reward: 0.05973211926600038\n",
      "403: reward: 0.0, discounted reward: 0.061510864739539914\n",
      "404: reward: 0.0, discounted reward: 0.06334462295968375\n",
      "405: reward: 0.0, discounted reward: 0.06523509535158462\n",
      "406: reward: 0.0, discounted reward: 0.06718403596179168\n",
      "407: reward: 0.0, discounted reward: 0.0691932530857165\n",
      "408: reward: 0.0, discounted reward: 0.07126461094543282\n",
      "409: reward: 0.0, discounted reward: 0.07340003141936717\n",
      "410: reward: 0.0, discounted reward: 0.07560149582548505\n",
      "411: reward: 0.0, discounted reward: 0.07787104675962718\n",
      "412: reward: 0.0, discounted reward: 0.08021078999070155\n",
      "413: reward: 0.0, discounted reward: 0.08262289641448957\n",
      "414: reward: 0.0, discounted reward: 0.08510960406787926\n",
      "415: reward: 0.0, discounted reward: 0.08767322020539443\n",
      "416: reward: 0.0, discounted reward: 0.09031612343994615\n",
      "417: reward: 0.0, discounted reward: 0.09304076594979327\n",
      "418: reward: 0.0, discounted reward: 0.09584967575375937\n",
      "419: reward: 0.0, discounted reward: 0.09874545905681721\n",
      "420: reward: 0.0, discounted reward: 0.10173080266821705\n",
      "421: reward: 0.0, discounted reward: 0.10480847649440246\n",
      "422: reward: 0.0, discounted reward: 0.10798133610902658\n",
      "423: reward: 0.0, discounted reward: 0.1112523254024535\n",
      "424: reward: 0.0, discounted reward: 0.11462447931320291\n",
      "425: reward: 0.0, discounted reward: 0.11810092664387241\n",
      "426: reward: 0.0, discounted reward: 0.12168489296415023\n",
      "427: reward: 0.0, discounted reward: 0.12537970360361192\n",
      "428: reward: 0.0, discounted reward: 0.12918878673707757\n",
      "429: reward: 0.0, discounted reward: 0.1331156765653927\n",
      "430: reward: 0.0, discounted reward: 0.13716401659458352\n",
      "431: reward: 0.0, discounted reward: 0.14133756301642975\n",
      "432: reward: 0.0, discounted reward: 0.1456401881935908\n",
      "433: reward: 0.0, discounted reward: 0.1500758842525197\n",
      "434: reward: 0.0, discounted reward: 0.15464876678749795\n",
      "435: reward: 0.0, discounted reward: 0.15936307867922814\n",
      "436: reward: 0.0, discounted reward: 0.1642231940315273\n",
      "437: reward: 0.0, discounted reward: 0.16923362222977384\n",
      "438: reward: 0.0, discounted reward: 0.17439901212487335\n",
      "439: reward: 0.0, discounted reward: 0.17972415634662545\n",
      "440: reward: 0.0, discounted reward: 0.1852139957504936\n",
      "441: reward: 0.0, discounted reward: 0.19087362400190402\n",
      "442: reward: 0.0, discounted reward: 0.19670829230232717\n",
      "443: reward: 0.0, discounted reward: 0.20272341426152632\n",
      "444: reward: 0.0, discounted reward: 0.2089245709204945\n",
      "445: reward: 0.0, discounted reward: 0.21531751592974002\n",
      "446: reward: 0.0, discounted reward: 0.2219081808877251\n",
      "447: reward: 0.0, discounted reward: 0.22870268084441076\n",
      "448: reward: 0.0, discounted reward: 0.2357073199750145\n",
      "449: reward: 0.0, discounted reward: 0.24292859742924522\n",
      "450: reward: 0.0, discounted reward: 0.2503732133614418\n",
      "451: reward: 0.0, discounted reward: 0.2580480751472115\n",
      "452: reward: 0.0, discounted reward: 0.2659603037923349\n",
      "453: reward: 0.0, discounted reward: 0.27411724053988473\n",
      "454: reward: 0.0, discounted reward: 0.2825264536816887\n",
      "455: reward: 0.0, discounted reward: 0.29119574558045574\n",
      "456: reward: 0.0, discounted reward: 0.3001331599090815\n",
      "457: reward: 0.0, discounted reward: 0.3093469891138504\n",
      "458: reward: 0.0, discounted reward: 0.31884578210845743\n",
      "459: reward: 0.0, discounted reward: 0.32863835220599047\n",
      "460: reward: 0.0, discounted reward: 0.3387337852962307\n",
      "461: reward: 0.0, discounted reward: 0.34914144827585986\n",
      "462: reward: 0.0, discounted reward: 0.35987099773939507\n",
      "463: reward: 0.0, discounted reward: 0.3709323889389159\n",
      "464: reward: 0.0, discounted reward: 0.3823358850208961\n",
      "465: reward: 0.0, discounted reward: 0.39409206654871076\n",
      "466: reward: 0.0, discounted reward: 0.4062118413196537\n",
      "467: reward: 0.0, discounted reward: 0.41870645448557425\n",
      "468: reward: 0.0, discounted reward: 0.43158749898652327\n",
      "469: reward: 0.0, discounted reward: 0.44486692630708924\n",
      "470: reward: 0.0, discounted reward: 0.45855705756540477\n",
      "471: reward: 5.0, discounted reward: 0.4726705949451115\n",
      "472: reward: 0.0, discounted reward: 0.2997975852602986\n",
      "473: reward: 0.0, discounted reward: 0.3090010358676824\n",
      "474: reward: 0.0, discounted reward: 0.31848912927735645\n",
      "475: reward: 0.0, discounted reward: 0.32827066887495854\n",
      "476: reward: 0.0, discounted reward: 0.33835473031578545\n",
      "477: reward: 0.0, discounted reward: 0.3487506699455039\n",
      "478: reward: 0.0, discounted reward: 0.35946813348129614\n",
      "479: reward: 0.0, discounted reward: 0.37051706496149434\n",
      "480: reward: 0.0, discounted reward: 0.3819077159720079\n",
      "481: reward: 0.0, discounted reward: 0.3936506551581044\n",
      "482: reward: 10.0, discounted reward: 0.4057567780303688\n",
      "483: reward: 0.0, discounted reward: 0.04339122063275437\n",
      "484: reward: 0.0, discounted reward: 0.04466457748877082\n",
      "485: reward: 0.0, discounted reward: 0.0459773165155919\n",
      "486: reward: 0.0, discounted reward: 0.04733065571850023\n",
      "487: reward: 0.0, discounted reward: 0.04872585077304491\n",
      "488: reward: 0.0, discounted reward: 0.05016419619010127\n",
      "489: reward: 0.0, discounted reward: 0.0516470265169635\n",
      "490: reward: 0.0, discounted reward: 0.05317571757558435\n",
      "491: reward: 0.0, discounted reward: 0.05475168773911101\n",
      "492: reward: 0.0, discounted reward: 0.056376399247901374\n",
      "493: reward: 0.0, discounted reward: 0.05805135956624197\n",
      "494: reward: 15.0, discounted reward: 0.05977812278102607\n",
      "495: reward: 0.0, discounted reward: -0.5007108536180888\n",
      "496: reward: 0.0, discounted reward: -0.5162653959657066\n",
      "497: reward: 0.0, discounted reward: -0.532301006633354\n",
      "498: reward: 0.0, discounted reward: -0.5488325640226811\n",
      "499: reward: 0.0, discounted reward: -0.5658754066920905\n",
      "500: reward: 0.0, discounted reward: -0.5834453475883888\n",
      "501: reward: 0.0, discounted reward: -0.6015586887185933\n",
      "502: reward: 0.0, discounted reward: -0.6202322362755051\n",
      "503: reward: 0.0, discounted reward: -0.6394833162310843\n",
      "504: reward: 0.0, discounted reward: -0.6593297904120938\n",
      "505: reward: 0.0, discounted reward: -0.6797900730729284\n",
      "506: reward: 0.0, discounted reward: -0.7008831479810053\n",
      "507: reward: 0.0, discounted reward: -0.7226285860305691\n",
      "508: reward: 0.0, discounted reward: -0.7450465634012534\n",
      "509: reward: 0.0, discounted reward: -0.7681578802782476\n",
      "510: reward: 20.0, discounted reward: -0.7919839801514373\n",
      "511: reward: 0.0, discounted reward: -1.5662391625937988\n",
      "512: reward: 0.0, discounted reward: -1.6147481887241706\n",
      "513: reward: 0.0, discounted reward: -1.6647574940132135\n",
      "514: reward: 0.0, discounted reward: -1.7163134788472785\n",
      "515: reward: 0.0, discounted reward: -1.7694639786762116\n",
      "516: reward: 0.0, discounted reward: -1.8242583083967614\n",
      "517: reward: 0.0, discounted reward: -1.880747308108668\n",
      "518: reward: 0.0, discounted reward: -1.9389833902858915\n",
      "519: reward: 0.0, discounted reward: -1.9990205884067405\n",
      "520: reward: 0.0, discounted reward: -2.060914607088028\n",
      "521: reward: 0.0, discounted reward: -2.124722873769768\n",
      "522: reward: 0.0, discounted reward: -2.1905045919983657\n",
      "523: reward: 0.0, discounted reward: -2.258320796357745\n",
      "524: reward: 0.0, discounted reward: -2.3282344090993727\n",
      "525: reward: 0.0, discounted reward: -2.4003102985237312\n",
      "526: reward: 0.0, discounted reward: -2.4746153391674\n",
      "527: reward: 0.0, discounted reward: -2.5512184738515944\n",
      "528: reward: 0.0, discounted reward: -2.6301907776497333\n",
      "529: reward: 0.0, discounted reward: -2.711605523833381\n",
      "530: reward: 0.0, discounted reward: -2.795538251857761\n",
      "531: reward: 0.0, discounted reward: -2.882066837449905\n",
      "532: reward: 0.0, discounted reward: -2.9712715648644865\n",
      "533: reward: 0.0, discounted reward: -3.063235201374364\n",
      "534: reward: 0.0, discounted reward: -3.1580430740649597\n",
      "535: reward: 0.0, discounted reward: -3.255783149003718\n",
      "536: reward: 0.0, discounted reward: -3.356546112858108\n",
      "537: reward: 0.0, discounted reward: -3.4604254570378914\n",
      "538: reward: 0.0, discounted reward: -3.56751756443973\n",
      "539: reward: 0.0, discounted reward: -3.6779217988746153\n",
      "540: reward: 0.0, discounted reward: -3.7917405972610947\n",
      "541: reward: 0.0, discounted reward: -3.9090795646698364\n",
      "542: reward: 0.0, discounted reward: -4.030047572307715\n",
      "543: reward: 0.0, discounted reward: -4.154756858532331\n",
      "544: reward: 0.0, discounted reward: -4.283323132990699\n",
      "545: reward: -121.66666666666667, discounted reward: -4.415865683978707\n",
      "546: reward: 0.0, discounted reward: 0.00812001758011258\n",
      "547: reward: 0.0, discounted reward: 0.008302512486047324\n",
      "548: reward: 0.0, discounted reward: 0.008490651564330565\n",
      "549: reward: 0.0, discounted reward: 0.0086846093769937\n",
      "550: reward: 0.0, discounted reward: 0.00888456588489384\n",
      "551: reward: 0.0, discounted reward: 0.009090706614687796\n",
      "552: reward: 0.0, discounted reward: 0.009303222830970227\n",
      "553: reward: 0.0, discounted reward: 0.009522311713735621\n",
      "554: reward: 0.0, discounted reward: 0.009748176541328809\n",
      "555: reward: 0.0, discounted reward: 0.009981026879053745\n",
      "556: reward: 0.0, discounted reward: 0.010221078773615536\n",
      "557: reward: 0.0, discounted reward: 0.010468554953576143\n",
      "558: reward: 0.0, discounted reward: 0.010723685036009761\n",
      "559: reward: 0.0, discounted reward: 0.010986705739549572\n",
      "560: reward: 0.0, discounted reward: 0.011257861104023603\n",
      "561: reward: 0.0, discounted reward: 0.011537402716883432\n",
      "562: reward: 0.0, discounted reward: 0.011825589946635831\n",
      "563: reward: 0.0, discounted reward: 0.012122690183493972\n",
      "564: reward: 0.0, discounted reward: 0.012428979087471441\n",
      "565: reward: 0.0, discounted reward: 0.012744740844149243\n",
      "566: reward: 0.0, discounted reward: 0.01307026842835316\n",
      "567: reward: 0.0, discounted reward: 0.013405863875986065\n",
      "568: reward: 0.0, discounted reward: 0.013751838564267411\n",
      "569: reward: 0.0, discounted reward: 0.014108513500639934\n",
      "570: reward: 0.0, discounted reward: 0.014476219620611606\n",
      "571: reward: 0.0, discounted reward: 0.014855298094809207\n",
      "572: reward: 0.0, discounted reward: 0.015246100645528383\n",
      "573: reward: 0.0, discounted reward: 0.015648989873073924\n",
      "574: reward: 0.0, discounted reward: 0.016064339592193038\n",
      "575: reward: 0.0, discounted reward: 0.016492535178913775\n",
      "576: reward: 0.0, discounted reward: 0.016933973928110413\n",
      "577: reward: 0.0, discounted reward: 0.017389065422127564\n",
      "578: reward: 0.0, discounted reward: 0.01785823191080504\n",
      "579: reward: 0.0, discounted reward: 0.018341908703256048\n",
      "580: reward: 0.0, discounted reward: 0.01884054457176224\n",
      "581: reward: 0.0, discounted reward: 0.019354602168160373\n",
      "582: reward: 0.0, discounted reward: 0.019884558453106906\n",
      "583: reward: 0.0, discounted reward: 0.020430905138618793\n",
      "584: reward: 0.0, discounted reward: 0.02099414914430115\n",
      "585: reward: 0.0, discounted reward: 0.021574813067685026\n",
      "586: reward: 0.0, discounted reward: 0.022173435669111705\n",
      "587: reward: 0.0, discounted reward: 0.022790572371613433\n",
      "588: reward: 0.0, discounted reward: 0.02342679577625439\n",
      "589: reward: 0.0, discounted reward: 0.02408269619341001\n",
      "590: reward: 0.0, discounted reward: 0.024758882190477664\n",
      "591: reward: 0.0, discounted reward: 0.025455981156526794\n",
      "592: reward: 0.0, discounted reward: 0.02617463988441249\n",
      "593: reward: 0.0, discounted reward: 0.02691552517089259\n",
      "594: reward: 0.0, discounted reward: 0.027679324435305068\n",
      "595: reward: 0.0, discounted reward: 0.02846674635737979\n",
      "596: reward: 0.0, discounted reward: 0.029278521534776406\n",
      "597: reward: 0.0, discounted reward: 0.030115403160958485\n",
      "598: reward: 0.0, discounted reward: 0.030978167724032793\n",
      "599: reward: 0.0, discounted reward: 0.03186761572720218\n",
      "600: reward: 0.0, discounted reward: 0.03278457243150052\n",
      "601: reward: 0.0, discounted reward: 0.03372988862149882\n",
      "602: reward: 0.0, discounted reward: 0.03470444139469293\n",
      "603: reward: 0.0, discounted reward: 0.03570913497530541\n",
      "604: reward: 0.0, discounted reward: 0.03674490155325643\n",
      "605: reward: 0.0, discounted reward: 0.03781270214908222\n",
      "606: reward: 0.0, discounted reward: 0.03891352750560366\n",
      "607: reward: 0.0, discounted reward: 0.04004839900717215\n",
      "608: reward: 0.0, discounted reward: 0.04121836962734585\n",
      "609: reward: 0.0, discounted reward: 0.042424524905875435\n",
      "610: reward: 0.0, discounted reward: 0.04366798395590594\n",
      "611: reward: 0.0, discounted reward: 0.04494990050232914\n",
      "612: reward: 0.0, discounted reward: 0.046271463952249965\n",
      "613: reward: 0.0, discounted reward: 0.04763390049856009\n",
      "614: reward: 0.0, discounted reward: 0.0490384742576427\n",
      "615: reward: 0.0, discounted reward: 0.050486488442263944\n",
      "616: reward: 0.0, discounted reward: 0.051979286570739455\n",
      "617: reward: 0.0, discounted reward: 0.05351825371349771\n",
      "618: reward: 0.0, discounted reward: 0.055104817778196945\n",
      "619: reward: 0.0, discounted reward: 0.05674045083458791\n",
      "620: reward: 0.0, discounted reward: 0.05842667048035179\n",
      "621: reward: 0.0, discounted reward: 0.06016504124918053\n",
      "622: reward: 0.0, discounted reward: 0.061957176062406034\n",
      "623: reward: 0.0, discounted reward: 0.06380473772552511\n",
      "624: reward: 0.0, discounted reward: 0.0657094404710087\n",
      "625: reward: 0.0, discounted reward: 0.06767305154882683\n",
      "626: reward: 0.0, discounted reward: 0.06969739286616511\n",
      "627: reward: 0.0, discounted reward: 0.07178434267785405\n",
      "628: reward: 0.0, discounted reward: 0.07393583732907978\n",
      "629: reward: 0.0, discounted reward: 0.07615387305199289\n",
      "630: reward: 0.0, discounted reward: 0.0784405078178827\n",
      "631: reward: 0.0, discounted reward: 0.08079786324663508\n",
      "632: reward: 0.0, discounted reward: 0.08322812657524578\n",
      "633: reward: 0.0, discounted reward: 0.08573355268721558\n",
      "634: reward: 0.0, discounted reward: 0.0883164662047102\n",
      "635: reward: 0.0, discounted reward: 0.09097926364542631\n",
      "636: reward: 0.0, discounted reward: 0.09372441564616459\n",
      "637: reward: 0.0, discounted reward: 0.09655446925517311\n",
      "638: reward: 0.0, discounted reward: 0.09947205029538807\n",
      "639: reward: 0.0, discounted reward: 0.10247986580076432\n",
      "640: reward: 0.0, discounted reward: 0.10558070652795634\n",
      "641: reward: 0.0, discounted reward: 0.10877744954568007\n",
      "642: reward: 0.0, discounted reward: 0.11207306090415814\n",
      "643: reward: 0.0, discounted reward: 0.11547059838712521\n",
      "644: reward: 0.0, discounted reward: 0.11897321434894695\n",
      "645: reward: 0.0, discounted reward: 0.12258415863948482\n",
      "646: reward: 0.0, discounted reward: 0.12630678161942077\n",
      "647: reward: 0.0, discounted reward: 0.1301445372688393\n",
      "648: reward: 0.0, discounted reward: 0.13410098639195114\n",
      "649: reward: 0.0, discounted reward: 0.13817979992093243\n",
      "650: reward: 0.0, discounted reward: 0.1423847623219441\n",
      "651: reward: 0.0, discounted reward: 0.14671977510649217\n",
      "652: reward: 0.0, discounted reward: 0.1511888604513871\n",
      "653: reward: 0.0, discounted reward: 0.15579616493066026\n",
      "654: reward: 0.0, discounted reward: 0.1605459633629006\n",
      "655: reward: 0.0, discounted reward: 0.16544266277758138\n",
      "656: reward: 0.0, discounted reward: 0.1704908065040564\n",
      "657: reward: 0.0, discounted reward: 0.17569507838702034\n",
      "658: reward: 0.0, discounted reward: 0.181060307132344\n",
      "659: reward: 0.0, discounted reward: 0.18659147078731683\n",
      "660: reward: 0.0, discounted reward: 0.19229370135945378\n",
      "661: reward: 0.0, discounted reward: 0.19817228957815167\n",
      "662: reward: 0.0, discounted reward: 0.2042326898036134\n",
      "663: reward: 0.0, discounted reward: 0.21048052508759454\n",
      "664: reward: 0.0, discounted reward: 0.2169215923906679\n",
      "665: reward: 0.0, discounted reward: 0.22356186796084665\n",
      "666: reward: 0.0, discounted reward: 0.23040751287855665\n",
      "667: reward: 0.0, discounted reward: 0.23746487877310307\n",
      "668: reward: 0.0, discounted reward: 0.24474051371593444\n",
      "669: reward: 0.0, discounted reward: 0.25224116829617294\n",
      "670: reward: 0.0, discounted reward: 0.25997380188404773\n",
      "671: reward: 0.0, discounted reward: 0.26794558908804234\n",
      "672: reward: 0.0, discounted reward: 0.2761639264117481\n",
      "673: reward: 0.0, discounted reward: 0.2846364391165994\n",
      "674: reward: 0.0, discounted reward: 0.2933709882968585\n",
      "675: reward: 0.0, discounted reward: 0.30237567817341426\n",
      "676: reward: 0.0, discounted reward: 0.3116588636131625\n",
      "677: reward: 0.0, discounted reward: 0.32122915788094414\n",
      "678: reward: 0.0, discounted reward: 0.3310954406312345\n",
      "679: reward: 0.0, discounted reward: 0.3412668661469978\n",
      "680: reward: 0.0, discounted reward: 0.3517528718333517\n",
      "681: reward: 0.0, discounted reward: 0.36256318697392276\n",
      "682: reward: 0.0, discounted reward: 0.3737078417580166\n",
      "683: reward: 0.0, discounted reward: 0.38519717658697933\n",
      "684: reward: 0.0, discounted reward: 0.3970418516683842\n",
      "685: reward: 0.0, discounted reward: 0.4092528569069459\n",
      "686: reward: 0.0, discounted reward: 0.42184152210133946\n",
      "687: reward: 0.0, discounted reward: 0.43481952745638436\n",
      "688: reward: 0.0, discounted reward: 0.4481989144203481\n",
      "689: reward: 0.0, discounted reward: 0.46199209685742415\n",
      "690: reward: 0.0, discounted reward: 0.47621187256575\n",
      "691: reward: 0.0, discounted reward: 0.49087143515165294\n",
      "692: reward: 0.0, discounted reward: 0.5059843862711405\n",
      "693: reward: 0.0, discounted reward: 0.5215647482499937\n",
      "694: reward: 0.0, discounted reward: 0.5376269770941722\n",
      "695: reward: 0.0, discounted reward: 0.5541859759026037\n",
      "696: reward: 0.0, discounted reward: 0.571257108694801\n",
      "697: reward: 0.0, discounted reward: 0.5888562146661385\n",
      "698: reward: 0.0, discounted reward: 0.6069996228840123\n",
      "699: reward: 0.0, discounted reward: 0.6257041674385213\n",
      "700: reward: 0.0, discounted reward: 0.6449872030617264\n",
      "701: reward: 0.0, discounted reward: 0.6648666212299791\n",
      "702: reward: 0.0, discounted reward: 0.6853608667642602\n",
      "703: reward: 0.0, discounted reward: 0.7064889549439314\n",
      "704: reward: 0.0, discounted reward: 0.728270489149778\n",
      "705: reward: 0.0, discounted reward: 0.7507256790527127\n",
      "706: reward: 0.0, discounted reward: 0.7738753593650166\n",
      "707: reward: 0.0, discounted reward: 0.7977410091715155\n",
      "708: reward: 0.0, discounted reward: 0.8223447718586275\n",
      "709: reward: 0.0, discounted reward: 0.8477094756597741\n",
      "710: reward: 0.0, discounted reward: 0.8738586548362138\n",
      "711: reward: 0.0, discounted reward: 0.9008165715129559\n",
      "712: reward: 0.0, discounted reward: 0.9286082381900096\n",
      "713: reward: 0.0, discounted reward: 0.9572594409498587\n",
      "714: reward: 0.0, discounted reward: 0.986796763382693\n",
      "715: reward: 0.0, discounted reward: 1.017247611251594\n",
      "716: reward: 0.0, discounted reward: 1.0486402379205644\n",
      "717: reward: 0.0, discounted reward: 1.0810037705689874\n",
      "718: reward: 0.0, discounted reward: 1.114368237216846\n",
      "719: reward: 0.0, discounted reward: 1.1487645945857725\n",
      "720: reward: 0.0, discounted reward: 1.1842247568217794\n",
      "721: reward: 0.0, discounted reward: 1.2207816251063226\n",
      "722: reward: 0.0, discounted reward: 1.2584691181831709\n",
      "723: reward: 0.0, discounted reward: 1.2973222038294066\n",
      "724: reward: 10.0, discounted reward: 1.3373769312997525\n",
      "725: reward: 0.0, discounted reward: 1.00382436833315\n",
      "726: reward: 0.0, discounted reward: 1.0348018431592818\n",
      "727: reward: 0.0, discounted reward: 1.0667373842171495\n",
      "728: reward: 0.0, discounted reward: 1.0996606224211372\n",
      "729: reward: 0.0, discounted reward: 1.1336021051056602\n",
      "730: reward: 0.0, discounted reward: 1.168593324368055\n",
      "731: reward: 0.0, discounted reward: 1.20466674628805\n",
      "732: reward: 0.0, discounted reward: 1.2418558410509315\n",
      "733: reward: 0.0, discounted reward: 1.2801951140023555\n",
      "734: reward: 0.0, discounted reward: 1.3197201376636174\n",
      "735: reward: 0.0, discounted reward: 1.3604675847370835\n",
      "736: reward: 0.0, discounted reward: 1.402475262132409\n",
      "737: reward: 15.0, discounted reward: 1.4457821460451157\n",
      "738: reward: 0.0, discounted reward: 0.9281592734582924\n",
      "739: reward: 0.0, discounted reward: 0.956796590710975\n",
      "740: reward: 0.0, discounted reward: 0.9863195981879673\n",
      "741: reward: 0.0, discounted reward: 1.0167556883704336\n",
      "742: reward: 0.0, discounted reward: 1.0481331009296773\n",
      "743: reward: 0.0, discounted reward: 1.0804809489288976\n",
      "744: reward: 0.0, discounted reward: 1.1138292458353103\n",
      "745: reward: 0.0, discounted reward: 1.1482089333676944\n",
      "746: reward: 0.0, discounted reward: 1.1836519102052039\n",
      "747: reward: 0.0, discounted reward: 1.2201910615840794\n",
      "748: reward: 0.0, discounted reward: 1.2578602898097246\n",
      "749: reward: 0.0, discounted reward: 1.2966945457124517\n",
      "750: reward: 0.0, discounted reward: 1.3367298610760878\n",
      "751: reward: 20.0, discounted reward: 1.3780033820695268\n",
      "752: reward: 0.0, discounted reward: 0.6708612108298758\n",
      "753: reward: 0.0, discounted reward: 0.6915408560425043\n",
      "754: reward: 0.0, discounted reward: 0.7128600779111933\n",
      "755: reward: 0.0, discounted reward: 0.7348386571572646\n",
      "756: reward: 0.0, discounted reward: 0.7574969862769257\n",
      "757: reward: 0.0, discounted reward: 0.7808560884621434\n",
      "758: reward: 0.0, discounted reward: 0.8049376371066975\n",
      "759: reward: 0.0, discounted reward: 0.8297639759155164\n",
      "760: reward: 0.0, discounted reward: 0.8553581396359482\n",
      "761: reward: 0.0, discounted reward: 0.8817438754302077\n",
      "762: reward: 0.0, discounted reward: 0.9089456649088259\n",
      "763: reward: 0.0, discounted reward: 0.9369887468455457\n",
      "764: reward: 0.0, discounted reward: 0.9658991405947412\n",
      "765: reward: 0.0, discounted reward: 0.9957036702330873\n",
      "766: reward: 0.0, discounted reward: 1.0264299894478768\n",
      "767: reward: 0.0, discounted reward: 1.0581066071950826\n",
      "768: reward: 0.0, discounted reward: 1.090762914150965\n",
      "769: reward: 25.0, discounted reward: 1.1244292099817714\n",
      "770: reward: 0.0, discounted reward: 0.22202149035386481\n",
      "771: reward: 0.0, discounted reward: 0.22881949472702903\n",
      "772: reward: 0.0, discounted reward: 0.23582774665812614\n",
      "773: reward: 0.0, discounted reward: 0.2430527486489479\n",
      "774: reward: 0.0, discounted reward: 0.2505012043095889\n",
      "775: reward: 0.0, discounted reward: 0.258180024578291\n",
      "776: reward: 0.0, discounted reward: 0.2660963341336539\n",
      "777: reward: 0.0, discounted reward: 0.2742574780051621\n",
      "778: reward: 0.0, discounted reward: 0.2826710283881602\n",
      "779: reward: 0.0, discounted reward: 0.29134479166960164\n",
      "780: reward: 0.0, discounted reward: 0.30028681567108756\n",
      "781: reward: 0.0, discounted reward: 0.30950539711591846\n",
      "782: reward: 0.0, discounted reward: 0.31900908932708427\n",
      "783: reward: 0.0, discounted reward: 0.3288067101633377\n",
      "784: reward: 0.0, discounted reward: 0.3389073502007124\n",
      "785: reward: 5.0, discounted reward: 0.3493203811670781\n",
      "786: reward: 0.0, discounted reward: 0.1726324164169651\n",
      "787: reward: 0.0, discounted reward: 0.17790292365806026\n",
      "788: reward: 0.0, discounted reward: 0.18333643627774598\n",
      "789: reward: 0.0, discounted reward: 0.18893799567948383\n",
      "790: reward: 0.0, discounted reward: 0.19471279918643006\n",
      "791: reward: 0.0, discounted reward: 0.20066620486369421\n",
      "792: reward: 0.0, discounted reward: 0.20680373648973974\n",
      "793: reward: 0.0, discounted reward: 0.21313108868153924\n",
      "794: reward: 0.0, discounted reward: 0.21965413217823973\n",
      "795: reward: 0.0, discounted reward: 0.22637891928824028\n",
      "796: reward: 0.0, discounted reward: 0.2333116895047357\n",
      "797: reward: 0.0, discounted reward: 0.24045887529493712\n",
      "798: reward: 0.0, discounted reward: 0.24782710806834066\n",
      "799: reward: 0.0, discounted reward: 0.2554232243295814\n",
      "800: reward: 0.0, discounted reward: 0.26325427202158225\n",
      "801: reward: 0.0, discounted reward: 0.271327517064882\n",
      "802: reward: 0.0, discounted reward: 0.2796504500992117\n",
      "803: reward: 0.0, discounted reward: 0.2882307934335722\n",
      "804: reward: 0.0, discounted reward: 0.29707650821126347\n",
      "805: reward: 0.0, discounted reward: 0.3061958017965122\n",
      "806: reward: 0.0, discounted reward: 0.3155971353895521\n",
      "807: reward: 0.0, discounted reward: 0.3252892318772221\n",
      "808: reward: 0.0, discounted reward: 0.3352810839263664\n",
      "809: reward: 0.0, discounted reward: 0.3455819623275461\n",
      "810: reward: 0.0, discounted reward: 0.35620142459680354\n",
      "811: reward: 0.0, discounted reward: 0.3671493238434607\n",
      "812: reward: 0.0, discounted reward: 0.3784358179121794\n",
      "813: reward: 0.0, discounted reward: 0.3900713788077657\n",
      "814: reward: 0.0, discounted reward: 0.402066802411463\n",
      "815: reward: 0.0, discounted reward: 0.4144332184977488\n",
      "816: reward: 0.0, discounted reward: 0.42718210106092996\n",
      "817: reward: 0.0, discounted reward: 0.44032527896111684\n",
      "818: reward: 0.0, discounted reward: 0.4538749468994538\n",
      "819: reward: 0.0, discounted reward: 0.46784367673279087\n",
      "820: reward: 0.0, discounted reward: 0.482244429138293\n",
      "821: reward: 0.0, discounted reward: 0.4970905656388107\n",
      "822: reward: 0.0, discounted reward: 0.5123958610001691\n",
      "823: reward: 0.0, discounted reward: 0.5281745160118788\n",
      "824: reward: 0.0, discounted reward: 0.5444411706631259\n",
      "825: reward: 0.0, discounted reward: 0.5612109177262673\n",
      "826: reward: 0.0, discounted reward: 0.5784993167604336\n",
      "827: reward: 0.0, discounted reward: 0.596322408548234\n",
      "828: reward: 0.0, discounted reward: 0.6146967299789561\n",
      "829: reward: 0.0, discounted reward: 0.6336393293920716\n",
      "830: reward: 0.0, discounted reward: 0.6531677823952833\n",
      "831: reward: 0.0, discounted reward: 0.6733002081717904\n",
      "832: reward: 0.0, discounted reward: 0.6940552862919007\n",
      "833: reward: 0.0, discounted reward: 0.7154522740445919\n",
      "834: reward: 0.0, discounted reward: 0.7375110243050982\n",
      "835: reward: 0.0, discounted reward: 0.7602520039551046\n",
      "836: reward: 0.0, discounted reward: 0.7836963128726371\n",
      "837: reward: 0.0, discounted reward: 0.8078657035092686\n",
      "838: reward: 0.0, discounted reward: 0.8327826010728061\n",
      "839: reward: 0.0, discounted reward: 0.8584701243341849\n",
      "840: reward: 0.0, discounted reward: 0.8849521070778745\n",
      "841: reward: 0.0, discounted reward: 0.9122531202156988\n",
      "842: reward: 0.0, discounted reward: 0.9403984945845898\n",
      "843: reward: 0.0, discounted reward: 0.969414344449426\n",
      "844: reward: 0.0, discounted reward: 0.9993275917327622\n",
      "845: reward: 0.0, discounted reward: 1.0301659909939336\n",
      "846: reward: 0.0, discounted reward: 1.061958155180708\n",
      "847: reward: 0.0, discounted reward: 1.094733582177383\n",
      "848: reward: 0.0, discounted reward: 1.128522682173955\n",
      "849: reward: 0.0, discounted reward: 1.1633568058817612\n",
      "850: reward: 0.0, discounted reward: 1.1992682736217677\n",
      "851: reward: 10.0, discounted reward: 1.2362904053124957\n",
      "852: reward: 0.0, discounted reward: 0.8996114549442253\n",
      "853: reward: 0.0, discounted reward: 0.9273658499748234\n",
      "854: reward: 0.0, discounted reward: 0.9559786283568833\n",
      "855: reward: 0.0, discounted reward: 0.9854763380291098\n",
      "856: reward: 0.0, discounted reward: 1.0158863480004774\n",
      "857: reward: 0.0, discounted reward: 1.0472368737441555\n",
      "858: reward: 0.0, discounted reward: 1.0795570033768134\n",
      "859: reward: 0.0, discounted reward: 1.1128767246475944\n",
      "860: reward: 0.0, discounted reward: 1.147226952761802\n",
      "861: reward: 0.0, discounted reward: 1.1826395590651084\n",
      "862: reward: 15.0, discounted reward: 1.219147400614909\n",
      "863: reward: 0.0, discounted reward: 0.6945152060044711\n",
      "864: reward: 0.0, discounted reward: 0.7159264180781695\n",
      "865: reward: 5.0, discounted reward: 0.737999832587137\n",
      "866: reward: 0.0, discounted reward: 0.5733328817984692\n",
      "867: reward: 0.0, discounted reward: 0.5909961869379614\n",
      "868: reward: 0.0, discounted reward: 0.609205779865273\n",
      "869: reward: 0.0, discounted reward: 0.6279785560789962\n",
      "870: reward: 0.0, discounted reward: 0.6473319336189172\n",
      "871: reward: 0.0, discounted reward: 0.667283869227083\n",
      "872: reward: 0.0, discounted reward: 0.6878528750086973\n",
      "873: reward: 0.0, discounted reward: 0.7090580356082996\n",
      "874: reward: 0.0, discounted reward: 0.7309190259171681\n",
      "875: reward: 0.0, discounted reward: 0.7534561293283725\n",
      "876: reward: 0.0, discounted reward: 0.7766902565564184\n",
      "877: reward: 0.0, discounted reward: 0.80064296503894\n",
      "878: reward: 20.0, discounted reward: 0.8253364789384466\n",
      "879: reward: 0.0, discounted reward: 0.10110151688030852\n",
      "880: reward: 0.0, discounted reward: 0.1041597282594452\n",
      "881: reward: 0.0, discounted reward: 0.10731252349566858\n",
      "882: reward: 0.0, discounted reward: 0.11056282786290918\n",
      "883: reward: 0.0, discounted reward: 0.11391365710748713\n",
      "884: reward: 0.0, discounted reward: 0.11736812024622727\n",
      "885: reward: 0.0, discounted reward: 0.12092942245111403\n",
      "886: reward: 0.0, discounted reward: 0.12460086802316221\n",
      "887: reward: 0.0, discounted reward: 0.12838586345826344\n",
      "888: reward: 0.0, discounted reward: 0.13228792060785236\n",
      "889: reward: 0.0, discounted reward: 0.13631065993732544\n",
      "890: reward: 0.0, discounted reward: 0.14045781388523584\n",
      "891: reward: 0.0, discounted reward: 0.14473323032638058\n",
      "892: reward: 0.0, discounted reward: 0.1491408761419937\n",
      "893: reward: 0.0, discounted reward: 0.15368484090035775\n",
      "894: reward: 0.0, discounted reward: 0.15836934065124852\n",
      "895: reward: 0.0, discounted reward: 0.16319872183773387\n",
      "896: reward: 0.0, discounted reward: 0.16817746532895586\n",
      "897: reward: 0.0, discounted reward: 0.17331019057763836\n",
      "898: reward: 0.0, discounted reward: 0.17860165990617702\n",
      "899: reward: 0.0, discounted reward: 0.184056782925289\n",
      "900: reward: 0.0, discounted reward: 0.189680621089322\n",
      "901: reward: 0.0, discounted reward: 0.19547839239244877\n",
      "902: reward: 0.0, discounted reward: 0.20145547621010526\n",
      "903: reward: 0.0, discounted reward: 0.2076174182901635\n",
      "904: reward: 0.0, discounted reward: 0.21396993589847096\n",
      "905: reward: 0.0, discounted reward: 0.2205189231235302\n",
      "906: reward: 0.0, discounted reward: 0.22727045634524073\n",
      "907: reward: 0.0, discounted reward: 0.2342307998727774\n",
      "908: reward: 0.0, discounted reward: 0.2414064117568358\n",
      "909: reward: 0.0, discounted reward: 0.24880394978163828\n",
      "910: reward: 0.0, discounted reward: 0.2564302776422594\n",
      "911: reward: 0.0, discounted reward: 0.2642924713130028\n",
      "912: reward: 0.0, discounted reward: 0.2723978256127383\n",
      "913: reward: 0.0, discounted reward: 0.2807538609732903\n",
      "914: reward: 0.0, discounted reward: 0.2893683304171584\n",
      "915: reward: 0.0, discounted reward: 0.29824922675104304\n",
      "916: reward: 0.0, discounted reward: 0.30740478998185194\n",
      "917: reward: 0.0, discounted reward: 0.31684351496206725\n",
      "918: reward: 0.0, discounted reward: 0.3265741592715677\n",
      "919: reward: 0.0, discounted reward: 0.3366057513432175\n",
      "920: reward: 0.0, discounted reward: 0.3469475988397638\n",
      "921: reward: 0.0, discounted reward: 0.35760929728981145\n",
      "922: reward: 25.0, discounted reward: 0.3686007389908916\n",
      "923: reward: 0.0, discounted reward: -0.5571831189150835\n",
      "924: reward: 0.0, discounted reward: -0.574484226168794\n",
      "925: reward: 0.0, discounted reward: -0.5923204192138564\n",
      "926: reward: 0.0, discounted reward: -0.610708247095364\n",
      "927: reward: 0.0, discounted reward: -0.6296647706845472\n",
      "928: reward: 0.0, discounted reward: -0.6492075785084472\n",
      "929: reward: 0.0, discounted reward: -0.669354803069169\n",
      "930: reward: 0.0, discounted reward: -0.6901251376678512\n",
      "931: reward: 0.0, discounted reward: -0.7115378537489669\n",
      "932: reward: 0.0, discounted reward: -0.733612818781045\n",
      "933: reward: 0.0, discounted reward: -0.7563705146904038\n",
      "934: reward: 0.0, discounted reward: -0.7798320568650006\n",
      "935: reward: 0.0, discounted reward: -0.804019213746028\n",
      "936: reward: 0.0, discounted reward: -0.8289544270254379\n",
      "937: reward: 0.0, discounted reward: -0.8546608324681284\n",
      "938: reward: 0.0, discounted reward: -0.8811622813781187\n",
      "939: reward: 0.0, discounted reward: -0.9084833627286241\n",
      "940: reward: 0.0, discounted reward: -0.936649425976568\n",
      "941: reward: 0.0, discounted reward: -0.9656866045826955\n",
      "942: reward: 0.0, discounted reward: -0.9956218402591157\n",
      "943: reward: 0.0, discounted reward: -1.0264829079667654\n",
      "944: reward: 30.0, discounted reward: -1.058298441685992\n",
      "945: reward: 0.0, discounted reward: -2.2156362503077216\n",
      "946: reward: 0.0, discounted reward: -2.284229722449864\n",
      "947: reward: 0.0, discounted reward: -2.354944642184032\n",
      "948: reward: 0.0, discounted reward: -2.427846621291421\n",
      "949: reward: 0.0, discounted reward: -2.503003300783575\n",
      "950: reward: 0.0, discounted reward: -2.5804844136620844\n",
      "951: reward: 0.0, discounted reward: -2.6603618496193104\n",
      "952: reward: 0.0, discounted reward: -2.7427097217401624\n",
      "953: reward: 0.0, discounted reward: -2.8276044352668137\n",
      "954: reward: 0.0, discounted reward: -2.915124758490166\n",
      "955: reward: 0.0, discounted reward: -3.0053518958338272\n",
      "956: reward: 0.0, discounted reward: -3.0983695631984265\n",
      "957: reward: 0.0, discounted reward: -3.194264065636158\n",
      "958: reward: 0.0, discounted reward: -3.2931243774276338\n",
      "959: reward: 0.0, discounted reward: -3.3950422246353407\n",
      "960: reward: 0.0, discounted reward: -3.5001121702102966\n",
      "961: reward: 0.0, discounted reward: -3.6084317017308694\n",
      "962: reward: 0.0, discounted reward: -3.7201013218551715\n",
      "963: reward: 0.0, discounted reward: -3.8352246415709463\n",
      "964: reward: 0.0, discounted reward: -3.953908476329478\n",
      "965: reward: 0.0, discounted reward: -4.076262945152706\n",
      "966: reward: 0.0, discounted reward: -4.202401572805519\n",
      "967: reward: 0.0, discounted reward: -4.332441395128005\n",
      "968: reward: 0.0, discounted reward: -4.466503067625414\n",
      "969: reward: 0.0, discounted reward: -4.604710977416557\n",
      "970: reward: -121.66666666666667, discounted reward: -4.747193358644541\n",
      "971: reward: 0.0, discounted reward: -0.3334549047557984\n",
      "972: reward: 0.0, discounted reward: -0.34383658270561357\n",
      "973: reward: 0.0, discounted reward: -0.3545393434786188\n",
      "974: reward: 0.0, discounted reward: -0.36557311747140775\n",
      "975: reward: 0.0, discounted reward: -0.37694814220624173\n",
      "976: reward: 0.0, discounted reward: -0.3886749718297819\n",
      "977: reward: 0.0, discounted reward: -0.40076448690559646\n",
      "978: reward: 0.0, discounted reward: -0.41322790450952895\n",
      "979: reward: 0.0, discounted reward: -0.4260767886372945\n",
      "980: reward: 0.0, discounted reward: -0.43932306093396\n",
      "981: reward: 0.0, discounted reward: -0.4529790117552646\n",
      "982: reward: 0.0, discounted reward: -0.4670573115710426\n",
      "983: reward: 0.0, discounted reward: -0.4815710227213292\n",
      "984: reward: 0.0, discounted reward: -0.4965336115360576\n",
      "985: reward: 0.0, discounted reward: -0.511958960829592\n",
      "986: reward: 0.0, discounted reward: -0.5278613827816894\n",
      "987: reward: 0.0, discounted reward: -0.5442556322168413\n",
      "988: reward: 0.0, discounted reward: -0.5611569202943175\n",
      "989: reward: 0.0, discounted reward: -0.5785809286216126\n",
      "990: reward: 0.0, discounted reward: -0.596543823804391\n",
      "991: reward: 0.0, discounted reward: -0.6150622724464306\n",
      "992: reward: 0.0, discounted reward: -0.6341534566134818\n",
      "993: reward: 0.0, discounted reward: -0.6538350897753902\n",
      "994: reward: 0.0, discounted reward: -0.6741254332412752\n",
      "995: reward: 0.0, discounted reward: -0.6950433131030123\n",
      "996: reward: 0.0, discounted reward: -0.7166081377027412\n",
      "997: reward: 0.0, discounted reward: -0.738839915640606\n",
      "998: reward: 0.0, discounted reward: -0.7617592743394359\n",
      "999: reward: 0.0, discounted reward: -0.7853874791835903\n",
      "1000: reward: 0.0, discounted reward: -0.8097464532497288\n",
      "1001: reward: 0.0, discounted reward: -0.8348587976478099\n",
      "1002: reward: 0.0, discounted reward: -0.8607478124911924\n",
      "1003: reward: 0.0, discounted reward: -0.8874375185152981\n",
      "1004: reward: 0.0, discounted reward: -0.9149526793648914\n",
      "1005: reward: 0.0, discounted reward: -0.943318824570658\n",
      "1006: reward: 0.0, discounted reward: -0.9725622732363965\n",
      "1007: reward: 0.0, discounted reward: -1.0027101584588074\n",
      "1008: reward: 0.0, discounted reward: -1.03379045250253\n",
      "1009: reward: 0.0, discounted reward: -1.0658319927537905\n",
      "1010: reward: 0.0, discounted reward: -1.0988645084767394\n",
      "1011: reward: 0.0, discounted reward: -1.1329186483973053\n",
      "1012: reward: 0.0, discounted reward: -1.1680260091401566\n",
      "1013: reward: 0.0, discounted reward: -1.2042191645451579\n",
      "1014: reward: 0.0, discounted reward: -1.2415316958905203\n",
      "1015: reward: 0.0, discounted reward: -1.2799982230506877\n",
      "1016: reward: 0.0, discounted reward: -1.3196544366178704\n",
      "1017: reward: 0.0, discounted reward: -1.360537131017028\n",
      "1018: reward: 0.0, discounted reward: -1.4026842386450253\n",
      "1019: reward: 0.0, discounted reward: -1.4461348650656414\n",
      "1020: reward: 0.0, discounted reward: -1.4909293252930806\n",
      "1021: reward: 0.0, discounted reward: -1.537109181197657\n",
      "1022: reward: 0.0, discounted reward: -1.5847172800683544\n",
      "1023: reward: 0.0, discounted reward: -1.6337977943680422\n",
      "1024: reward: 0.0, discounted reward: -1.684396262718236\n",
      "1025: reward: 0.0, discounted reward: -1.7365596321514256\n",
      "1026: reward: 0.0, discounted reward: -1.7903363016701774\n",
      "1027: reward: 0.0, discounted reward: -1.8457761671534272\n",
      "1028: reward: 10.0, discounted reward: -1.9029306676516227\n",
      "1029: reward: 0.0, discounted reward: -2.3366989295548657\n",
      "1030: reward: 0.0, discounted reward: -2.4090366082716623\n",
      "1031: reward: 0.0, discounted reward: -2.4836115347838237\n",
      "1032: reward: 0.0, discounted reward: -2.560492902322135\n",
      "1033: reward: 0.0, discounted reward: -2.6397520441142075\n",
      "1034: reward: 0.0, discounted reward: -2.721462499569953\n",
      "1035: reward: 0.0, discounted reward: -2.8057000825140204\n",
      "1036: reward: 0.0, discounted reward: -2.8925429515285233\n",
      "1037: reward: 0.0, discounted reward: -2.982071682471309\n",
      "1038: reward: 0.0, discounted reward: -3.074369343237068\n",
      "1039: reward: 0.0, discounted reward: -3.1695215708306335\n",
      "1040: reward: 0.0, discounted reward: -3.2676166508240003\n",
      "1041: reward: 15.0, discounted reward: -3.3687455992707704\n",
      "1042: reward: 0.0, discounted reward: -4.035271391815817\n",
      "1043: reward: 0.0, discounted reward: -4.160142239468519\n",
      "1044: reward: 0.0, discounted reward: -4.288875072100171\n",
      "1045: reward: -121.66666666666667, discounted reward: -4.421589332545174\n",
      "1046: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1047: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1048: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1049: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1050: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1051: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1052: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1053: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1054: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1055: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1056: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1057: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1058: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1059: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1060: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1061: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1062: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1063: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1064: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1065: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1066: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1067: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1068: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1069: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1070: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1071: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1072: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1073: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1074: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1075: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1076: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1077: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1078: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1079: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1080: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1081: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1082: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1083: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1084: reward: 0.0, discounted reward: 0.0022193489548892084\n"
     ]
    }
   ],
   "source": [
    "print(\"num_frames: \" + str(num_frames))\n",
    "\n",
    "\n",
    "#show_observation(frames[:,:,125])\n",
    "#show_observation(concatenated_frames)\n",
    "\n",
    "\n",
    "# print(\"rewards: \" + str(rewards))\n",
    "# print(\"discounted_rewards: \" + str(discounted_rewards))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(\"action_taken_one_hot_reshaped: \" + str(action_taken_one_hot_reshaped))\n",
    "#rint(\"all_logits: \" + str(all_logits))\n",
    "\n",
    "\n",
    "\n",
    "# num_frames = np.ma.size(frames, axis=2)\n",
    "\n",
    "# X_input = np.empty([n_steps, 210 * 160])\n",
    "# input_index = 0\n",
    "\n",
    "# for i in range(num_frames, num_frames-n_steps, -1):\n",
    "#     this_frame = frames[:,:,i-1]\n",
    "#     X_input[input_index] = this_frame.flatten()\n",
    "#     input_index = input_index + 1\n",
    "    \n",
    "#obs_greyscale_reshape.shape\n",
    "\n",
    "\n",
    "\n",
    "#show_observation(obs_greyscale_reshape[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_input = np.empty([n_steps, 10])\n",
    "for i in range(0,  29):\n",
    "    y_input[i] = np.zeros(10)\n",
    "    \n",
    "y_input  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#len(rewards)\n",
    "#len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 180.31735229, -245.90328979,  -55.46500015,  -39.02265549,\n",
       "        -171.08708191,   39.90904999, -253.30400085,   10.52585697,\n",
       "           9.43663216], dtype=float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(all_gradients)\n",
    "np.shape(all_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getsizeof(all_variables))\n",
    "#np.shape(all_gradients[1])\n",
    "#all_gradients\n",
    "#all_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8392"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "def get_size(obj, seen=None):\n",
    "    \"\"\"Recursively finds size of objects\"\"\"\n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    # Important mark as seen *before* entering recursion to gracefully handle\n",
    "    # self-referential objects\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "    return size\n",
    "\n",
    "get_size(all_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
