{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs.shape: (210, 160, 3)\n",
      "env.action_space: Discrete(6)\n",
      "[0.11849965 0.00589975 0.8756006 ]\n",
      "test_softmax: [0.13103449 0.13103449 0.13824064 0.13103449 0.33762142 0.13103449]\n",
      "multinomial_action_array: [0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "\n",
    "# To plot pretty figures and animations\n",
    "%matplotlib nbagg\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "####### Space Invaders\n",
    "load_model           = True\n",
    "environment_name     = \"SpaceInvaders-v0\"\n",
    "discrete_actions     = 6\n",
    "saver_file           = \"./models/space_invaders_few_layers_rl\"\n",
    "height               = 210\n",
    "width                = 160\n",
    "channels             = 1\n",
    "frames_captured      = 5\n",
    "learning_rate        = .00000002  #.00000002\n",
    "n_epochs             = 1001\n",
    "use_ai_every_x_epoch = 1\n",
    "discount_decay_rate  = 0.95\n",
    "frame_limit          = 2000\n",
    "max_score            = 5\n",
    "\n",
    "####### Pitfall\n",
    "# environment_name     = \"Pitfall-v0\"\n",
    "# discrete_actions     = 18\n",
    "# load_model           = False\n",
    "# saver_file           = \"./models/pitfall_rl\"\n",
    "# height               = 210\n",
    "# width                = 160\n",
    "# channels             = 1\n",
    "# frames_captured      = 5\n",
    "# learning_rate        =.00001\n",
    "# n_epochs             = 11\n",
    "# use_ai_every_x_epoch = 5\n",
    "# discount_decay_rate  = 0.95\n",
    "# frame_limit          = 1000\n",
    "\n",
    "####### River Raid\n",
    "# environment_name     = \"Riverraid-v0\"\n",
    "# discrete_actions     = 18\n",
    "# load_model           = True\n",
    "# saver_file           = \"./models/pitfall_rl\"\n",
    "# height               = 210\n",
    "# width                = 160\n",
    "# channels             = 1\n",
    "# frames_captured      = 5\n",
    "# learning_rate        =.00001\n",
    "# n_epochs             = 501\n",
    "# use_ai_every_x_epoch = 5\n",
    "# discount_decay_rate  = 0.95\n",
    "# frame_limit          = 1000\n",
    "\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    img = obs\n",
    "    img = img.mean(axis=2) # to greyscale\n",
    "    #img = (img - 128) / 128 - 1 # normalize from -1. to 1.\n",
    "    img = img / 256.0  # normalize from 0 to 1.\n",
    "    return img\n",
    "\n",
    "def show_observation(image, title=\"Image\"):\n",
    "    plt.figure(figsize=(11, 7))\n",
    "    plt.subplot(121)\n",
    "    plt.title(title)\n",
    "    plt.imshow(image) #cmap=\"gray\"\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x)\n",
    "    sum_ex = np.sum( np.exp(x))\n",
    "    return ex/sum_ex\n",
    "    \n",
    "env = gym.make(environment_name)\n",
    "observation = env.reset()\n",
    "print(\"obs.shape: {}\".format(observation.shape)) #obs.shape: (210, 160, 3)\n",
    "print(\"env.action_space: {}\".format(env.action_space)) #env.action_space: Discrete(9)\n",
    "\n",
    "for step in range(102):\n",
    "    observation, reward_float, done_bool, info_dict = env.step(1)\n",
    "    obs_greyscale = preprocess_observation(observation)\n",
    "\n",
    "#show_observation(observation)\n",
    "#show_observation(obs_greyscale)\n",
    "    \n",
    "print (softmax([1,-2,3]))\n",
    "\n",
    "test_softmax = softmax([4.3210541e-25, 5.4929095e-33, 5.3535387e-02, 1.2303401e-42, 9.4646466e-01, 1.9473004e-27])\n",
    "print (\"test_softmax: {}\".format(test_softmax))\n",
    "\n",
    "multinomial_action_array = np.random.multinomial(1, test_softmax)\n",
    "print (\"multinomial_action_array: {}\".format(multinomial_action_array))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "eps_min = 0.00\n",
    "eps_max = 0.5\n",
    "eps_decay_steps = 25000\n",
    "\n",
    "def helper_discount_rewards(rewards, discount_rate, begin_index, end_index):\n",
    "    '''\n",
    "    Takes in rewards and applies discount rate\n",
    "    '''\n",
    "    discounted_rewards = np.zeros(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "        \n",
    "    relevant_discounted_rewards = discounted_rewards[begin_index:end_index]\n",
    "    \n",
    "    reward_mean = relevant_discounted_rewards.mean()\n",
    "    reward_std = relevant_discounted_rewards.std()\n",
    "        \n",
    "    #return discounted_rewards\n",
    "    return [(discounted_reward - reward_mean)/reward_std for discounted_reward in discounted_rewards]\n",
    "\n",
    "def epsilon_greedy(optimal_action, number_outputs, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    \n",
    "    if step % 1000 == 0:\n",
    "        print(\"step: \" + str(step) + \", epsilon: \" + str(epsilon))\n",
    "        \n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(number_outputs-1) # random action\n",
    "    else:\n",
    "        return optimal_action # optimal action\n",
    "\n",
    "# print(\"epsilon_greedy(4): \" + str(epsilon_greedy(4,9,50))) \n",
    "# print(\"epsilon_greedy(4): \" + str(epsilon_greedy(4,9,5000)))\n",
    "# print(\"epsilon_greedy(4): \" + str(epsilon_greedy(4,9,500000)))\n",
    "    \n",
    "def action_to_one_hot(action, possible_action_count):\n",
    "    #9 possible positions of the joystick \n",
    "    #(0=center, 1=up, 2=right, 3=left, 4=down, 5=upper-right, 6=upper-left, 7=lower-right, 8=lower-left)\n",
    "    \n",
    "    return_array = np.zeros(possible_action_count)\n",
    "    action_int = int(action)\n",
    "    \n",
    "    return_array[action_int] = 1.0\n",
    "\n",
    "    return return_array\n",
    "\n",
    "def get_average_logits (logits_list, discounted_rewards):\n",
    "\n",
    "    logit_sums = np.zeros(len(logits_list[0][0]))\n",
    "    logit_sums_counter = np.ones(len(logits_list[0][0]))\n",
    "\n",
    "    for this_logit, this_reward in zip(logits_list, discounted_rewards):\n",
    "        temp_array = np.zeros(len(logits_list[0][0]))\n",
    "        temp_counter_array = np.zeros(len(logits_list[0][0]))\n",
    "        \n",
    "        action = np.argmax(this_logit)\n",
    "        temp_array[action] = this_logit[0][action]\n",
    "        logit_sums = logit_sums + temp_array*this_reward\n",
    "        temp_counter_array[action] = 1\n",
    "        logit_sums_counter = logit_sums_counter + temp_counter_array\n",
    "        \n",
    "    return (logit_sums/logit_sums_counter)\n",
    "\n",
    "\n",
    "# print(\"action_to_one_hot(3, 9): \" + str(action_to_one_hot(3.0, 9)))\n",
    "# print(\"action_to_one_hot(9, 9): \" + str(action_to_one_hot(8, 9)))\n",
    "\n",
    "#print(get_average_logits(all_logits, discounted_rewards))\n",
    "\n",
    "\n",
    "#print(all_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_fmaps = 32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 2\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "conv2_fmaps = 64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "n_hidden_in = 64 * 263 * 40  # conv3 has 64 maps of 525x80 each\n",
    "\n",
    "n_inputs  = discrete_actions\n",
    "n_outputs = discrete_actions\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 256\n",
    "n_hidden3 = 256\n",
    "n_hidden4 = 256\n",
    "n_hidden5 = 256\n",
    "n_hidden6 = 256\n",
    "n_hidden7 = 128\n",
    "n_hidden8 = 64\n",
    "\n",
    "dropout_keep_prob = 1.0\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#with tf.name_scope(\"inputs\"):\n",
    "#tf_input_frame = tf.placeholder(tf.float32, shape=(None, height * width * channels))\n",
    "tf_input_frame = tf.placeholder(tf.float32, shape=(None, height*frames_captured, width, channels))\n",
    "tf_input_value = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
    "tf_input_learning_rate = tf.placeholder(tf.float32)\n",
    "tf_dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "tf_reward = tf.placeholder(tf.float32)\n",
    "    \n",
    "#with tf.name_scope(\"hidden\"):\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "\n",
    "convs   = [32,64,64]\n",
    "kerns   = [8,4,3]\n",
    "strides = [4,2,2]\n",
    "pads    = 'valid'\n",
    "activ   = tf.nn.elu\n",
    "\n",
    "# Policy Network\n",
    "conv1 = tf.layers.conv2d(\n",
    "        inputs = tf_input_frame,\n",
    "        filters = convs[0],\n",
    "        kernel_size = kerns[0],\n",
    "        strides = strides[0],\n",
    "        padding = pads,\n",
    "        activation = activ,\n",
    "        name='conv1')\n",
    "\n",
    "conv2 = tf.layers.conv2d(\n",
    "        inputs=conv1,\n",
    "        filters = convs[1],\n",
    "        kernel_size = kerns[1],\n",
    "        strides = strides[1],\n",
    "        padding = pads,\n",
    "        activation = activ,\n",
    "        name='conv2')\n",
    "\n",
    "conv3 = tf.layers.conv2d(\n",
    "        inputs=conv2,\n",
    "        filters = convs[2],\n",
    "        kernel_size = kerns[2],\n",
    "        strides = strides[2],\n",
    "        padding = pads,\n",
    "        activation = activ,\n",
    "        name='conv3')\n",
    "\n",
    "flat = tf.layers.flatten(conv3)\n",
    "\n",
    "\n",
    "\n",
    "hidden1 = tf.layers.dense(flat, n_hidden1, activation=tf.nn.elu, name=\"hidden1\", kernel_initializer=initializer)\n",
    "#hidden1_drop = tf.nn.dropout(hidden1, tf_dropout_keep_prob)\n",
    "\n",
    "\n",
    "#with tf.name_scope(\"output\"):\n",
    "logits = tf.layers.dense(hidden1, n_outputs, name=\"output\", activation=None)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(logits-tf_input_value))\n",
    "optimizer = tf.train.AdamOptimizer(tf_input_learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model before training: ./models/space_invaders_few_layers_rl\n",
      "INFO:tensorflow:Restoring parameters from ./models/space_invaders_few_layers_rl\n",
      "Using strict AI actions\n",
      "Death at frame 189\n",
      "Death at frame 272\n",
      "Death at frame 359\n",
      "Epoch: 0, frames: 360, score: 35.0, average loss: 0.07836907350307962\n",
      "actions trained:           [146. 121.  79.  13.   0.   0.]\n",
      "actions out while training:[116. 124. 106.  13.   0.   0.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 242\n",
      "Death at frame 320\n",
      "Death at frame 430\n",
      "Epoch: 1, frames: 431, score: 75.0, average loss: 0.0891012638523662\n",
      "actions trained:           [ 15. 162. 240.  13.   0.   0.]\n",
      "actions out while training:[ 68. 191. 141.  28.   0.   2.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 262\n",
      "Death at frame 671\n",
      "Death at frame 1151\n",
      "Epoch: 2, frames: 1152, score: 185.0, average loss: 0.13627873583845557\n",
      "actions trained:           [340. 156. 200. 353.  18.  84.]\n",
      "actions out while training:[147. 364. 234. 190.  90. 126.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 417\n",
      "Death at frame 622\n",
      "Death at frame 1124\n",
      "Epoch: 3, frames: 1125, score: 335.0, average loss: 0.13655348022407715\n",
      "actions trained:           [ 15. 582. 121.  60.  73. 273.]\n",
      "actions out while training:[ 48. 364. 127. 121. 147. 317.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 463\n",
      "Death at frame 878\n",
      "Death at frame 1069\n",
      "Epoch: 4, frames: 1070, score: 215.0, average loss: 0.14171993999018243\n",
      "actions trained:           [ 58. 141. 278. 169. 157. 266.]\n",
      "actions out while training:[123. 184. 160. 162. 160. 280.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 333\n",
      "Death at frame 471\n",
      "Death at frame 714\n",
      "Epoch: 5, frames: 715, score: 155.0, average loss: 0.13624870372551498\n",
      "actions trained:           [269. 137.  15.  16. 204.  73.]\n",
      "actions out while training:[210. 152.  32.  62. 126. 132.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 207\n",
      "Death at frame 306\n",
      "Death at frame 479\n",
      "Epoch: 6, frames: 480, score: 15.0, average loss: 0.07719255366617747\n",
      "actions trained:           [ 19. 189. 140.  77.  12.  42.]\n",
      "actions out while training:[ 17. 182. 161.  76.   8.  35.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 202\n",
      "Death at frame 296\n",
      "Death at frame 804\n",
      "Epoch: 7, frames: 805, score: 155.0, average loss: 0.13094334766274598\n",
      "actions trained:           [ 69. 129.  44. 307.  37. 218.]\n",
      "actions out while training:[ 68. 199.  88.  98. 103. 248.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 389\n",
      "Death at frame 549\n",
      "Death at frame 923\n",
      "Epoch: 8, frames: 924, score: 350.0, average loss: 0.14142434075328245\n",
      "actions trained:           [ 19. 275. 289.  18.  38. 284.]\n",
      "actions out while training:[ 31. 267. 263.  22.  82. 258.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 551\n",
      "Death at frame 666\n",
      "Death at frame 851\n",
      "Epoch: 9, frames: 852, score: 215.0, average loss: 0.13349404763214945\n",
      "actions trained:           [115. 266. 114.  11. 154. 191.]\n",
      "actions out while training:[116. 225.  99.  29. 165. 217.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 265\n",
      "Death at frame 509\n",
      "Death at frame 642\n",
      "Saving model at epoch 10: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 10, frames: 643, score: 115.0, average loss: 0.12960285808360603\n",
      "actions trained:           [ 79. 206.  28.  10. 154. 165.]\n",
      "actions out while training:[ 53. 162.  41.  30. 234. 122.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 221\n",
      "Death at frame 931\n",
      "Death at frame 1154\n",
      "Epoch: 11, frames: 1155, score: 340.0, average loss: 0.13434092047832769\n",
      "actions trained:           [ 81. 249. 108. 190. 265. 261.]\n",
      "actions out while training:[102. 285. 155. 111. 264. 237.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 563\n",
      "Death at frame 1031\n",
      "Death at frame 1156\n",
      "Epoch: 12, frames: 1157, score: 180.0, average loss: 0.13078100049973126\n",
      "actions trained:           [ 58. 297.  54.  94. 405. 248.]\n",
      "actions out while training:[ 99. 293.  78. 122. 283. 281.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 322\n",
      "Death at frame 648\n",
      "Death at frame 1059\n",
      "Epoch: 13, frames: 1060, score: 185.0, average loss: 0.13824955902574326\n",
      "actions trained:           [258. 185. 233. 117.  83. 183.]\n",
      "actions out while training:[ 97. 272. 175. 165. 152. 198.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 164\n",
      "Death at frame 294\n",
      "Death at frame 513\n",
      "Epoch: 14, frames: 514, score: 155.0, average loss: 0.14177240719210468\n",
      "actions trained:           [ 19. 241.  13.  61. 153.  26.]\n",
      "actions out while training:[ 37. 187.  23.  84. 127.  55.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 174\n",
      "Death at frame 259\n",
      "Death at frame 642\n",
      "Epoch: 15, frames: 643, score: 210.0, average loss: 0.15515615999028204\n",
      "actions trained:           [  8. 183.  75. 103. 116. 157.]\n",
      "actions out while training:[ 15. 168. 100. 124. 122. 113.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 294\n",
      "Death at frame 621\n",
      "Death at frame 817\n",
      "Epoch: 16, frames: 818, score: 215.0, average loss: 0.1368265815768335\n",
      "actions trained:           [110. 119. 176. 230.  51. 131.]\n",
      "actions out while training:[120. 222. 147.  73. 107. 148.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 205\n",
      "Death at frame 289\n",
      "Death at frame 507\n",
      "Epoch: 17, frames: 508, score: 115.0, average loss: 0.13783090992200078\n",
      "actions trained:           [ 23. 217.  67.   2. 100.  98.]\n",
      "actions out while training:[ 27. 151.  65.   5. 176.  83.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 179\n",
      "Death at frame 266\n",
      "Death at frame 906\n",
      "Epoch: 18, frames: 907, score: 270.0, average loss: 0.15023371740076766\n",
      "actions trained:           [ 94. 209.  64. 122. 235. 182.]\n",
      "actions out while training:[ 88. 203.  69. 112. 259. 175.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 213\n",
      "Death at frame 438\n",
      "Death at frame 869\n",
      "Epoch: 19, frames: 870, score: 275.0, average loss: 0.12477887972400144\n",
      "actions trained:           [ 28. 143.  40.  37. 445. 176.]\n",
      "actions out while training:[ 56. 202.  82.  54. 266. 209.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 202\n",
      "Death at frame 606\n",
      "Death at frame 1168\n",
      "Saving model at epoch 20: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 20, frames: 1169, score: 340.0, average loss: 0.1450732845105453\n",
      "actions trained:           [158. 184. 277. 192.  81. 276.]\n",
      "actions out while training:[154. 245. 197. 200. 139. 233.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 514\n",
      "Death at frame 620\n",
      "Death at frame 809\n",
      "Epoch: 21, frames: 810, score: 210.0, average loss: 0.13767471999011963\n",
      "actions trained:           [ 14. 414.  43. 103.  73. 162.]\n",
      "actions out while training:[ 38. 263.  86. 150.  99. 173.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 258\n",
      "Death at frame 605\n",
      "Death at frame 714\n",
      "Epoch: 22, frames: 715, score: 195.0, average loss: 0.1262356902209054\n",
      "actions trained:           [172. 207.  59.  36. 110. 130.]\n",
      "actions out while training:[ 40. 214.  91.  98.  95. 176.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 547\n",
      "Death at frame 942\n",
      "Death at frame 1204\n",
      "Epoch: 23, frames: 1205, score: 305.0, average loss: 0.13412563828065888\n",
      "actions trained:           [ 18. 157. 259. 346. 104. 320.]\n",
      "actions out while training:[ 50. 299. 212. 128. 138. 377.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 215\n",
      "Death at frame 398\n",
      "Death at frame 575\n",
      "Epoch: 24, frames: 576, score: 150.0, average loss: 0.13547506443504978\n",
      "actions trained:           [ 48. 313.  18.   0. 122.  74.]\n",
      "actions out while training:[ 68. 275.  25.   0. 122.  85.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 199\n",
      "Death at frame 443\n",
      "Death at frame 831\n",
      "Epoch: 25, frames: 832, score: 290.0, average loss: 0.14431797640036714\n",
      "actions trained:           [221. 143. 180.  12.  61. 214.]\n",
      "actions out while training:[167. 136. 139.  39.  72. 278.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 279\n",
      "Death at frame 571\n",
      "Death at frame 702\n",
      "Epoch: 26, frames: 703, score: 165.0, average loss: 0.14131243075677555\n",
      "actions trained:           [ 44. 228. 100.  20. 167. 143.]\n",
      "actions out while training:[ 71. 173.  86.  93. 131. 148.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 209\n",
      "Death at frame 306\n",
      "Death at frame 404\n",
      "Epoch: 27, frames: 405, score: 15.0, average loss: 0.08286206485211364\n",
      "actions trained:           [ 50. 132.  77.  64.  42.  39.]\n",
      "actions out while training:[ 42. 133.  77.  66.  50.  36.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 191\n",
      "Death at frame 302\n",
      "Death at frame 651\n",
      "Epoch: 28, frames: 652, score: 105.0, average loss: 0.043753209525667094\n",
      "actions trained:           [150.  95.  10. 250.  39. 107.]\n",
      "actions out while training:[ 86. 102.  13. 259.  45. 146.]\n",
      "\n",
      "Using strict AI actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death at frame 200\n",
      "Death at frame 299\n",
      "Death at frame 406\n",
      "Epoch: 29, frames: 407, score: 85.0, average loss: 0.08484573287093126\n",
      "actions trained:           [ 94.  74.  24. 176.   6.  32.]\n",
      "actions out while training:[ 39.  81.  28. 196.   9.  53.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 262\n",
      "Death at frame 581\n",
      "Death at frame 804\n",
      "Saving model at epoch 30: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 30, frames: 805, score: 190.0, average loss: 0.13216957761492185\n",
      "actions trained:           [ 14. 186.  50. 246.  79. 229.]\n",
      "actions out while training:[ 60. 177. 101. 100. 129. 237.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 707\n",
      "Death at frame 920\n",
      "Death at frame 1005\n",
      "Epoch: 31, frames: 1006, score: 260.0, average loss: 0.13518166088969874\n",
      "actions trained:           [ 32. 201. 161.  14. 148. 449.]\n",
      "actions out while training:[ 99. 198. 109.  65. 144. 390.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 258\n",
      "Death at frame 631\n",
      "Death at frame 727\n",
      "Epoch: 32, frames: 728, score: 135.0, average loss: 0.12371931162321041\n",
      "actions trained:           [109. 193.  25. 124.  74. 202.]\n",
      "actions out while training:[ 96. 306.  61.  55.  58. 151.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 205\n",
      "Death at frame 288\n",
      "Death at frame 382\n",
      "Epoch: 33, frames: 383, score: 35.0, average loss: 0.07313897397169587\n",
      "actions trained:           [ 45. 312.   9.   5.   7.   4.]\n",
      "actions out while training:[ 31. 329.   8.   4.   6.   4.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 200\n",
      "Death at frame 283\n",
      "Death at frame 436\n",
      "Epoch: 34, frames: 437, score: 135.0, average loss: 0.12578963531419907\n",
      "actions trained:           [ 17. 299.  47.  58.   6.   9.]\n",
      "actions out while training:[ 40. 257.  59.  54.  10.  16.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 220\n",
      "Death at frame 360\n",
      "Death at frame 915\n",
      "Epoch: 35, frames: 916, score: 320.0, average loss: 0.13771258197579994\n",
      "actions trained:           [ 75. 316. 131.  37. 122. 234.]\n",
      "actions out while training:[115. 310. 132.  45.  89. 224.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 194\n",
      "Death at frame 341\n",
      "Death at frame 530\n",
      "Epoch: 36, frames: 531, score: 120.0, average loss: 0.1331634227908779\n",
      "actions trained:           [124. 128. 176.  61.   7.  34.]\n",
      "actions out while training:[ 68. 192. 108.  83.   9.  70.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 206\n",
      "Death at frame 310\n",
      "Death at frame 643\n",
      "Epoch: 37, frames: 644, score: 105.0, average loss: 0.03777840455114476\n",
      "actions trained:           [192. 186.   6.  72. 101.  86.]\n",
      "actions out while training:[167. 195.   7.  76. 102.  96.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 215\n",
      "Death at frame 317\n",
      "Death at frame 978\n",
      "Epoch: 38, frames: 979, score: 225.0, average loss: 0.13061149354800292\n",
      "actions trained:           [ 50. 398.  85.  25.  79. 341.]\n",
      "actions out while training:[ 65. 227.  39.  99. 127. 421.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 272\n",
      "Death at frame 596\n",
      "Death at frame 798\n",
      "Epoch: 39, frames: 799, score: 210.0, average loss: 0.1251635701596198\n",
      "actions trained:           [ 67. 101.  44. 228.  80. 278.]\n",
      "actions out while training:[ 75. 175.  73. 119. 126. 230.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 187\n",
      "Death at frame 563\n",
      "Death at frame 903\n",
      "Saving model at epoch 40: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 40, frames: 904, score: 290.0, average loss: 0.1444824608247509\n",
      "actions trained:           [ 97. 186.  61. 153. 169. 237.]\n",
      "actions out while training:[103. 187.  76. 122. 238. 177.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 223\n",
      "Death at frame 307\n",
      "Death at frame 404\n",
      "Epoch: 41, frames: 405, score: 90.0, average loss: 0.1087575768719278\n",
      "actions trained:           [ 29. 165.   6. 117.  67.  20.]\n",
      "actions out while training:[ 37. 214.   6.  37.  82.  28.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 174\n",
      "Death at frame 262\n",
      "Death at frame 369\n",
      "Epoch: 42, frames: 370, score: 135.0, average loss: 0.13501835677314677\n",
      "actions trained:           [159. 154.   5.   3.  48.   0.]\n",
      "actions out while training:[108. 154.   6.  15.  86.   0.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 213\n",
      "Death at frame 361\n",
      "Death at frame 994\n",
      "Epoch: 43, frames: 995, score: 560.0, average loss: 0.13845844558497133\n",
      "actions trained:           [194. 143. 167.  58. 238. 194.]\n",
      "actions out while training:[151. 179. 188. 102. 185. 189.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 218\n",
      "Death at frame 301\n",
      "Death at frame 397\n",
      "Epoch: 44, frames: 398, score: 45.0, average loss: 0.08040186014645116\n",
      "actions trained:           [ 16. 158.   7.  70. 120.  26.]\n",
      "actions out while training:[ 16. 166.  12.  79.  96.  28.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 366\n",
      "Death at frame 470\n",
      "Death at frame 661\n",
      "Epoch: 45, frames: 662, score: 120.0, average loss: 0.12171124601165235\n",
      "actions trained:           [ 33. 300.  50.  23. 163.  92.]\n",
      "actions out while training:[ 48. 247.  61.  38. 113. 154.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 289\n",
      "Death at frame 545\n",
      "Death at frame 671\n",
      "Epoch: 46, frames: 672, score: 120.0, average loss: 0.1343802195393432\n",
      "actions trained:           [187.  96. 100.  98.  38. 152.]\n",
      "actions out while training:[ 83. 104. 192.  73.  78. 141.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 217\n",
      "Death at frame 348\n",
      "Death at frame 997\n",
      "Epoch: 47, frames: 998, score: 325.0, average loss: 0.13320092289941454\n",
      "actions trained:           [ 49. 274. 108.  84. 206. 276.]\n",
      "actions out while training:[ 52. 298. 144. 108. 104. 291.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 137\n",
      "Death at frame 264\n",
      "Death at frame 512\n",
      "Epoch: 48, frames: 513, score: 95.0, average loss: 0.12181345061445736\n",
      "actions trained:           [ 49. 183. 120.  82.  61.  17.]\n",
      "actions out while training:[ 63. 183.  81.  81.  77.  27.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 213\n",
      "Death at frame 295\n",
      "Death at frame 389\n",
      "Epoch: 49, frames: 390, score: 80.0, average loss: 0.08248510765316126\n",
      "actions trained:           [ 33. 208.  43.   1.  47.  57.]\n",
      "actions out while training:[ 54. 208.  28.   1.  54.  44.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 182\n",
      "Death at frame 543\n",
      "Death at frame 694\n",
      "Saving model at epoch 50: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 50, frames: 695, score: 155.0, average loss: 0.13400588937425634\n",
      "actions trained:           [104. 285.  80.  13.  97. 115.]\n",
      "actions out while training:[ 90. 254.  89.  25. 112. 124.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 378\n",
      "Death at frame 538\n",
      "Death at frame 629\n",
      "Epoch: 51, frames: 630, score: 110.0, average loss: 0.15152997582451602\n",
      "actions trained:           [ 30. 150. 153.  17. 124. 155.]\n",
      "actions out while training:[ 32. 138.  78.  18. 208. 155.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 249\n",
      "Death at frame 575\n",
      "Death at frame 965\n",
      "Epoch: 52, frames: 966, score: 325.0, average loss: 0.14083078506287344\n",
      "actions trained:           [109. 147.  23. 105. 438. 143.]\n",
      "actions out while training:[ 91. 154.  57. 177. 325. 161.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 159\n",
      "Death at frame 247\n",
      "Death at frame 645\n",
      "Epoch: 53, frames: 646, score: 200.0, average loss: 0.13640882316567587\n",
      "actions trained:           [109. 203.  45.  79. 129.  80.]\n",
      "actions out while training:[107. 194.  61.  66. 127.  90.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 223\n",
      "Death at frame 958\n",
      "Death at frame 1127\n",
      "Epoch: 54, frames: 1128, score: 265.0, average loss: 0.14190102149571074\n",
      "actions trained:           [142.  84. 141. 329. 212. 219.]\n",
      "actions out while training:[157. 120. 195. 231. 233. 191.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 275\n",
      "Death at frame 876\n",
      "Death at frame 1042\n",
      "Epoch: 55, frames: 1043, score: 225.0, average loss: 0.1321527638144894\n",
      "actions trained:           [165. 326. 109.  83. 211. 148.]\n",
      "actions out while training:[160. 283. 139.  94. 179. 187.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 748\n",
      "Death at frame 909\n",
      "Death at frame 1021\n",
      "Epoch: 56, frames: 1022, score: 260.0, average loss: 0.13654029771686366\n",
      "actions trained:           [127. 227. 150. 102. 115. 300.]\n",
      "actions out while training:[130. 289. 209.  72. 102. 219.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 210\n",
      "Death at frame 655\n",
      "Death at frame 801\n",
      "Epoch: 57, frames: 802, score: 215.0, average loss: 0.13452097654985182\n",
      "actions trained:           [ 40. 581.  48.  35.  29.  68.]\n",
      "actions out while training:[ 88. 390.  76.  91.  76.  80.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 536\n",
      "Death at frame 1089\n",
      "Death at frame 1275\n",
      "Epoch: 58, frames: 1276, score: 220.0, average loss: 0.14264689825390983\n",
      "actions trained:           [132. 109. 361. 364.  99. 210.]\n",
      "actions out while training:[240. 205. 215. 150. 152. 313.]\n",
      "\n",
      "Using strict AI actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death at frame 450\n",
      "Death at frame 774\n",
      "Death at frame 919\n",
      "Epoch: 59, frames: 920, score: 425.0, average loss: 0.14190296341471795\n",
      "actions trained:           [424. 152.  62.   6.  46. 229.]\n",
      "actions out while training:[133. 180. 122.  40. 208. 236.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 206\n",
      "Death at frame 301\n",
      "Death at frame 387\n",
      "Saving model at epoch 60: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 60, frames: 388, score: 35.0, average loss: 0.06766682289744683\n",
      "actions trained:           [  3. 125.   7.  16. 213.  23.]\n",
      "actions out while training:[  2. 102.   6.  14. 248.  15.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 211\n",
      "Death at frame 437\n",
      "Death at frame 934\n",
      "Epoch: 61, frames: 935, score: 310.0, average loss: 0.1356230746120216\n",
      "actions trained:           [ 14. 148. 118.  37. 336. 281.]\n",
      "actions out while training:[ 23. 163. 162.  72. 270. 244.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 561\n",
      "Death at frame 757\n",
      "Death at frame 877\n",
      "Epoch: 62, frames: 878, score: 290.0, average loss: 0.13188670363089328\n",
      "actions trained:           [ 13. 172. 239. 172. 186.  95.]\n",
      "actions out while training:[ 64. 253. 133. 104. 204. 119.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 404\n",
      "Death at frame 953\n",
      "Death at frame 1042\n",
      "Epoch: 63, frames: 1043, score: 195.0, average loss: 0.13915834900288093\n",
      "actions trained:           [126. 261. 236. 107. 162. 150.]\n",
      "actions out while training:[105. 202. 190. 117. 152. 276.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 214\n",
      "Death at frame 303\n",
      "Death at frame 406\n",
      "Epoch: 64, frames: 407, score: 30.0, average loss: 0.06845556196526612\n",
      "actions trained:           [ 71.  99.   6.   6.  26. 198.]\n",
      "actions out while training:[ 63.  96.   6.   6.  11. 224.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 240\n",
      "Death at frame 350\n",
      "Death at frame 532\n",
      "Epoch: 65, frames: 533, score: 75.0, average loss: 0.06834626834314354\n",
      "actions trained:           [ 15. 131.  25.  33. 117. 211.]\n",
      "actions out while training:[ 30. 128.  32.  41. 146. 155.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 208\n",
      "Death at frame 311\n",
      "Death at frame 796\n",
      "Epoch: 66, frames: 797, score: 240.0, average loss: 0.13973015788681148\n",
      "actions trained:           [212.  93.  31.  49. 138. 273.]\n",
      "actions out while training:[151. 104.  78.  71. 203. 189.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 305\n",
      "Death at frame 545\n",
      "Death at frame 662\n",
      "Epoch: 67, frames: 663, score: 155.0, average loss: 0.14086937681503284\n",
      "actions trained:           [ 62. 127.  91.  77. 155. 150.]\n",
      "actions out while training:[ 56. 128. 114.  88. 136. 140.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 368\n",
      "Death at frame 553\n",
      "Death at frame 825\n",
      "Epoch: 68, frames: 826, score: 190.0, average loss: 0.14167124031157924\n",
      "actions trained:           [ 24. 148. 190. 117. 127. 219.]\n",
      "actions out while training:[ 36. 163. 100. 115. 175. 236.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 242\n",
      "Death at frame 328\n",
      "Death at frame 451\n",
      "Epoch: 69, frames: 452, score: 45.0, average loss: 0.08007337979669636\n",
      "actions trained:           [  4. 256.   5.  33. 125.  28.]\n",
      "actions out while training:[  5. 257.   7.  36. 118.  28.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 287\n",
      "Death at frame 578\n",
      "Death at frame 788\n",
      "Saving model at epoch 70: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 70, frames: 789, score: 210.0, average loss: 0.13307258144528164\n",
      "actions trained:           [ 14. 270.  19.  64. 258. 163.]\n",
      "actions out while training:[ 25. 246.  67. 108. 119. 223.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 277\n",
      "Death at frame 500\n",
      "Death at frame 651\n",
      "Epoch: 71, frames: 652, score: 135.0, average loss: 0.11894017460465203\n",
      "actions trained:           [ 71. 160. 150.  78.   9. 183.]\n",
      "actions out while training:[ 94. 167. 107.  64.  30. 189.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 212\n",
      "Death at frame 293\n",
      "Death at frame 390\n",
      "Epoch: 72, frames: 391, score: 15.0, average loss: 0.07775398221056679\n",
      "actions trained:           [151. 140.  11.  24.  33.  31.]\n",
      "actions out while training:[139. 145.  12.  19.  34.  41.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 374\n",
      "Death at frame 513\n",
      "Death at frame 653\n",
      "Epoch: 73, frames: 654, score: 140.0, average loss: 0.14065754227553784\n",
      "actions trained:           [212. 180.  16.  36. 141.  68.]\n",
      "actions out while training:[167. 172.  38.  90. 116.  70.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 229\n",
      "Death at frame 326\n",
      "Death at frame 461\n",
      "Epoch: 74, frames: 462, score: 35.0, average loss: 0.05181514062580964\n",
      "actions trained:           [ 24. 185.  14.  88.  88.  62.]\n",
      "actions out while training:[ 28. 202.  15.  86.  80.  50.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 283\n",
      "Death at frame 808\n",
      "Death at frame 1240\n",
      "Epoch: 75, frames: 1241, score: 260.0, average loss: 0.13599049304392125\n",
      "actions trained:           [ 59. 185. 178. 248. 204. 366.]\n",
      "actions out while training:[153. 180. 146. 167. 161. 433.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 247\n",
      "Death at frame 351\n",
      "Death at frame 642\n",
      "Epoch: 76, frames: 643, score: 105.0, average loss: 0.07245437760363331\n",
      "actions trained:           [ 97. 195.  21.  40. 119. 170.]\n",
      "actions out while training:[120. 136.  26.  50. 129. 181.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 230\n",
      "Death at frame 320\n",
      "Death at frame 615\n",
      "Epoch: 77, frames: 616, score: 105.0, average loss: 0.060085272060598154\n",
      "actions trained:           [225.  97.  77.  33.  26. 157.]\n",
      "actions out while training:[184.  92.  99.  43.  39. 158.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 208\n",
      "Death at frame 293\n",
      "Death at frame 389\n",
      "Epoch: 78, frames: 390, score: 50.0, average loss: 0.0766882271182806\n",
      "actions trained:           [152.  94.  12.  48.  30.  53.]\n",
      "actions out while training:[174.  86.  10.  43.  26.  50.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 226\n",
      "Death at frame 323\n",
      "Death at frame 423\n",
      "Epoch: 79, frames: 424, score: 30.0, average loss: 0.050108188804424414\n",
      "actions trained:           [ 85.  90.  26.  15.  39. 168.]\n",
      "actions out while training:[ 97.  90.  30.  11.  19. 176.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 270\n",
      "Death at frame 565\n",
      "Death at frame 957\n",
      "Saving model at epoch 80: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 80, frames: 958, score: 255.0, average loss: 0.13946606829364883\n",
      "actions trained:           [263.  90.  96. 118.  33. 357.]\n",
      "actions out while training:[ 90.  97. 204. 153. 156. 257.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 216\n",
      "Death at frame 301\n",
      "Death at frame 394\n",
      "Epoch: 81, frames: 395, score: 80.0, average loss: 0.1024339389551826\n",
      "actions trained:           [  3. 188.  39.  32.  72.  60.]\n",
      "actions out while training:[  2. 162.  61.  31. 105.  33.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 218\n",
      "Death at frame 946\n",
      "Death at frame 1024\n",
      "Epoch: 82, frames: 1025, score: 515.0, average loss: 0.12463591667576332\n",
      "actions trained:           [ 66. 227. 147. 177. 238. 169.]\n",
      "actions out while training:[ 70. 235. 171. 121. 234. 193.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 251\n",
      "Death at frame 585\n",
      "Death at frame 961\n",
      "Epoch: 83, frames: 962, score: 385.0, average loss: 0.13548679129150965\n",
      "actions trained:           [ 42. 141. 190.  55. 308. 225.]\n",
      "actions out while training:[ 77. 191. 169. 104. 240. 180.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 241\n",
      "Death at frame 339\n",
      "Death at frame 625\n",
      "Epoch: 84, frames: 626, score: 210.0, average loss: 0.12875128303959854\n",
      "actions trained:           [ 70. 280.  79.  83.  22.  91.]\n",
      "actions out while training:[ 81. 194.  96.  74.  65. 115.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 234\n",
      "Death at frame 333\n",
      "Death at frame 629\n",
      "Epoch: 85, frames: 630, score: 180.0, average loss: 0.12896807481065828\n",
      "actions trained:           [169. 110.  66.  78.  74. 132.]\n",
      "actions out while training:[ 82. 163. 151.  64.  75.  94.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 158\n",
      "Death at frame 246\n",
      "Death at frame 1345\n",
      "Epoch: 86, frames: 1346, score: 575.0, average loss: 0.14529572343220085\n",
      "actions trained:           [112. 305. 216. 106. 331. 275.]\n",
      "actions out while training:[108. 269. 236. 157. 267. 308.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 172\n",
      "Death at frame 259\n",
      "Death at frame 715\n",
      "Epoch: 87, frames: 716, score: 380.0, average loss: 0.13895900522801685\n",
      "actions trained:           [109. 151. 198.  72.  80. 105.]\n",
      "actions out while training:[ 65. 151. 170. 116.  94. 119.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 237\n",
      "Death at frame 333\n",
      "Death at frame 628\n",
      "Epoch: 88, frames: 629, score: 180.0, average loss: 0.13198138122597475\n",
      "actions trained:           [ 32.  80. 211.  80.  75. 150.]\n",
      "actions out while training:[ 60.  95.  66. 133.  78. 196.]\n",
      "\n",
      "Using strict AI actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death at frame 202\n",
      "Death at frame 283\n",
      "Death at frame 382\n",
      "Epoch: 89, frames: 383, score: 55.0, average loss: 0.0669559996030296\n",
      "actions trained:           [ 74. 187.   6.  31.  54.  30.]\n",
      "actions out while training:[ 85. 195.   6.  12.  56.  28.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 231\n",
      "Death at frame 331\n",
      "Death at frame 709\n",
      "Saving model at epoch 90: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 90, frames: 710, score: 180.0, average loss: 0.13046460888232914\n",
      "actions trained:           [ 20. 359.  40.  54.  61. 175.]\n",
      "actions out while training:[ 69. 231.  42.  86.  85. 196.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 230\n",
      "Death at frame 331\n",
      "Death at frame 956\n",
      "Epoch: 91, frames: 957, score: 280.0, average loss: 0.14091903711616657\n",
      "actions trained:           [198. 142.  80.  98. 183. 255.]\n",
      "actions out while training:[ 87. 226. 122.  87. 163. 271.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 648\n",
      "Death at frame 834\n",
      "Death at frame 940\n",
      "Epoch: 92, frames: 941, score: 350.0, average loss: 0.12713778169992962\n",
      "actions trained:           [ 63. 256. 106.  47. 136. 332.]\n",
      "actions out while training:[ 51. 217. 119. 103. 157. 293.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 255\n",
      "Death at frame 395\n",
      "Death at frame 816\n",
      "Epoch: 93, frames: 817, score: 190.0, average loss: 0.138135625260876\n",
      "actions trained:           [ 48. 106. 182. 120. 114. 246.]\n",
      "actions out while training:[ 96. 101. 131. 163. 120. 205.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 173\n",
      "Death at frame 315\n",
      "Death at frame 452\n",
      "Epoch: 94, frames: 453, score: 65.0, average loss: 0.08055247038269694\n",
      "actions trained:           [168. 136.  21.  88.  39.   0.]\n",
      "actions out while training:[ 89. 142.  35. 130.  50.   6.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 276\n",
      "Death at frame 660\n",
      "Death at frame 787\n",
      "Epoch: 95, frames: 788, score: 215.0, average loss: 0.13533289600801432\n",
      "actions trained:           [  6. 179. 108. 183. 128. 183.]\n",
      "actions out while training:[ 14. 192. 118. 129. 146. 188.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 257\n",
      "Death at frame 716\n",
      "Death at frame 817\n",
      "Epoch: 96, frames: 818, score: 285.0, average loss: 0.13558073338335275\n",
      "actions trained:           [ 43. 159.  67. 140. 136. 272.]\n",
      "actions out while training:[ 59. 240.  83.  74. 136. 225.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 139\n",
      "Death at frame 333\n",
      "Death at frame 710\n",
      "Epoch: 97, frames: 711, score: 245.0, average loss: 0.1375980714327541\n",
      "actions trained:           [182. 223.  90.  56. 102.  57.]\n",
      "actions out while training:[196. 175. 119. 105.  45.  70.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 244\n",
      "Death at frame 445\n",
      "Death at frame 682\n",
      "Epoch: 98, frames: 683, score: 125.0, average loss: 0.12382386635715262\n",
      "actions trained:           [ 61. 276. 171.  15.  36. 123.]\n",
      "actions out while training:[116. 186.  73.  70.  62. 175.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 396\n",
      "Death at frame 741\n",
      "Death at frame 945\n",
      "Epoch: 99, frames: 946, score: 260.0, average loss: 0.13041896405474343\n",
      "actions trained:           [190. 111. 103. 204. 151. 186.]\n",
      "actions out while training:[135. 201. 136.  77. 121. 275.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 278\n",
      "Death at frame 541\n",
      "Death at frame 672\n",
      "Saving model at epoch 100: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 100, frames: 673, score: 135.0, average loss: 0.14071621811711923\n",
      "actions trained:           [ 26. 348.  12.  11.  79. 196.]\n",
      "actions out while training:[ 68. 288.  41.  21. 117. 137.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 202\n",
      "Death at frame 284\n",
      "Death at frame 512\n",
      "Epoch: 101, frames: 513, score: 150.0, average loss: 0.11934128882595868\n",
      "actions trained:           [230. 116.  48.  60.  32.  26.]\n",
      "actions out while training:[ 99. 135. 104.  86.  56.  32.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 243\n",
      "Death at frame 345\n",
      "Death at frame 593\n",
      "Epoch: 102, frames: 594, score: 70.0, average loss: 0.12156564311476296\n",
      "actions trained:           [ 17. 136. 156. 109.  29. 146.]\n",
      "actions out while training:[ 27. 190.  93.  57.  92. 134.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 153\n",
      "Death at frame 359\n",
      "Death at frame 635\n",
      "Epoch: 103, frames: 636, score: 175.0, average loss: 0.13105530862682246\n",
      "actions trained:           [162. 229.  33.  15. 144.  52.]\n",
      "actions out while training:[ 82. 205.  48. 110.  95.  95.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 199\n",
      "Death at frame 286\n",
      "Death at frame 386\n",
      "Epoch: 104, frames: 387, score: 80.0, average loss: 0.06764180095189212\n",
      "actions trained:           [ 48. 156.  13.  30.  77.  62.]\n",
      "actions out while training:[ 50. 158.  17.  44.  69.  48.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 263\n",
      "Death at frame 385\n",
      "Death at frame 474\n",
      "Epoch: 105, frames: 475, score: 55.0, average loss: 0.06608620048943475\n",
      "actions trained:           [ 21. 103.  20.  38. 219.  73.]\n",
      "actions out while training:[ 47. 122.  38.  42. 135.  90.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 516\n",
      "Death at frame 774\n",
      "Death at frame 912\n",
      "Epoch: 106, frames: 913, score: 155.0, average loss: 0.13095614764797736\n",
      "actions trained:           [ 27. 189. 225. 228.  18. 225.]\n",
      "actions out while training:[ 78. 289. 166. 114.  30. 235.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 343\n",
      "Death at frame 473\n",
      "Death at frame 613\n",
      "Epoch: 107, frames: 614, score: 180.0, average loss: 0.13487444494983328\n",
      "actions trained:           [ 36. 298. 120.   0.  57. 102.]\n",
      "actions out while training:[ 74. 232. 113.   1.  46. 147.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 196\n",
      "Death at frame 297\n",
      "Death at frame 638\n",
      "Epoch: 108, frames: 639, score: 105.0, average loss: 0.10675143849190641\n",
      "actions trained:           [281. 116.  25.   9.  34. 173.]\n",
      "actions out while training:[161. 113.  41.  26.  80. 217.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 205\n",
      "Death at frame 376\n",
      "Death at frame 510\n",
      "Epoch: 109, frames: 511, score: 105.0, average loss: 0.12185842686336831\n",
      "actions trained:           [ 48. 126.   8.  22. 118. 188.]\n",
      "actions out while training:[ 47. 190.   8.  27. 183.  55.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 805\n",
      "Death at frame 909\n",
      "Death at frame 991\n",
      "Saving model at epoch 110: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 110, frames: 992, score: 335.0, average loss: 0.13028926723513704\n",
      "actions trained:           [ 75. 323. 203. 119. 123. 148.]\n",
      "actions out while training:[ 80. 263. 157. 107. 195. 189.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 251\n",
      "Death at frame 419\n",
      "Death at frame 907\n",
      "Epoch: 111, frames: 908, score: 535.0, average loss: 0.13118620608055512\n",
      "actions trained:           [ 48. 183.  78.  89. 212. 297.]\n",
      "actions out while training:[ 72. 242. 110.  88. 232. 163.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 149\n",
      "Death at frame 336\n",
      "Death at frame 808\n",
      "Epoch: 112, frames: 809, score: 240.0, average loss: 0.13822440640401829\n",
      "actions trained:           [123. 319.  65.  59.  94. 148.]\n",
      "actions out while training:[123. 233. 100. 103.  94. 155.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 294\n",
      "Death at frame 594\n",
      "Death at frame 1167\n",
      "Epoch: 113, frames: 1168, score: 410.0, average loss: 0.14338968112604975\n",
      "actions trained:           [217. 247. 133. 111. 240. 219.]\n",
      "actions out while training:[217. 246. 121. 149. 220. 214.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 753\n",
      "Death at frame 934\n",
      "Death at frame 1149\n",
      "Epoch: 114, frames: 1150, score: 350.0, average loss: 0.13664127979780433\n",
      "actions trained:           [312. 229.  76. 137. 170. 225.]\n",
      "actions out while training:[174. 279. 176. 122. 165. 233.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 386\n",
      "Death at frame 493\n",
      "Death at frame 644\n",
      "Epoch: 115, frames: 645, score: 215.0, average loss: 0.12993890022314933\n",
      "actions trained:           [ 31. 163. 229.  65.  28. 128.]\n",
      "actions out while training:[ 39. 274.  76.  91.  49. 115.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 238\n",
      "Death at frame 474\n",
      "Death at frame 636\n",
      "Epoch: 116, frames: 637, score: 145.0, average loss: 0.13604888719155328\n",
      "actions trained:           [  8. 294.   7.  28. 222.  77.]\n",
      "actions out while training:[ 29. 211.  11. 111. 172. 102.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 202\n",
      "Death at frame 284\n",
      "Death at frame 383\n",
      "Epoch: 117, frames: 384, score: 30.0, average loss: 0.07726487279171619\n",
      "actions trained:           [184. 118.  10.  60.   7.   4.]\n",
      "actions out while training:[239.  91.  10.  38.   2.   3.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 203\n",
      "Death at frame 402\n",
      "Death at frame 793\n",
      "Epoch: 118, frames: 794, score: 185.0, average loss: 0.13109434685063626\n",
      "actions trained:           [248. 123.  12. 103. 152. 155.]\n",
      "actions out while training:[141. 142.  36. 115. 133. 226.]\n",
      "\n",
      "Using strict AI actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death at frame 205\n",
      "Death at frame 485\n",
      "Death at frame 689\n",
      "Epoch: 119, frames: 690, score: 135.0, average loss: 0.12007130949925328\n",
      "actions trained:           [ 92. 258.  74.  53.  54. 158.]\n",
      "actions out while training:[127. 215.  87.  76.  49. 135.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 279\n",
      "Death at frame 587\n",
      "Death at frame 680\n",
      "Saving model at epoch 120: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 120, frames: 681, score: 120.0, average loss: 0.12122473956700495\n",
      "actions trained:           [101. 126. 187.  85.  13. 168.]\n",
      "actions out while training:[ 78. 164.  51.  48.  87. 252.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 218\n",
      "Death at frame 313\n",
      "Death at frame 821\n",
      "Epoch: 121, frames: 822, score: 210.0, average loss: 0.1380062660754828\n",
      "actions trained:           [172. 156.  61.  68. 116. 248.]\n",
      "actions out while training:[ 80. 149.  87. 122. 138. 245.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 230\n",
      "Death at frame 325\n",
      "Death at frame 638\n",
      "Epoch: 122, frames: 639, score: 105.0, average loss: 0.10279909583784404\n",
      "actions trained:           [  1. 182.  53. 101.  55. 246.]\n",
      "actions out while training:[ 10. 167.  81.  71.  76. 233.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 479\n",
      "Death at frame 744\n",
      "Death at frame 860\n",
      "Epoch: 123, frames: 861, score: 200.0, average loss: 0.1321905466531786\n",
      "actions trained:           [ 43. 114.  49.  28. 282. 344.]\n",
      "actions out while training:[ 65. 140.  83.  37. 205. 330.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 234\n",
      "Death at frame 595\n",
      "Death at frame 818\n",
      "Epoch: 124, frames: 819, score: 180.0, average loss: 0.13691863653470684\n",
      "actions trained:           [ 56. 181. 151. 143.  55. 232.]\n",
      "actions out while training:[ 75. 189. 107. 134.  84. 229.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 410\n",
      "Death at frame 623\n",
      "Death at frame 732\n",
      "Epoch: 125, frames: 733, score: 210.0, average loss: 0.1262829633597455\n",
      "actions trained:           [107. 172.  67.  76. 212.  98.]\n",
      "actions out while training:[ 98. 300.  86.  65.  73. 110.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 169\n",
      "Death at frame 293\n",
      "Death at frame 408\n",
      "Epoch: 126, frames: 409, score: 90.0, average loss: 0.12263215343985957\n",
      "actions trained:           [ 19. 286.  68.  32.   1.   2.]\n",
      "actions out while training:[ 46. 217.  53.  78.   5.   9.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 198\n",
      "Death at frame 328\n",
      "Death at frame 651\n",
      "Epoch: 127, frames: 652, score: 105.0, average loss: 0.10086403744443907\n",
      "actions trained:           [134. 115.  27.  35.  17. 323.]\n",
      "actions out while training:[ 43. 116.  45. 129.  50. 268.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 201\n",
      "Death at frame 332\n",
      "Death at frame 631\n",
      "Epoch: 128, frames: 632, score: 110.0, average loss: 0.11609733814732087\n",
      "actions trained:           [ 74. 131.  34. 120.  79. 193.]\n",
      "actions out while training:[142. 132.  81.  46.  88. 142.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 362\n",
      "Death at frame 462\n",
      "Death at frame 647\n",
      "Epoch: 129, frames: 648, score: 185.0, average loss: 0.13515709681133758\n",
      "actions trained:           [209. 107. 230.  17.   4.  80.]\n",
      "actions out while training:[138. 155. 162.  40.  52. 100.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 274\n",
      "Death at frame 581\n",
      "Death at frame 811\n",
      "Saving model at epoch 130: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 130, frames: 812, score: 185.0, average loss: 0.1290639863984099\n",
      "actions trained:           [ 18. 216.  56. 192.  88. 241.]\n",
      "actions out while training:[ 80. 142.  98.  73. 128. 290.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 211\n",
      "Death at frame 301\n",
      "Death at frame 402\n",
      "Epoch: 131, frames: 403, score: 60.0, average loss: 0.06225085368512736\n",
      "actions trained:           [139.  92.  14.   1.  43. 113.]\n",
      "actions out while training:[143.  97.  12.   1.  44. 105.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 231\n",
      "Death at frame 329\n",
      "Death at frame 671\n",
      "Epoch: 132, frames: 672, score: 105.0, average loss: 0.08924416303405341\n",
      "actions trained:           [ 43. 166. 119.  84.  55. 204.]\n",
      "actions out while training:[ 53. 168. 123.  78.  67. 182.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 225\n",
      "Death at frame 312\n",
      "Death at frame 451\n",
      "Epoch: 133, frames: 452, score: 30.0, average loss: 0.06801397572100279\n",
      "actions trained:           [ 53. 158.  11.  49.  63. 117.]\n",
      "actions out while training:[ 45. 170.  15.  61.  54. 106.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 226\n",
      "Death at frame 597\n",
      "Death at frame 738\n",
      "Epoch: 134, frames: 739, score: 160.0, average loss: 0.1389648252854504\n",
      "actions trained:           [119. 280.  26.  23. 120. 170.]\n",
      "actions out while training:[ 96. 239.  43.  68. 124. 168.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 544\n",
      "Death at frame 787\n",
      "Death at frame 1002\n",
      "Epoch: 135, frames: 1003, score: 260.0, average loss: 0.12455890900127548\n",
      "actions trained:           [ 80. 185.  73. 173. 185. 306.]\n",
      "actions out while training:[109. 270. 111.  71.  85. 356.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 253\n",
      "Death at frame 749\n",
      "Death at frame 1030\n",
      "Epoch: 136, frames: 1031, score: 390.0, average loss: 0.13023455226713426\n",
      "actions trained:           [136. 223. 249. 110.  10. 302.]\n",
      "actions out while training:[142. 305. 119. 100.  52. 312.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 225\n",
      "Death at frame 321\n",
      "Death at frame 648\n",
      "Epoch: 137, frames: 649, score: 105.0, average loss: 0.09380866918356318\n",
      "actions trained:           [ 55. 373.  26.   2.  30. 162.]\n",
      "actions out while training:[ 36. 208.  50.   7.  71. 276.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 226\n",
      "Death at frame 903\n",
      "Death at frame 982\n",
      "Epoch: 138, frames: 983, score: 260.0, average loss: 0.14139661046020824\n",
      "actions trained:           [ 59. 287.  44.  60. 207. 325.]\n",
      "actions out while training:[107. 278.  71. 123. 152. 251.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 234\n",
      "Death at frame 333\n",
      "Death at frame 1116\n",
      "Epoch: 139, frames: 1117, score: 320.0, average loss: 0.13050280589170746\n",
      "actions trained:           [261. 170. 154. 157.  64. 310.]\n",
      "actions out while training:[ 92. 283. 103. 107. 128. 403.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 164\n",
      "Death at frame 264\n",
      "Death at frame 415\n",
      "Saving model at epoch 140: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 140, frames: 416, score: 105.0, average loss: 0.11610224362482471\n",
      "actions trained:           [ 24. 201.  13.  48.  91.  38.]\n",
      "actions out while training:[ 24. 206.  14.  42.  91.  38.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 215\n",
      "Death at frame 301\n",
      "Death at frame 401\n",
      "Epoch: 141, frames: 402, score: 90.0, average loss: 0.11474167622733007\n",
      "actions trained:           [  8. 275.  13.  11.  52.  42.]\n",
      "actions out while training:[  8. 203.  19.  20.  73.  78.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 178\n",
      "Death at frame 335\n",
      "Death at frame 441\n",
      "Epoch: 142, frames: 442, score: 60.0, average loss: 0.08598261200735417\n",
      "actions trained:           [ 19. 206.  17.  13. 140.  46.]\n",
      "actions out while training:[ 34. 211.  24.  18. 113.  41.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 236\n",
      "Death at frame 336\n",
      "Death at frame 623\n",
      "Epoch: 143, frames: 624, score: 240.0, average loss: 0.1344433062085271\n",
      "actions trained:           [ 56. 149.  81.  67. 114. 156.]\n",
      "actions out while training:[121. 198.  81.  58.  81.  84.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 192\n",
      "Death at frame 452\n",
      "Death at frame 759\n",
      "Epoch: 144, frames: 760, score: 250.0, average loss: 0.12982622251555162\n",
      "actions trained:           [103. 217. 196.  66.  73. 104.]\n",
      "actions out while training:[123. 246. 133.  64.  84. 109.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 794\n",
      "Death at frame 1313\n",
      "Death at frame 1406\n",
      "Epoch: 145, frames: 1407, score: 395.0, average loss: 0.13699788745558467\n",
      "actions trained:           [199. 463.  65. 190. 271. 218.]\n",
      "actions out while training:[168. 486. 153. 203. 179. 217.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 211\n",
      "Death at frame 313\n",
      "Death at frame 900\n",
      "Epoch: 146, frames: 901, score: 215.0, average loss: 0.13758920656682952\n",
      "actions trained:           [184. 407.  72.  95.  28. 114.]\n",
      "actions out while training:[129. 292.  84.  79.  56. 260.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 180\n",
      "Death at frame 312\n",
      "Death at frame 586\n",
      "Epoch: 147, frames: 587, score: 115.0, average loss: 0.12986356931174647\n",
      "actions trained:           [ 27. 161. 134.  27.  42. 195.]\n",
      "actions out while training:[ 38. 174. 145.  60.  66. 103.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 826\n",
      "Death at frame 932\n",
      "Death at frame 1457\n",
      "Epoch: 148, frames: 1458, score: 465.0, average loss: 0.1391399987010633\n",
      "actions trained:           [229. 319. 248.  95. 294. 272.]\n",
      "actions out while training:[178. 480. 238.  88. 243. 230.]\n",
      "\n",
      "Using strict AI actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death at frame 170\n",
      "Death at frame 317\n",
      "Death at frame 432\n",
      "Epoch: 149, frames: 433, score: 90.0, average loss: 0.12180801598001985\n",
      "actions trained:           [ 13. 265.  39.  95.  20.   0.]\n",
      "actions out while training:[ 25. 215.  41.  66.  85.   0.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 207\n",
      "Death at frame 352\n",
      "Death at frame 583\n",
      "Saving model at epoch 150: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 150, frames: 584, score: 200.0, average loss: 0.13067240698197105\n",
      "actions trained:           [236. 174.  30.  28.  61.  54.]\n",
      "actions out while training:[113. 223. 108.  32.  51.  56.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 199\n",
      "Death at frame 349\n",
      "Death at frame 511\n",
      "Epoch: 151, frames: 512, score: 130.0, average loss: 0.12022707091980604\n",
      "actions trained:           [  1. 341. 143.  13.  10.   3.]\n",
      "actions out while training:[  1. 320.  81.  39.  65.   5.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 233\n",
      "Death at frame 333\n",
      "Death at frame 644\n",
      "Epoch: 152, frames: 645, score: 155.0, average loss: 0.12362534714417932\n",
      "actions trained:           [ 28. 342.  25.  58.  92.  99.]\n",
      "actions out while training:[ 49. 251.  56. 105.  50. 133.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 226\n",
      "Death at frame 312\n",
      "Death at frame 987\n",
      "Epoch: 153, frames: 988, score: 460.0, average loss: 0.13325800805051863\n",
      "actions trained:           [ 71. 310.  86. 136. 112. 272.]\n",
      "actions out while training:[100. 313. 175. 103. 124. 172.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 167\n",
      "Death at frame 251\n",
      "Death at frame 467\n",
      "Epoch: 154, frames: 468, score: 155.0, average loss: 0.1419536593646671\n",
      "actions trained:           [ 96. 228. 112.  27.   4.   0.]\n",
      "actions out while training:[ 93. 232.  60.  40.  22.  20.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 274\n",
      "Death at frame 445\n",
      "Death at frame 836\n",
      "Epoch: 155, frames: 837, score: 310.0, average loss: 0.12860726977380735\n",
      "actions trained:           [ 69. 245.  30.  40. 304. 148.]\n",
      "actions out while training:[ 86. 227.  78.  80. 197. 168.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 244\n",
      "Death at frame 782\n",
      "Death at frame 990\n",
      "Epoch: 156, frames: 991, score: 265.0, average loss: 0.13731004904205046\n",
      "actions trained:           [ 65. 146. 240. 300.   8. 231.]\n",
      "actions out while training:[203. 250. 131.  90.  40. 276.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 202\n",
      "Death at frame 293\n",
      "Death at frame 400\n",
      "Epoch: 157, frames: 401, score: 55.0, average loss: 0.06155991863385751\n",
      "actions trained:           [236. 135.   0.   1.  20.   8.]\n",
      "actions out while training:[192. 175.   1.   0.  23.   9.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 248\n",
      "Death at frame 349\n",
      "Death at frame 518\n",
      "Epoch: 158, frames: 519, score: 55.0, average loss: 0.06642474955144351\n",
      "actions trained:           [ 20. 324.   2.   3.  99.  70.]\n",
      "actions out while training:[ 36. 329.   6.   4.  76.  67.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 206\n",
      "Death at frame 922\n",
      "Death at frame 1220\n",
      "Epoch: 159, frames: 1221, score: 515.0, average loss: 0.13899423114171827\n",
      "actions trained:           [213. 557.  67.  48. 117. 218.]\n",
      "actions out while training:[118. 316. 149. 118. 122. 397.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 207\n",
      "Death at frame 295\n",
      "Death at frame 394\n",
      "Saving model at epoch 160: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 160, frames: 395, score: 60.0, average loss: 0.06245116877701205\n",
      "actions trained:           [ 50. 101.  50.   6.  26. 161.]\n",
      "actions out while training:[ 50. 107.  60.   5.  29. 143.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 256\n",
      "Death at frame 625\n",
      "Death at frame 851\n",
      "Epoch: 161, frames: 852, score: 180.0, average loss: 0.13569222703472866\n",
      "actions trained:           [ 14. 189. 112. 192. 123. 221.]\n",
      "actions out while training:[ 96. 174. 160.  85. 108. 228.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 424\n",
      "Death at frame 771\n",
      "Death at frame 978\n",
      "Epoch: 162, frames: 979, score: 210.0, average loss: 0.1254828483258293\n",
      "actions trained:           [342. 106. 137.   1.  66. 326.]\n",
      "actions out while training:[102. 150. 164.  16. 210. 336.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 795\n",
      "Death at frame 830\n",
      "Epoch: 163, frames: 831, score: 200.0, average loss: 0.1397075656906376\n",
      "actions trained:           [  3. 295. 103.  37. 151. 241.]\n",
      "actions out while training:[  2. 318. 155.  56. 142. 157.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 234\n",
      "Death at frame 329\n",
      "Death at frame 642\n",
      "Epoch: 164, frames: 643, score: 220.0, average loss: 0.13183418911558115\n",
      "actions trained:           [  1. 351. 142.  23.  60.  65.]\n",
      "actions out while training:[  1. 264. 123.  75.  89.  90.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 260\n",
      "Death at frame 483\n",
      "Death at frame 643\n",
      "Epoch: 165, frames: 644, score: 185.0, average loss: 0.12826907808061738\n",
      "actions trained:           [  6. 171.  76. 154. 161.  75.]\n",
      "actions out while training:[ 42. 262.  97.  65.  82.  95.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 204\n",
      "Death at frame 335\n",
      "Death at frame 968\n",
      "Epoch: 166, frames: 969, score: 470.0, average loss: 0.13542052743521113\n",
      "actions trained:           [199. 331. 128.  56.  41. 213.]\n",
      "actions out while training:[190. 296.  86. 100.  85. 211.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 555\n",
      "Death at frame 646\n",
      "Death at frame 829\n",
      "Epoch: 167, frames: 830, score: 440.0, average loss: 0.14008262576701705\n",
      "actions trained:           [ 59. 258.   5.  29. 218. 260.]\n",
      "actions out while training:[ 54. 246.  48.  44. 220. 217.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 192\n",
      "Death at frame 327\n",
      "Death at frame 505\n",
      "Epoch: 168, frames: 506, score: 150.0, average loss: 0.12445765304852888\n",
      "actions trained:           [ 15. 181. 100. 150.  25.  34.]\n",
      "actions out while training:[ 34. 220.  81.  55.  79.  36.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 288\n",
      "Death at frame 725\n",
      "Death at frame 867\n",
      "Epoch: 169, frames: 868, score: 215.0, average loss: 0.13354704074998427\n",
      "actions trained:           [165. 218. 168.  98.  46. 172.]\n",
      "actions out while training:[ 86. 204. 188.  90. 100. 199.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 163\n",
      "Death at frame 265\n",
      "Death at frame 659\n",
      "Saving model at epoch 170: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 170, frames: 660, score: 240.0, average loss: 0.14951167565345694\n",
      "actions trained:           [ 18. 192.  19.   1. 222. 207.]\n",
      "actions out while training:[ 34. 184.  31.  23. 246. 141.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 137\n",
      "Death at frame 253\n",
      "Death at frame 501\n",
      "Epoch: 171, frames: 502, score: 120.0, average loss: 0.12207442556182373\n",
      "actions trained:           [ 25. 177.  21.  54. 181.  43.]\n",
      "actions out while training:[ 61. 166.  42.  47. 109.  76.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 377\n",
      "Death at frame 599\n",
      "Death at frame 757\n",
      "Epoch: 172, frames: 758, score: 245.0, average loss: 0.12576819716042945\n",
      "actions trained:           [160. 111. 259.  18.  63. 146.]\n",
      "actions out while training:[ 78. 183. 137.  58. 102. 199.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 391\n",
      "Death at frame 570\n",
      "Death at frame 674\n",
      "Epoch: 173, frames: 675, score: 75.0, average loss: 0.04974558752185372\n",
      "actions trained:           [  6. 280.  20.  16.  63. 289.]\n",
      "actions out while training:[  3. 224.  15.  18.  43. 371.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 215\n",
      "Death at frame 320\n",
      "Death at frame 793\n",
      "Epoch: 174, frames: 794, score: 210.0, average loss: 0.1291170019293581\n",
      "actions trained:           [103. 148.  69. 119.  63. 291.]\n",
      "actions out while training:[ 64. 221. 131. 117.  78. 182.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 212\n",
      "Death at frame 333\n",
      "Death at frame 1141\n",
      "Epoch: 175, frames: 1142, score: 375.0, average loss: 0.128898774799031\n",
      "actions trained:           [ 24. 425. 207. 171. 127. 187.]\n",
      "actions out while training:[ 39. 402. 141. 121. 177. 261.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 559\n",
      "Death at frame 903\n",
      "Death at frame 1238\n",
      "Epoch: 176, frames: 1239, score: 355.0, average loss: 0.14336276869052314\n",
      "actions trained:           [ 46. 534.  74.  82. 248. 254.]\n",
      "actions out while training:[ 88. 323.  77. 169. 186. 395.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 237\n",
      "Death at frame 343\n",
      "Death at frame 811\n",
      "Epoch: 177, frames: 812, score: 190.0, average loss: 0.13356953812649733\n",
      "actions trained:           [222. 117.  47. 116.  77. 232.]\n",
      "actions out while training:[ 48. 125.  84. 146. 120. 288.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 225\n",
      "Death at frame 318\n",
      "Death at frame 815\n",
      "Epoch: 178, frames: 816, score: 210.0, average loss: 0.13025610365916027\n",
      "actions trained:           [  1. 160. 137.  58.  12. 447.]\n",
      "actions out while training:[ 10. 171. 232.  77.  65. 260.]\n",
      "\n",
      "Using strict AI actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death at frame 222\n",
      "Death at frame 311\n",
      "Death at frame 696\n",
      "Epoch: 179, frames: 697, score: 160.0, average loss: 0.13353295625399428\n",
      "actions trained:           [ 57. 173. 197. 148.  49.  72.]\n",
      "actions out while training:[ 82. 176. 140.  80. 120.  98.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 256\n",
      "Death at frame 421\n",
      "Death at frame 567\n",
      "Saving model at epoch 180: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 180, frames: 568, score: 130.0, average loss: 0.11845186220360905\n",
      "actions trained:           [ 85. 107.  43.  12. 191. 129.]\n",
      "actions out while training:[ 84. 115.  60.  11. 224.  73.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 412\n",
      "Death at frame 639\n",
      "Death at frame 904\n",
      "Epoch: 181, frames: 905, score: 480.0, average loss: 0.13082996922939147\n",
      "actions trained:           [108. 205.  32.  93. 311. 155.]\n",
      "actions out while training:[141. 268.  62. 103. 180. 150.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 201\n",
      "Death at frame 279\n",
      "Death at frame 381\n",
      "Epoch: 182, frames: 382, score: 105.0, average loss: 0.11177972469539951\n",
      "actions trained:           [ 52. 226.  50.  53.   0.   0.]\n",
      "actions out while training:[ 51. 215.  53.  59.   2.   1.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 160\n",
      "Death at frame 395\n",
      "Death at frame 651\n",
      "Epoch: 183, frames: 652, score: 165.0, average loss: 0.14916922058382037\n",
      "actions trained:           [ 79. 269. 139.  25.  77.  62.]\n",
      "actions out while training:[120. 246.  55.  63.  94.  73.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 223\n",
      "Death at frame 322\n",
      "Death at frame 815\n",
      "Epoch: 184, frames: 816, score: 410.0, average loss: 0.13162044023427436\n",
      "actions trained:           [ 52. 214.  96. 278.  67. 108.]\n",
      "actions out while training:[ 86. 281. 122.  91.  90. 145.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 206\n",
      "Death at frame 295\n",
      "Death at frame 511\n",
      "Epoch: 185, frames: 512, score: 205.0, average loss: 0.1340088223205859\n",
      "actions trained:           [190. 174.  16.   0.  93.  38.]\n",
      "actions out while training:[ 85. 211.  52.   1. 107.  55.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 571\n",
      "Death at frame 850\n",
      "Death at frame 1018\n",
      "Epoch: 186, frames: 1019, score: 530.0, average loss: 0.13835994931908221\n",
      "actions trained:           [ 12. 313. 110. 127. 214. 242.]\n",
      "actions out while training:[ 47. 413. 106.  87. 156. 209.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 404\n",
      "Death at frame 545\n",
      "Death at frame 703\n",
      "Epoch: 187, frames: 704, score: 235.0, average loss: 0.13404273934576041\n",
      "actions trained:           [ 76. 238. 164.  17.   0. 208.]\n",
      "actions out while training:[ 80. 262. 136.  31.   6. 188.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 302\n",
      "Death at frame 762\n",
      "Death at frame 1360\n",
      "Epoch: 188, frames: 1361, score: 585.0, average loss: 0.13381006097132905\n",
      "actions trained:           [380. 323.  93. 156.  99. 309.]\n",
      "actions out while training:[132. 301. 199. 116. 224. 388.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 129\n",
      "Death at frame 265\n",
      "Death at frame 362\n",
      "Epoch: 189, frames: 363, score: 135.0, average loss: 0.12130334683991995\n",
      "actions trained:           [  1. 216.  77.   5.  43.  20.]\n",
      "actions out while training:[  0. 201.  60.  14.  64.  23.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 205\n",
      "Death at frame 572\n",
      "Death at frame 874\n",
      "Saving model at epoch 190: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 190, frames: 875, score: 320.0, average loss: 0.13804885235635775\n",
      "actions trained:           [  2. 215.  71.  66. 247. 273.]\n",
      "actions out while training:[  4. 291. 156. 125. 119. 179.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 383\n",
      "Death at frame 753\n",
      "Death at frame 1313\n",
      "Epoch: 191, frames: 1314, score: 365.0, average loss: 0.13297284926811104\n",
      "actions trained:           [ 24. 460. 251. 332.  66. 180.]\n",
      "actions out while training:[149. 401. 159. 139. 193. 272.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 206\n",
      "Death at frame 310\n",
      "Death at frame 431\n",
      "Epoch: 192, frames: 432, score: 105.0, average loss: 0.07913600605252105\n",
      "actions trained:           [ 91. 177.   0.   0.  44. 119.]\n",
      "actions out while training:[ 70. 199.   0.   0.  39. 123.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 209\n",
      "Death at frame 314\n",
      "Death at frame 800\n",
      "Epoch: 193, frames: 801, score: 165.0, average loss: 0.12069537291999415\n",
      "actions trained:           [  5. 363.  27.   5.  66. 334.]\n",
      "actions out while training:[ 91. 220.  88.  50. 136. 215.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 203\n",
      "Death at frame 657\n",
      "Death at frame 967\n",
      "Epoch: 194, frames: 968, score: 275.0, average loss: 0.1353174341227033\n",
      "actions trained:           [116. 327.  98.  38. 129. 259.]\n",
      "actions out while training:[ 71. 293. 168. 114.  88. 233.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 244\n",
      "Death at frame 331\n",
      "Death at frame 702\n",
      "Epoch: 195, frames: 703, score: 180.0, average loss: 0.12036501890561244\n",
      "actions trained:           [ 54. 176. 175. 116.  35. 146.]\n",
      "actions out while training:[ 57. 223. 150.  80.  62. 130.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 250\n",
      "Death at frame 559\n",
      "Death at frame 873\n",
      "Epoch: 196, frames: 874, score: 340.0, average loss: 0.1317798539714334\n",
      "actions trained:           [ 55. 277. 184.  81. 165. 111.]\n",
      "actions out while training:[100. 366. 140.  82.  84. 101.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 305\n",
      "Death at frame 911\n",
      "Death at frame 1025\n",
      "Epoch: 197, frames: 1026, score: 495.0, average loss: 0.1290070628786689\n",
      "actions trained:           [282. 299.  73. 141.  42. 188.]\n",
      "actions out while training:[ 62. 258. 150. 138. 126. 291.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 267\n",
      "Death at frame 587\n",
      "Death at frame 800\n",
      "Epoch: 198, frames: 801, score: 155.0, average loss: 0.13521158975553968\n",
      "actions trained:           [  2. 263. 118.  46.  61. 310.]\n",
      "actions out while training:[  9. 260.  87. 110. 146. 188.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 387\n",
      "Death at frame 496\n",
      "Death at frame 1129\n",
      "Epoch: 199, frames: 1130, score: 365.0, average loss: 0.12629651460803762\n",
      "actions trained:           [ 66. 194. 246. 242. 185. 196.]\n",
      "actions out while training:[ 91. 341. 170. 133. 106. 288.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 353\n",
      "Death at frame 877\n",
      "Death at frame 988\n",
      "Saving model at epoch 200: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 200, frames: 989, score: 305.0, average loss: 0.14037428348573036\n",
      "actions trained:           [139. 411.  57.   5.  96. 280.]\n",
      "actions out while training:[172. 304.  99.  42.  90. 281.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 534\n",
      "Death at frame 641\n",
      "Death at frame 1020\n",
      "Epoch: 201, frames: 1021, score: 385.0, average loss: 0.12992780985923955\n",
      "actions trained:           [102. 268. 110.  95. 128. 317.]\n",
      "actions out while training:[ 90. 258. 183. 119.  87. 283.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 613\n",
      "Death at frame 806\n",
      "Death at frame 1008\n",
      "Epoch: 202, frames: 1009, score: 345.0, average loss: 0.13730356548680922\n",
      "actions trained:           [111. 322. 269.  94.  32. 180.]\n",
      "actions out while training:[129. 294. 217. 114.  70. 184.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 246\n",
      "Death at frame 736\n",
      "Death at frame 931\n",
      "Epoch: 203, frames: 932, score: 210.0, average loss: 0.13405722491333724\n",
      "actions trained:           [ 53. 259. 173. 114.  55. 277.]\n",
      "actions out while training:[ 55. 234. 178. 141.  71. 252.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 531\n",
      "Death at frame 774\n",
      "Death at frame 976\n",
      "Epoch: 204, frames: 977, score: 260.0, average loss: 0.12562478886427011\n",
      "actions trained:           [ 83. 144.  76. 277. 193. 203.]\n",
      "actions out while training:[178. 222. 150.  90. 111. 225.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 235\n",
      "Death at frame 479\n",
      "Death at frame 642\n",
      "Epoch: 205, frames: 643, score: 180.0, average loss: 0.12614243779150594\n",
      "actions trained:           [ 80. 408.  33.   1.  22.  98.]\n",
      "actions out while training:[133. 258.  65.   3.  68. 115.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 205\n",
      "Death at frame 772\n",
      "Death at frame 972\n",
      "Epoch: 206, frames: 973, score: 260.0, average loss: 0.13351515020780605\n",
      "actions trained:           [323. 141. 179.  28.  53. 248.]\n",
      "actions out while training:[124. 213. 161.  54. 143. 277.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 207\n",
      "Death at frame 288\n",
      "Death at frame 660\n",
      "Epoch: 207, frames: 661, score: 210.0, average loss: 0.12822190888229093\n",
      "actions trained:           [ 55. 221.  33.  41. 107. 203.]\n",
      "actions out while training:[ 52. 188.  57. 100. 102. 161.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 209\n",
      "Death at frame 372\n",
      "Death at frame 857\n",
      "Epoch: 208, frames: 858, score: 270.0, average loss: 0.14584390082304294\n",
      "actions trained:           [ 21. 309. 119. 137.  73. 198.]\n",
      "actions out while training:[ 38. 236.  93. 111. 122. 257.]\n",
      "\n",
      "Using strict AI actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death at frame 255\n",
      "Death at frame 444\n",
      "Death at frame 645\n",
      "Epoch: 209, frames: 646, score: 230.0, average loss: 0.12771121222332765\n",
      "actions trained:           [ 50.  98.  28.  39. 238. 192.]\n",
      "actions out while training:[ 65. 132.  83.  95. 141. 129.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 195\n",
      "Death at frame 277\n",
      "Death at frame 366\n",
      "Saving model at epoch 210: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 210, frames: 367, score: 90.0, average loss: 0.1257702681756004\n",
      "actions trained:           [  6. 167.  56. 136.   0.   1.]\n",
      "actions out while training:[ 13. 172.  74.  96.   0.  11.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 214\n",
      "Death at frame 878\n",
      "Death at frame 1099\n",
      "Epoch: 211, frames: 1100, score: 595.0, average loss: 0.13227425579807126\n",
      "actions trained:           [123. 290. 158.  54. 132. 342.]\n",
      "actions out while training:[106. 344. 105.  72. 164. 308.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 413\n",
      "Death at frame 775\n",
      "Death at frame 928\n",
      "Epoch: 212, frames: 929, score: 260.0, average loss: 0.1251633803568062\n",
      "actions trained:           [ 46. 200. 335.  67.   3. 277.]\n",
      "actions out while training:[ 97. 266. 157. 120.  31. 257.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 215\n",
      "Death at frame 302\n",
      "Death at frame 677\n",
      "Epoch: 213, frames: 678, score: 180.0, average loss: 0.14145693768929124\n",
      "actions trained:           [237. 240.  10.  41.  76.  73.]\n",
      "actions out while training:[152. 208.  18. 104.  58. 137.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 214\n",
      "Death at frame 316\n",
      "Death at frame 653\n",
      "Epoch: 214, frames: 654, score: 105.0, average loss: 0.10207788700700557\n",
      "actions trained:           [  7. 195.  68.  63.  96. 224.]\n",
      "actions out while training:[ 17. 178.  79.  71. 129. 179.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 200\n",
      "Death at frame 571\n",
      "Death at frame 791\n",
      "Epoch: 215, frames: 792, score: 210.0, average loss: 0.138948829865147\n",
      "actions trained:           [ 76. 122.  72. 214. 138. 169.]\n",
      "actions out while training:[114. 131.  94. 124.  94. 234.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 254\n",
      "Death at frame 567\n",
      "Death at frame 969\n",
      "Epoch: 216, frames: 970, score: 285.0, average loss: 0.13547612629469413\n",
      "actions trained:           [434. 104.  65.  26.  88. 252.]\n",
      "actions out while training:[ 89. 210. 130.  53. 227. 260.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 392\n",
      "Death at frame 594\n",
      "Death at frame 690\n",
      "Epoch: 217, frames: 691, score: 140.0, average loss: 0.1271269010959301\n",
      "actions trained:           [  1. 465.   8.   9.  85. 122.]\n",
      "actions out while training:[  0. 210.  30.  58. 215. 177.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 252\n",
      "Death at frame 581\n",
      "Death at frame 704\n",
      "Epoch: 218, frames: 705, score: 120.0, average loss: 0.12431531844759397\n",
      "actions trained:           [  1. 123.  40. 151. 166. 223.]\n",
      "actions out while training:[  2. 128. 140. 126.  75. 233.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 224\n",
      "Death at frame 320\n",
      "Death at frame 814\n",
      "Epoch: 219, frames: 815, score: 210.0, average loss: 0.12673288211046427\n",
      "actions trained:           [  2. 104. 243.  82.  26. 357.]\n",
      "actions out while training:[ 89. 119. 118. 145.  89. 254.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 220\n",
      "Death at frame 588\n",
      "Death at frame 963\n",
      "Saving model at epoch 220: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 220, frames: 964, score: 225.0, average loss: 0.13450362991833745\n",
      "actions trained:           [ 73. 199.  72. 279. 200. 140.]\n",
      "actions out while training:[ 95. 228. 114. 136. 230. 160.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 405\n",
      "Death at frame 573\n",
      "Death at frame 807\n",
      "Epoch: 221, frames: 808, score: 220.0, average loss: 0.12323782319378654\n",
      "actions trained:           [162. 267.  65.   4. 135. 174.]\n",
      "actions out while training:[100. 259. 130.  13.  72. 233.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 265\n",
      "Death at frame 593\n",
      "Death at frame 824\n",
      "Epoch: 222, frames: 825, score: 210.0, average loss: 0.1259578594152849\n",
      "actions trained:           [ 19. 225. 137. 137.  45. 261.]\n",
      "actions out while training:[ 97. 234. 120.  48. 107. 218.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 204\n",
      "Death at frame 301\n",
      "Death at frame 690\n",
      "Epoch: 223, frames: 691, score: 105.0, average loss: 0.05443862897820686\n",
      "actions trained:           [339. 113.  16.   9.  62. 151.]\n",
      "actions out while training:[339. 128.  24.  13.  54. 132.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 212\n",
      "Death at frame 425\n",
      "Death at frame 807\n",
      "Epoch: 224, frames: 808, score: 180.0, average loss: 0.1310036823139178\n",
      "actions trained:           [313. 233.   7.   5.  53. 196.]\n",
      "actions out while training:[ 76. 191.  79. 120.  95. 246.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 202\n",
      "Death at frame 285\n",
      "Death at frame 382\n",
      "Epoch: 225, frames: 383, score: 105.0, average loss: 0.12619724921563868\n",
      "actions trained:           [  1. 121.  63. 171.  20.   6.]\n",
      "actions out while training:[  1. 193.  54.  52.  68.  14.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 195\n",
      "Death at frame 321\n",
      "Death at frame 467\n",
      "Epoch: 226, frames: 468, score: 150.0, average loss: 0.1249460841068213\n",
      "actions trained:           [  2. 284.  82.   1.  60.  38.]\n",
      "actions out while training:[  3. 257.  88.  11.  59.  49.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 197\n",
      "Death at frame 286\n",
      "Death at frame 524\n",
      "Epoch: 227, frames: 525, score: 150.0, average loss: 0.12735186980498417\n",
      "actions trained:           [ 23. 211. 115.  47.  44.  84.]\n",
      "actions out while training:[ 30. 194. 113.  68.  62.  57.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 854\n",
      "Death at frame 1010\n",
      "Death at frame 1381\n",
      "Epoch: 228, frames: 1382, score: 490.0, average loss: 0.13896260994215462\n",
      "actions trained:           [ 70. 286. 130. 288. 251. 356.]\n",
      "actions out while training:[186. 296. 182. 160. 223. 334.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 407\n",
      "Death at frame 698\n",
      "Death at frame 1073\n",
      "Epoch: 229, frames: 1074, score: 370.0, average loss: 0.14427823162581513\n",
      "actions trained:           [143. 286. 263.   4.  64. 313.]\n",
      "actions out while training:[ 93. 268. 285.   8. 120. 299.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 219\n",
      "Death at frame 306\n",
      "Death at frame 434\n",
      "Saving model at epoch 230: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 230, frames: 435, score: 110.0, average loss: 0.11647741238122271\n",
      "actions trained:           [ 96. 205.  34.  12.  77.  10.]\n",
      "actions out while training:[ 79. 196.  34.  28.  79.  18.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 877\n",
      "Death at frame 1004\n",
      "Death at frame 1248\n",
      "Epoch: 231, frames: 1249, score: 405.0, average loss: 0.13190721101232197\n",
      "actions trained:           [226. 124. 163. 128. 325. 282.]\n",
      "actions out while training:[126. 198. 164. 144. 178. 438.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 232\n",
      "Death at frame 325\n",
      "Death at frame 689\n",
      "Epoch: 232, frames: 690, score: 155.0, average loss: 0.1388897157528107\n",
      "actions trained:           [ 25. 310.  73.  11.  61. 209.]\n",
      "actions out while training:[ 63. 225. 127.  71.  65. 138.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 239\n",
      "Death at frame 584\n",
      "Death at frame 799\n",
      "Epoch: 233, frames: 800, score: 215.0, average loss: 0.1308062847484089\n",
      "actions trained:           [  8. 130. 130. 271.  48. 212.]\n",
      "actions out while training:[ 55. 199. 155.  53. 109. 228.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 383\n",
      "Death at frame 534\n",
      "Death at frame 630\n",
      "Epoch: 234, frames: 631, score: 155.0, average loss: 0.14238061723124276\n",
      "actions trained:           [ 92. 212.  59.   0. 121. 146.]\n",
      "actions out while training:[ 99. 227.  76.   1.  85. 142.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 156\n",
      "Death at frame 284\n",
      "Death at frame 696\n",
      "Epoch: 235, frames: 697, score: 225.0, average loss: 0.13400733850305233\n",
      "actions trained:           [264. 199.  91.   1.  72.  69.]\n",
      "actions out while training:[114. 242. 109.  23.  96. 112.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 169\n",
      "Death at frame 255\n",
      "Death at frame 403\n",
      "Epoch: 236, frames: 404, score: 100.0, average loss: 0.12205332902067492\n",
      "actions trained:           [ 40. 282.  13.   6.  21.  41.]\n",
      "actions out while training:[ 32. 235.  26.  20.  48.  42.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 296\n",
      "Death at frame 431\n",
      "Death at frame 808\n",
      "Epoch: 237, frames: 809, score: 240.0, average loss: 0.11691698028563603\n",
      "actions trained:           [  4. 120. 352.  42.  40. 250.]\n",
      "actions out while training:[  9. 194.  87. 101. 102. 315.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 205\n",
      "Death at frame 306\n",
      "Death at frame 646\n",
      "Epoch: 238, frames: 647, score: 105.0, average loss: 0.042673417167212846\n",
      "actions trained:           [ 25. 155.   1. 141. 126. 198.]\n",
      "actions out while training:[ 30. 190.   2. 139. 113. 172.]\n",
      "\n",
      "Using strict AI actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death at frame 284\n",
      "Death at frame 596\n",
      "Death at frame 824\n",
      "Epoch: 239, frames: 825, score: 155.0, average loss: 0.12027191218149923\n",
      "actions trained:           [168. 194.   2. 100.  75. 285.]\n",
      "actions out while training:[ 64. 220.  54.  72. 175. 239.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 207\n",
      "Death at frame 350\n",
      "Death at frame 576\n",
      "Saving model at epoch 240: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 240, frames: 577, score: 155.0, average loss: 0.12466885683067916\n",
      "actions trained:           [  5. 163.  14. 104. 209.  81.]\n",
      "actions out while training:[  8. 201.  23.  53. 219.  72.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 438\n",
      "Death at frame 614\n",
      "Death at frame 804\n",
      "Epoch: 241, frames: 805, score: 210.0, average loss: 0.13939281700783393\n",
      "actions trained:           [ 24. 338.  82.  52. 112. 196.]\n",
      "actions out while training:[ 92. 274.  90.  42.  82. 224.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 389\n",
      "Death at frame 555\n",
      "Death at frame 633\n",
      "Epoch: 242, frames: 634, score: 105.0, average loss: 0.07833734776979788\n",
      "actions trained:           [ 38. 170. 208.  35.  38. 144.]\n",
      "actions out while training:[ 51. 172. 136.  49.  32. 193.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 209\n",
      "Death at frame 309\n",
      "Death at frame 571\n",
      "Epoch: 243, frames: 572, score: 105.0, average loss: 0.06958093980912226\n",
      "actions trained:           [159. 125.   1.  22. 100. 164.]\n",
      "actions out while training:[145. 135.   8.  26.  59. 198.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 214\n",
      "Death at frame 314\n",
      "Death at frame 812\n",
      "Epoch: 244, frames: 813, score: 180.0, average loss: 0.13796153909505582\n",
      "actions trained:           [152. 264.   4.  45.  66. 281.]\n",
      "actions out while training:[ 57. 231.  96. 157.  77. 194.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 215\n",
      "Death at frame 315\n",
      "Death at frame 812\n",
      "Epoch: 245, frames: 813, score: 185.0, average loss: 0.14048997166874982\n",
      "actions trained:           [  1. 321. 107. 149.  36. 198.]\n",
      "actions out while training:[  1. 207.  71.  96. 137. 300.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 1008\n",
      "Death at frame 1213\n",
      "Death at frame 1319\n",
      "Epoch: 246, frames: 1320, score: 360.0, average loss: 0.13209007086595279\n",
      "actions trained:           [218. 186. 288.  36. 207. 384.]\n",
      "actions out while training:[205. 213. 123. 109. 140. 529.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 214\n",
      "Death at frame 315\n",
      "Death at frame 665\n",
      "Epoch: 247, frames: 666, score: 105.0, average loss: 0.1001306478921709\n",
      "actions trained:           [ 17. 182.  32.  93.  51. 290.]\n",
      "actions out while training:[ 85. 172.  71.  47.  85. 205.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 299\n",
      "Death at frame 801\n",
      "Death at frame 1195\n",
      "Epoch: 248, frames: 1196, score: 315.0, average loss: 0.12242215567377951\n",
      "actions trained:           [178. 203.  57. 207. 293. 257.]\n",
      "actions out while training:[ 91. 238.  89. 144. 104. 529.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 222\n",
      "Death at frame 320\n",
      "Death at frame 648\n",
      "Epoch: 249, frames: 649, score: 105.0, average loss: 0.09602897147818978\n",
      "actions trained:           [  1. 154.  37.   0.   1. 455.]\n",
      "actions out while training:[  6. 161. 204.  11.   2. 264.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 392\n",
      "Death at frame 851\n",
      "Death at frame 1122\n",
      "Saving model at epoch 250: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 250, frames: 1123, score: 245.0, average loss: 0.13083419933816945\n",
      "actions trained:           [148. 258. 168. 163.  12. 373.]\n",
      "actions out while training:[106. 246.  73. 163.  95. 439.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 240\n",
      "Death at frame 334\n",
      "Death at frame 687\n",
      "Epoch: 251, frames: 688, score: 110.0, average loss: 0.12085625389484304\n",
      "actions trained:           [ 68. 158.  20.  19. 227. 195.]\n",
      "actions out while training:[109. 172.  75.  64.  86. 181.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 273\n",
      "Death at frame 755\n",
      "Death at frame 1106\n",
      "Epoch: 252, frames: 1107, score: 330.0, average loss: 0.12570838844866972\n",
      "actions trained:           [ 25. 226. 168. 192.  50. 445.]\n",
      "actions out while training:[ 80. 258. 109. 106. 121. 432.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 223\n",
      "Death at frame 315\n",
      "Death at frame 630\n",
      "Epoch: 253, frames: 631, score: 135.0, average loss: 0.12312639744559509\n",
      "actions trained:           [152. 162.  27.  30.  56. 203.]\n",
      "actions out while training:[ 65. 199.  91.  69.  70. 136.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 271\n",
      "Death at frame 386\n",
      "Death at frame 985\n",
      "Epoch: 254, frames: 986, score: 275.0, average loss: 0.13755291077728143\n",
      "actions trained:           [140. 186.  28. 130. 265. 236.]\n",
      "actions out while training:[139. 292.  89. 137.  98. 230.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 429\n",
      "Death at frame 583\n",
      "Death at frame 693\n",
      "Epoch: 255, frames: 694, score: 210.0, average loss: 0.11942231543009774\n",
      "actions trained:           [ 21. 321. 188.  26.   0. 137.]\n",
      "actions out while training:[ 74. 288. 100.  60.   1. 170.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 227\n",
      "Death at frame 322\n",
      "Death at frame 452\n",
      "Epoch: 256, frames: 453, score: 80.0, average loss: 0.12357747192241612\n",
      "actions trained:           [ 54. 275.  62.  43.   1.  17.]\n",
      "actions out while training:[ 60. 201.  67.  87.   6.  31.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 169\n",
      "Death at frame 250\n",
      "Death at frame 388\n",
      "Epoch: 257, frames: 389, score: 105.0, average loss: 0.11309140084531102\n",
      "actions trained:           [ 37. 238.  21.  65.  20.   7.]\n",
      "actions out while training:[ 33. 221.  34.  49.  28.  23.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 731\n",
      "Death at frame 823\n",
      "Death at frame 969\n",
      "Epoch: 258, frames: 970, score: 460.0, average loss: 0.13420410671898358\n",
      "actions trained:           [227. 140.  99.  54. 114. 335.]\n",
      "actions out while training:[ 98. 229. 203. 125.  97. 217.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 236\n",
      "Death at frame 338\n",
      "Death at frame 471\n",
      "Epoch: 259, frames: 472, score: 100.0, average loss: 0.12946827484787818\n",
      "actions trained:           [ 12. 266.  65. 107.  10.  11.]\n",
      "actions out while training:[ 26. 198. 116.  58.  38.  35.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 253\n",
      "Death at frame 502\n",
      "Death at frame 919\n",
      "Saving model at epoch 260: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 260, frames: 920, score: 265.0, average loss: 0.13979929744659164\n",
      "actions trained:           [ 73. 171. 189. 149. 126. 211.]\n",
      "actions out while training:[131. 280. 101.  89. 137. 181.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 291\n",
      "Death at frame 758\n",
      "Death at frame 956\n",
      "Epoch: 261, frames: 957, score: 270.0, average loss: 0.1310282318735969\n",
      "actions trained:           [333. 316.  45.  45.  61. 156.]\n",
      "actions out while training:[122. 292. 135.  80. 141. 186.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 544\n",
      "Death at frame 909\n",
      "Death at frame 1022\n",
      "Epoch: 262, frames: 1023, score: 515.0, average loss: 0.14913563586438747\n",
      "actions trained:           [  5. 277. 104.  71. 208. 357.]\n",
      "actions out while training:[ 43. 258. 144. 133. 178. 266.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 517\n",
      "Death at frame 792\n",
      "Death at frame 970\n",
      "Epoch: 263, frames: 971, score: 260.0, average loss: 0.13688927976059603\n",
      "actions trained:           [ 38. 274. 116. 139. 196. 207.]\n",
      "actions out while training:[ 84. 248. 146. 155. 134. 203.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 186\n",
      "Death at frame 319\n",
      "Death at frame 699\n",
      "Epoch: 264, frames: 700, score: 220.0, average loss: 0.13246934438737054\n",
      "actions trained:           [163. 205. 153.  77.  46.  55.]\n",
      "actions out while training:[125. 283.  66.  94.  67.  64.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 421\n",
      "Death at frame 502\n",
      "Death at frame 797\n",
      "Epoch: 265, frames: 798, score: 210.0, average loss: 0.13326573431385447\n",
      "actions trained:           [ 24. 364. 183.  41.  40. 145.]\n",
      "actions out while training:[ 67. 256.  91. 106.  79. 198.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 288\n",
      "Death at frame 1509\n",
      "Epoch: 266, frames: 1510, score: 530.0, average loss: 0.1349787423441966\n",
      "actions trained:           [148. 219. 117. 339. 249. 437.]\n",
      "actions out while training:[198. 352. 240. 168. 142. 409.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 291\n",
      "Death at frame 759\n",
      "Death at frame 970\n",
      "Epoch: 267, frames: 971, score: 260.0, average loss: 0.13783185042675114\n",
      "actions trained:           [ 75. 501.  71.  16.  32. 275.]\n",
      "actions out while training:[121. 237.  87.  79. 137. 309.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 279\n",
      "Death at frame 908\n",
      "Death at frame 1296\n",
      "Epoch: 268, frames: 1297, score: 535.0, average loss: 0.13124106088555051\n",
      "actions trained:           [165. 166. 139. 290. 139. 397.]\n",
      "actions out while training:[126. 267. 235. 113. 161. 394.]\n",
      "\n",
      "Using strict AI actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death at frame 230\n",
      "Death at frame 824\n",
      "Death at frame 981\n",
      "Epoch: 269, frames: 982, score: 460.0, average loss: 0.12747399661416284\n",
      "actions trained:           [ 43. 281. 142.   6.  56. 453.]\n",
      "actions out while training:[170. 321. 129.  22.  87. 252.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 248\n",
      "Death at frame 447\n",
      "Death at frame 644\n",
      "Saving model at epoch 270: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 270, frames: 645, score: 130.0, average loss: 0.12679434755692703\n",
      "actions trained:           [ 85. 316.  12.  88.  93.  50.]\n",
      "actions out while training:[ 59. 293.  83.  49.  69.  91.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 341\n",
      "Death at frame 432\n",
      "Death at frame 696\n",
      "Epoch: 271, frames: 697, score: 170.0, average loss: 0.12810867342607168\n",
      "actions trained:           [ 17. 347. 107.   2. 103. 120.]\n",
      "actions out while training:[ 78. 220. 109.  34.  39. 216.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 336\n",
      "Death at frame 753\n",
      "Death at frame 838\n",
      "Epoch: 272, frames: 839, score: 210.0, average loss: 0.12718795356629417\n",
      "actions trained:           [133.  81. 285.  72.  10. 257.]\n",
      "actions out while training:[ 97. 158.  83. 117.  68. 315.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 209\n",
      "Death at frame 305\n",
      "Death at frame 999\n",
      "Epoch: 273, frames: 1000, score: 295.0, average loss: 0.1333296984341811\n",
      "actions trained:           [ 80. 189.  39. 151. 121. 419.]\n",
      "actions out while training:[136. 208.  91.  83. 121. 360.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 176\n",
      "Death at frame 546\n",
      "Death at frame 658\n",
      "Epoch: 274, frames: 659, score: 205.0, average loss: 0.13781808670731668\n",
      "actions trained:           [101. 293.  55.   8.  61. 140.]\n",
      "actions out while training:[128. 227.  63.  16.  83. 141.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 174\n",
      "Death at frame 259\n",
      "Death at frame 355\n",
      "Epoch: 275, frames: 356, score: 45.0, average loss: 0.08096884512049161\n",
      "actions trained:           [135. 143.  19.  20.  20.  18.]\n",
      "actions out while training:[158. 137.  16.  14.  16.  14.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 563\n",
      "Death at frame 834\n",
      "Death at frame 979\n",
      "Epoch: 276, frames: 980, score: 305.0, average loss: 0.14075352646884462\n",
      "actions trained:           [236. 113.  64.  98. 194. 274.]\n",
      "actions out while training:[ 66. 142. 180. 140. 179. 272.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 206\n",
      "Death at frame 582\n",
      "Death at frame 847\n",
      "Epoch: 277, frames: 848, score: 120.0, average loss: 0.1346486537406306\n",
      "actions trained:           [ 38. 229. 199. 117.  22. 242.]\n",
      "actions out while training:[ 65. 202. 130.  86.  80. 284.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 173\n",
      "Death at frame 335\n",
      "Death at frame 643\n",
      "Epoch: 278, frames: 644, score: 195.0, average loss: 0.14216724312284498\n",
      "actions trained:           [ 14. 158.  97.  50. 165. 159.]\n",
      "actions out while training:[ 31. 157. 165.  61. 109. 120.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 588\n",
      "Death at frame 909\n",
      "Death at frame 1175\n",
      "Epoch: 279, frames: 1176, score: 220.0, average loss: 0.13421166684564317\n",
      "actions trained:           [106. 296. 183. 101. 246. 243.]\n",
      "actions out while training:[161. 215. 154. 198.  93. 354.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 226\n",
      "Death at frame 313\n",
      "Death at frame 637\n",
      "Saving model at epoch 280: ./models/space_invaders_few_layers_rl\n",
      "Epoch: 280, frames: 638, score: 135.0, average loss: 0.12607419664677239\n",
      "actions trained:           [ 50.  78.  93. 165.   2. 249.]\n",
      "actions out while training:[134.  96. 159.  69.  16. 163.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 211\n",
      "Death at frame 574\n",
      "Death at frame 670\n",
      "Epoch: 281, frames: 671, score: 175.0, average loss: 0.14474129501739877\n",
      "actions trained:           [197. 142.  24.  47.  87. 173.]\n",
      "actions out while training:[134. 188.  89.  50.  93. 116.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 209\n",
      "Death at frame 874\n",
      "Death at frame 902\n",
      "Epoch: 282, frames: 903, score: 345.0, average loss: 0.13613840908269656\n",
      "actions trained:           [214. 223. 211.  45.  98. 111.]\n",
      "actions out while training:[176. 306. 136.  71.  91. 122.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 315\n",
      "Death at frame 515\n",
      "Death at frame 920\n",
      "Epoch: 283, frames: 921, score: 285.0, average loss: 0.1289884014066543\n",
      "actions trained:           [213. 318.  70.  23.  52. 244.]\n",
      "actions out while training:[ 57. 437.  93.  55.  55. 223.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 205\n",
      "Death at frame 290\n",
      "Death at frame 509\n",
      "Epoch: 284, frames: 510, score: 65.0, average loss: 0.08097773926794238\n",
      "actions trained:           [  1. 223. 224.  51.  10.   0.]\n",
      "actions out while training:[  1. 254. 180.  59.  13.   2.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 232\n",
      "Death at frame 903\n",
      "Death at frame 988\n",
      "Epoch: 285, frames: 989, score: 510.0, average loss: 0.13471595113267837\n",
      "actions trained:           [ 14. 418.  65. 221. 128. 142.]\n",
      "actions out while training:[ 39. 250.  96.  97. 279. 227.]\n",
      "\n",
      "Using strict AI actions\n",
      "Death at frame 223\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name)\n",
    "global_step = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if load_model:\n",
    "        print(\"Loading existing model before training: {}\".format(saver_file))\n",
    "        saver.restore(sess, saver_file)\n",
    "    else:\n",
    "        print(\"Creating new model before training: {}\".format(saver_file))\n",
    "        sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):  #play n_epochs games\n",
    "        observation = env.reset()\n",
    "        temp_lives = 3\n",
    "        score = 0.0\n",
    "        frames = np.empty([height, width, 0])\n",
    "        actions = np.empty([0])\n",
    "        rewards = np.empty([0])\n",
    "        all_logits = []\n",
    "        punish_frames = []\n",
    "        \n",
    "        #get the first frame\n",
    "        input_value = 0  #set an initial input value\n",
    "        #observation, reward_float, done_bool, info_dict = env.step(input_value)\n",
    "        obs_greyscale = preprocess_observation(observation)\n",
    "        obs_greyscale_reshape = np.reshape(obs_greyscale, (height,width,1))\n",
    "        frames = np.append(frames, obs_greyscale_reshape, axis=2)\n",
    "        actions = np.append(actions, input_value)\n",
    "        last_action_step = 0\n",
    "        decision_step_counter = 0\n",
    "    \n",
    "        \n",
    "        game_step_counter = 0\n",
    "        action_from_ai_epsilon_greedy = 0\n",
    "        \n",
    "        #play the game until the first death, recording the frames along the way\n",
    "        gc.disable()\n",
    "        while True:\n",
    "            #this_frame = frames[:,:,np.ma.size(frames, axis=2)-1]\n",
    "            #this_flattened_frame = this_frame.flatten()\n",
    "            #this_flattened_frame_reshaped = np.reshape(this_flattened_frame, (1, this_flattened_frame.size))\n",
    "            \n",
    "            num_frames = np.ma.size(frames, axis=2)\n",
    "                       \n",
    "            concatenated_frames = frames[:,:,num_frames-1]\n",
    "            \n",
    "            for j in range(1,frames_captured,1):\n",
    "                if num_frames-j <= 1:\n",
    "                    this_frame = frames[:,:,0]\n",
    "                elif j >= num_frames:\n",
    "                    this_frame = frames[:,:,num_frames-1]\n",
    "                else:\n",
    "                    this_frame = frames[:,:,num_frames-1-j]\n",
    "                concatenated_frames = np.append(concatenated_frames, this_frame, axis=0)\n",
    "            \n",
    "            #concatenated_frames_reshaped = np.reshape(concatenated_frames, (1, 210*5*160))\n",
    "            concatenated_frames_reshaped = np.reshape(concatenated_frames, (1, height*frames_captured, width, 1))\n",
    "            \n",
    "            #todo - write a function to build the concatenated_frames\n",
    "            \n",
    "            #use the input value from the AI\n",
    "            #temp_input = [0,0,0,0,0,0]\n",
    "            temp_input = np.zeros(discrete_actions) #[0, 0, 0, ...]\n",
    "            temp_input[action_from_ai_epsilon_greedy] = 1\n",
    "            temp_input_reshaped = np.reshape(temp_input, (1, len(temp_input)))\n",
    "            \n",
    "            #tf_dropout_keep_prob: 1.0,\n",
    "            feed_dict = {tf_input_frame : concatenated_frames_reshaped, \n",
    "                         tf_input_value : temp_input_reshaped,\n",
    "                         tf_input_learning_rate: 0.0,\n",
    "                         tf_dropout_keep_prob: 1.0,\n",
    "                         tf_reward: 0.0}\n",
    "            logits_out = sess.run([logits], feed_dict=feed_dict)\n",
    "            #all_gradients.append(gradients_out)\n",
    "            #all_variables.append(variables_out)\n",
    "\n",
    "            all_logits.append(logits_out[0])\n",
    "            \n",
    "            #if global_step % 5 == 0:  #only allow a change of direction every 5 steps.  \n",
    "            action_from_ai_logits_argmax = np.argmax(logits_out[0])\n",
    "            \n",
    "#             print(str(logits_out[0]) + \": \" + str(np.argmax(logits_out[0])))\n",
    "            \n",
    "#             print(\"output_probability_out: \" + str(output_probability_out))\n",
    "#             print(\"action_from_ai_logits_argmax: \" + str(action_from_ai_logits_argmax) + \", action_from_ai: \" + str(action_from_ai), \", numpy_choice: \" + str(numpy_choice))\n",
    "#             print(\"var: \" + str(var_out))\n",
    "             \n",
    "    \n",
    "#            print (\"logits_out[0]: {}\".format(logits_out[0]))\n",
    "    \n",
    "            positive_logits = logits_out[0] + abs(np.amin(logits_out[0]))\n",
    "            softmax_logits = softmax(positive_logits / np.amax(positive_logits))\n",
    "#            print (\"softmax_logits[0]: {}\".format(softmax_logits[0]))\n",
    "\n",
    "            try:\n",
    "                multinomial_action_array = np.random.multinomial(1, softmax_logits[0])\n",
    "                action_from_multinomial_action = np.argmax(multinomial_action_array)\n",
    "#                print (\"multinomial_action_array: {}\".format(multinomial_action_array))\n",
    "            except ValueError:\n",
    "                #I have no idea why this occassionally errors out.\n",
    "                action_from_multinomial_action = np.argmax(softmax_logits)\n",
    "                print (\"multinomial error, using action {}\".format(action_from_multinomial_action))\n",
    "                continue\n",
    "                \n",
    "#            print (\"action_from_multinomial_action: {}\".format(action_from_multinomial_action))\n",
    "        \n",
    "            # decide which action to use\n",
    "            if epoch % use_ai_every_x_epoch == 0 and epoch >= 0:\n",
    "                if game_step_counter == 0:\n",
    "                    print(\"Using strict AI actions\")\n",
    "                action_from_ai_epsilon_greedy = action_from_ai_logits_argmax  #do what the AI says\n",
    "            else:\n",
    "#                print(\"Using probability-based actions\")\n",
    "                action_from_ai_epsilon_greedy = action_from_multinomial_action  #use probability-based action\n",
    "            #action_from_ai_epsilon_greedy = action_from_ai  #do what the AI says with probabilities using tf.multinomial probabilities\n",
    "            #action_from_ai_epsilon_greedy = numpy_choice #do what the AI says with probabilities using numpy  probabilities\n",
    "                \n",
    "            #run the next step given the input from the logits\n",
    "            observation, reward_float, done_bool, info_dict = env.step(action_from_ai_epsilon_greedy)\n",
    "            \n",
    "            #add this frame to our frame buffer\n",
    "            obs_greyscale = preprocess_observation(observation)\n",
    "            obs_greyscale_reshape = np.reshape(obs_greyscale, (height,width,1))\n",
    "            frames = np.append(frames, obs_greyscale_reshape, axis=2)\n",
    "            actions = np.append(actions, action_from_ai_epsilon_greedy)\n",
    "            \n",
    "            score = score + reward_float\n",
    "            \n",
    "            if reward_float > max_score:\n",
    "                rewards = np.append(rewards, max_score)\n",
    "            else:\n",
    "                rewards = np.append(rewards, reward_float)\n",
    "            \n",
    "            lives = info_dict['ale.lives']\n",
    "\n",
    "            if done_bool:\n",
    "                punish_frames.append(len(rewards) - 40)\n",
    "                print(\"Death at frame {}\".format(len(rewards)))\n",
    "                break\n",
    "                \n",
    "            if game_step_counter > frame_limit:\n",
    "                break\n",
    "                \n",
    "            if lives != temp_lives:  #we lost a life.  consider this game over.\n",
    "                #print(\"Lost a life.  Current lives: {}\".format(lives))\n",
    "                temp_lives = lives\n",
    "                if len(rewards) > 5:\n",
    "                    punish_frames.append(len(rewards) - 40)\n",
    "                    print(\"Death at frame {}\".format(len(rewards)))\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                env.render()  #display the current frame.\n",
    "                \n",
    "            decision_step_counter += 1\n",
    "            game_step_counter += 1\n",
    "            global_step += 1\n",
    "        gc.enable()\n",
    "        if score == 0:\n",
    "            continue #causes an error and breaks the model.  just continue.\n",
    "        \n",
    "        if epoch % 10 == 0 and epoch > 0:\n",
    "            print(\"Saving model at epoch {}: {}\".format(epoch, saver_file))\n",
    "            saver.save(sess, saver_file)\n",
    "        \n",
    "        #punish death\n",
    "        for this_frame in punish_frames:\n",
    "            #rewards[this_frame] = (-1.0 * score / len(punish_frames)) / 2.0\n",
    "            rewards[this_frame] = -10\n",
    "        \n",
    "        num_frames = np.ma.size(frames, axis=2)\n",
    "        frames_to_skip_begin = 0\n",
    "        frames_to_skip_end  = 0  #number of frames between pacman being eaten and game reset\n",
    "    \n",
    "        discounted_rewards = helper_discount_rewards(rewards, discount_decay_rate, frames_to_skip_begin, num_frames) #-1-frames_to_skip_end\n",
    "        discounted_rewards_median = np.median(discounted_rewards)\n",
    "        discounted_rewards_mean = np.mean(discounted_rewards)\n",
    "        average_logits = get_average_logits(all_logits, discounted_rewards)\n",
    "        \n",
    "        display_actions = np.zeros(discrete_actions)\n",
    "        ai_actions = np.zeros(discrete_actions)\n",
    "        loss_out_sum = 0.0\n",
    "        frames_taught = 0.000000001\n",
    "        \n",
    "        skipped_frames = 0\n",
    "        temp_action = 0\n",
    "        \n",
    "        reward_frame_counter = 0\n",
    "        punish_frame_counter = 0\n",
    "            \n",
    "        frames_to_train = np.arange(num_frames-1-frames_to_skip_end)\n",
    "        np.random.shuffle(frames_to_train)\n",
    "        \n",
    "        for i in range(frames_to_skip_begin, num_frames-1-frames_to_skip_end):   #skip the first frames\n",
    "\n",
    "            this_random_frame_index = frames_to_train[i]\n",
    "\n",
    "            if i >= num_frames-1-frames_to_skip_end: \n",
    "                continue  #only train if frame is not during pacman's death throws \n",
    "            \n",
    "            if True: \n",
    "                concatenated_frames = frames[:,:,this_random_frame_index]\n",
    "                for j in range(-1,-1*frames_captured,-1):\n",
    "                    this_frame = frames[:,:,this_random_frame_index+j]\n",
    "                    this_flattened_frame = this_frame.flatten()\n",
    "                    concatenated_frames = np.append(concatenated_frames, this_frame, axis=0)\n",
    "                    \n",
    "#                concatenated_frames_reshaped = np.reshape(concatenated_frames, (1, 210*5*160))\n",
    "                concatenated_frames_reshaped = np.reshape(concatenated_frames, (1, height*frames_captured, width, 1))\n",
    "            \n",
    "#                action_taken_one_hot = action_to_one_hot(actions[i], n_outputs, rewards[i])\n",
    "                action_taken_one_hot = action_to_one_hot(actions[this_random_frame_index], n_outputs)\n",
    "                display_actions = np.add(display_actions, action_taken_one_hot)\n",
    "                \n",
    "                #reward_for_frame = discounted_rewards[i] + (-1.0 * abs(discounted_rewards_median))  # hopefully, adding the median will keep the AI from getting stuck taking one action\n",
    "                #reward_for_frame = discounted_rewards[i] - .05\n",
    "                reward_for_frame = discounted_rewards[this_random_frame_index]\n",
    "                #print(\"reward for frame {} is {}.  action: {}\".format(i, reward_for_frame, action_taken_one_hot_reshaped))\n",
    "               \n",
    "                if reward_for_frame > 0.85:\n",
    "                    reward_for_frame = 1.0\n",
    "                elif reward_for_frame < 0.0:\n",
    "                    reward_for_frame = -1.0\n",
    "                    \n",
    "                action_taken_one_hot = action_taken_one_hot * reward_for_frame\n",
    "                \n",
    "#                 if reward_for_frame != 1.0:\n",
    "#                     action_taken_one_hot = action_taken_one_hot - average_logits[int(actions[this_random_frame_index])]\n",
    "    \n",
    "                action_taken_one_hot_reshaped = np.reshape(action_taken_one_hot, (1, len(action_taken_one_hot)))\n",
    "    \n",
    "#                print(action_taken_one_hot_reshaped)\n",
    "                \n",
    "                feed_dict = {tf_input_frame : concatenated_frames_reshaped, \n",
    "                             tf_input_value : action_taken_one_hot_reshaped,\n",
    "                             tf_input_learning_rate: learning_rate,\n",
    "                             tf_dropout_keep_prob: dropout_keep_prob,\n",
    "                             tf_reward: reward_for_frame}\n",
    "                loss_out, _, logits_out = sess.run([loss, training_op, logits], feed_dict=feed_dict)\n",
    "\n",
    "                frames_taught = frames_taught + 1\n",
    "                loss_out_sum += loss_out\n",
    "\n",
    "                action_from_ai = np.argmax(logits_out[0])\n",
    "                action_from_ai_one_hot = action_to_one_hot(action_from_ai, n_outputs)\n",
    "                ai_actions = np.add(ai_actions, action_from_ai_one_hot)\n",
    "            else:\n",
    "                skipped_frames = skipped_frames + 1\n",
    "            \n",
    "            temp_action = actions[i]\n",
    "#             if epoch % 10 == 0:\n",
    "#                 print(\"ai action[\" + str(i) + \"]: \" + str(actions[i]) + \", action_taken_one_hot: \" + \n",
    "#                       str(action_taken_one_hot_reshaped) + \", loss_out: \" + str(loss_out))\n",
    "#                 print(\"logits: \" + str(logits_out[0]))\n",
    "        \n",
    "        print(\"Epoch: \" + str(epoch) + \", frames: \" + str(num_frames) + \", score: \" + str(score) + \", average loss: \" + str(loss_out_sum/frames_taught))\n",
    "        #print(\"actions trained: {}, rewards: {}, punishments: {}, median: {}, mean: {}\".format(display_actions, reward_frame_counter, punish_frame_counter, discounted_rewards_median, discounted_rewards_mean))\n",
    "        print(\"actions trained:           {}\".format(display_actions))\n",
    "        print(\"actions out while training:{}\".format(ai_actions))\n",
    "        #print(\"discounted_rewards: \" + str(discounted_rewards))\n",
    "        #TODO - should learning rate decrease over time?\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set above epochs to 1.  \n",
    "#create a concatenated image of the last 5 frames\n",
    "\n",
    "\n",
    "\n",
    "frame_to_view = 222\n",
    "concatenated_frames = frames[:,:,frame_to_view]\n",
    "\n",
    "\n",
    "for j in range(frame_to_view-1, frame_to_view):\n",
    "    print(\"discounted rewared at frame {}: {}\".format(j, discounted_rewards[j]))\n",
    "    this_frame = frames[:,:,j]\n",
    "    #print(\"this_frame: \" + str(this_frame.shape))\n",
    "    concatenated_frames = np.append(concatenated_frames, this_frame, axis=0)\n",
    "\n",
    "    \n",
    "# print(concatenated_frames)\n",
    "# print the image\n",
    "#show_observation(concatenated_frames)\n",
    "\n",
    "counter = 0\n",
    "for this_reward, this_discounted_reward in zip(rewards, discounted_rewards):\n",
    "    print(\"{}: reward: {}, discounted reward: {}\".format(counter, this_reward, this_discounted_reward))\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "#print(actions)\n",
    "#print(all_logits)\n",
    "\n",
    "# logit_sums = np.zeros(6)\n",
    "\n",
    "# for this_logit in all_logits:\n",
    "#     logit_sums = logit_sums + this_logit\n",
    "# #    print(this_logit)\n",
    "\n",
    "# print(logit_sums/len(all_logits))\n",
    "# print(get_average_logits(all_logits, discounted_rewards))\n",
    "\n",
    "# print(actions[5])\n",
    "# print(average_logits[int(actions[5])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_frames: 1086\n",
      "0: reward: 0.0, discounted reward: 0.0766933783174092\n",
      "1: reward: 0.0, discounted reward: 0.07899669881315724\n",
      "2: reward: 0.0, discounted reward: 0.08137125602526862\n",
      "3: reward: 0.0, discounted reward: 0.08381925315115664\n",
      "4: reward: 0.0, discounted reward: 0.0863429615283608\n",
      "5: reward: 0.0, discounted reward: 0.08894472274197332\n",
      "6: reward: 0.0, discounted reward: 0.09162695079724396\n",
      "7: reward: 0.0, discounted reward: 0.09439213435937864\n",
      "8: reward: 0.0, discounted reward: 0.09724283906261028\n",
      "9: reward: 0.0, discounted reward: 0.10018170989068412\n",
      "10: reward: 0.0, discounted reward: 0.10321147363096643\n",
      "11: reward: 0.0, discounted reward: 0.10633494140445336\n",
      "12: reward: 0.0, discounted reward: 0.1095550112740275\n",
      "13: reward: 0.0, discounted reward: 0.1128746709333823\n",
      "14: reward: 0.0, discounted reward: 0.1162970004791089\n",
      "15: reward: 0.0, discounted reward: 0.11982517526851774\n",
      "16: reward: 0.0, discounted reward: 0.12346246886584647\n",
      "17: reward: 0.0, discounted reward: 0.12721225607958742\n",
      "18: reward: 0.0, discounted reward: 0.13107801609375336\n",
      "19: reward: 0.0, discounted reward: 0.13506333569598628\n",
      "20: reward: 0.0, discounted reward: 0.13917191260550474\n",
      "21: reward: 0.0, discounted reward: 0.1434075589039774\n",
      "22: reward: 0.0, discounted reward: 0.1477742045725059\n",
      "23: reward: 0.0, discounted reward: 0.1522759011379992\n",
      "24: reward: 0.0, discounted reward: 0.15691682543232216\n",
      "25: reward: 0.0, discounted reward: 0.16170128346770668\n",
      "26: reward: 0.0, discounted reward: 0.16663371443202063\n",
      "27: reward: 0.0, discounted reward: 0.17171869480760202\n",
      "28: reward: 0.0, discounted reward: 0.17696094261747972\n",
      "29: reward: 0.0, discounted reward: 0.1823653218029207\n",
      "30: reward: 0.0, discounted reward: 0.18793684673636496\n",
      "31: reward: 0.0, discounted reward: 0.1936806868739364\n",
      "32: reward: 0.0, discounted reward: 0.1996021715518451\n",
      "33: reward: 0.0, discounted reward: 0.2057067949311324\n",
      "34: reward: 0.0, discounted reward: 0.21200022109534608\n",
      "35: reward: 0.0, discounted reward: 0.2184882893058757\n",
      "36: reward: 0.0, discounted reward: 0.22517701941982374\n",
      "37: reward: 0.0, discounted reward: 0.23207261747544028\n",
      "38: reward: 0.0, discounted reward: 0.2391814814503027\n",
      "39: reward: 0.0, discounted reward: 0.24651020719758354\n",
      "40: reward: 0.0, discounted reward: 0.2540655945659143\n",
      "41: reward: 0.0, discounted reward: 0.26185465370852334\n",
      "42: reward: 0.0, discounted reward: 0.2698846115875017\n",
      "43: reward: 0.0, discounted reward: 0.278162918679232\n",
      "44: reward: 0.0, discounted reward: 0.28669725588720135\n",
      "45: reward: 0.0, discounted reward: 0.2954955416686131\n",
      "46: reward: 0.0, discounted reward: 0.3045659393814087\n",
      "47: reward: 0.0, discounted reward: 0.31391686485851755\n",
      "48: reward: 0.0, discounted reward: 0.32355699421636175\n",
      "49: reward: 0.0, discounted reward: 0.33349527190486095\n",
      "50: reward: 0.0, discounted reward: 0.3437409190064064\n",
      "51: reward: 5.0, discounted reward: 0.3543034417915049\n",
      "52: reward: 0.0, discounted reward: 0.17776959231843614\n",
      "53: reward: 0.0, discounted reward: 0.18319898128844275\n",
      "54: reward: 0.0, discounted reward: 0.18879628950494443\n",
      "55: reward: 0.0, discounted reward: 0.19456671034669873\n",
      "56: reward: 0.0, discounted reward: 0.20051559781242478\n",
      "57: reward: 0.0, discounted reward: 0.20664847148843105\n",
      "58: reward: 0.0, discounted reward: 0.21297102166988083\n",
      "59: reward: 0.0, discounted reward: 0.21948911464044757\n",
      "60: reward: 0.0, discounted reward: 0.22620879811525865\n",
      "61: reward: 0.0, discounted reward: 0.23313630685217732\n",
      "62: reward: 0.0, discounted reward: 0.24027806843662952\n",
      "63: reward: 0.0, discounted reward: 0.24764070924534315\n",
      "64: reward: 0.0, discounted reward: 0.2552310605945325\n",
      "65: reward: 0.0, discounted reward: 0.26305616507823276\n",
      "66: reward: 0.0, discounted reward: 0.2711232831026661\n",
      "67: reward: 0.0, discounted reward: 0.27943989962270044\n",
      "68: reward: 0.0, discounted reward: 0.28801373108665335\n",
      "69: reward: 0.0, discounted reward: 0.2968527325958832\n",
      "70: reward: 0.0, discounted reward: 0.30596510528581083\n",
      "71: reward: 0.0, discounted reward: 0.3153593039352208\n",
      "72: reward: 0.0, discounted reward: 0.32504404481090116\n",
      "73: reward: 0.0, discounted reward: 0.3350283137549015\n",
      "74: reward: 0.0, discounted reward: 0.3453213745219122\n",
      "75: reward: 0.0, discounted reward: 0.3559327773745005\n",
      "76: reward: 0.0, discounted reward: 0.3668723679441792\n",
      "77: reward: 0.0, discounted reward: 0.37815029636652847\n",
      "78: reward: 0.0, discounted reward: 0.3897770266988472\n",
      "79: reward: 0.0, discounted reward: 0.4017633466290727\n",
      "80: reward: 0.0, discounted reward: 0.41412037748497527\n",
      "81: reward: 0.0, discounted reward: 0.42685958455291606\n",
      "82: reward: 0.0, discounted reward: 0.43999278771574163\n",
      "83: reward: 0.0, discounted reward: 0.4535321724196856\n",
      "84: reward: 0.0, discounted reward: 0.46749030098045247\n",
      "85: reward: 0.0, discounted reward: 0.481880124238975\n",
      "86: reward: 0.0, discounted reward: 0.4967149935776581\n",
      "87: reward: 0.0, discounted reward: 0.5120086733082592\n",
      "88: reward: 0.0, discounted reward: 0.5277753534428995\n",
      "89: reward: 0.0, discounted reward: 0.5440296628600545\n",
      "90: reward: 0.0, discounted reward: 0.56078668287774\n",
      "91: reward: 0.0, discounted reward: 0.578061961246488\n",
      "92: reward: 0.0, discounted reward: 0.5958715265750942\n",
      "93: reward: 0.0, discounted reward: 0.6142319032025233\n",
      "94: reward: 0.0, discounted reward: 0.6331601265297697\n",
      "95: reward: 0.0, discounted reward: 0.6526737588259001\n",
      "96: reward: 0.0, discounted reward: 0.6727909055229415\n",
      "97: reward: 0.0, discounted reward: 0.6935302320147371\n",
      "98: reward: 0.0, discounted reward: 0.7149109809753509\n",
      "99: reward: 0.0, discounted reward: 0.7369529902130971\n",
      "100: reward: 0.0, discounted reward: 0.759676711076753\n",
      "101: reward: 0.0, discounted reward: 0.7831032274310374\n",
      "102: reward: 0.0, discounted reward: 0.8072542752189596\n",
      "103: reward: 0.0, discounted reward: 0.8321522626291886\n",
      "104: reward: 0.0, discounted reward: 0.8578202908871566\n",
      "105: reward: 0.0, discounted reward: 0.8842821756891855\n",
      "106: reward: 0.0, discounted reward: 0.9115624692995247\n",
      "107: reward: 0.0, discounted reward: 0.9396864833308021\n",
      "108: reward: 0.0, discounted reward: 0.9686803122290263\n",
      "109: reward: 0.0, discounted reward: 0.9985708574849275\n",
      "110: reward: 0.0, discounted reward: 1.029385852594104\n",
      "111: reward: 0.0, discounted reward: 1.0611538887891312\n",
      "112: reward: 0.0, discounted reward: 1.0939044415675099\n",
      "113: reward: 0.0, discounted reward: 1.1276678980400652\n",
      "114: reward: 0.0, discounted reward: 1.1624755851251738\n",
      "115: reward: 0.0, discounted reward: 1.1983597986149763\n",
      "116: reward: 0.0, discounted reward: 1.235353833140546\n",
      "117: reward: 0.0, discounted reward: 1.2734920130638139\n",
      "118: reward: 0.0, discounted reward: 1.3128097243249146\n",
      "119: reward: 0.0, discounted reward: 1.3533434472745032\n",
      "120: reward: 0.0, discounted reward: 1.3951307905215014\n",
      "121: reward: 0.0, discounted reward: 1.4382105258276854\n",
      "122: reward: 0.0, discounted reward: 1.482622624081483\n",
      "123: reward: 0.0, discounted reward: 1.5284082923843676\n",
      "124: reward: 0.0, discounted reward: 1.5756100122842482\n",
      "125: reward: 25.0, discounted reward: 1.6242715791913418\n",
      "126: reward: 0.0, discounted reward: 0.737322901910123\n",
      "127: reward: 0.0, discounted reward: 0.7600580633417281\n",
      "128: reward: 0.0, discounted reward: 0.7834963740959602\n",
      "129: reward: 0.0, discounted reward: 0.8076595810590861\n",
      "130: reward: 0.0, discounted reward: 0.832570103701484\n",
      "131: reward: 0.0, discounted reward: 0.8582510548792137\n",
      "132: reward: 5.0, discounted reward: 0.8847262622789351\n",
      "133: reward: 0.0, discounted reward: 0.7245972423054776\n",
      "134: reward: 0.0, discounted reward: 0.7469388266359083\n",
      "135: reward: 0.0, discounted reward: 0.7699713878013006\n",
      "136: reward: 0.0, discounted reward: 0.7937162962192309\n",
      "137: reward: 0.0, discounted reward: 0.8181955832480249\n",
      "138: reward: 0.0, discounted reward: 0.843431961628225\n",
      "139: reward: 0.0, discounted reward: 0.8694488465562663\n",
      "140: reward: 0.0, discounted reward: 0.8962703774099171\n",
      "141: reward: 0.0, discounted reward: 0.9239214401456397\n",
      "142: reward: 0.0, discounted reward: 0.9524276903886527\n",
      "143: reward: 10.0, discounted reward: 0.9818155772371197\n",
      "144: reward: 0.0, discounted reward: 0.6372662713613635\n",
      "145: reward: 0.0, discounted reward: 0.6569068978275431\n",
      "146: reward: 0.0, discounted reward: 0.6771549663493778\n",
      "147: reward: 0.0, discounted reward: 0.6980292637945681\n",
      "148: reward: 0.0, discounted reward: 0.7195491580679603\n",
      "149: reward: 0.0, discounted reward: 0.7417346160817666\n",
      "150: reward: 0.0, discounted reward: 0.764606222281567\n",
      "151: reward: 0.0, discounted reward: 0.7881851977452787\n",
      "152: reward: 0.0, discounted reward: 0.8124934198728165\n",
      "153: reward: 0.0, discounted reward: 0.8375534426847112\n",
      "154: reward: 0.0, discounted reward: 0.8633885177485202\n",
      "155: reward: 0.0, discounted reward: 0.890022615752447\n",
      "156: reward: 0.0, discounted reward: 0.917480448746186\n",
      "157: reward: 0.0, discounted reward: 0.9457874930696282\n",
      "158: reward: 0.0, discounted reward: 0.9749700129907025\n",
      "159: reward: 0.0, discounted reward: 1.0050550850742843\n",
      "160: reward: 15.0, discounted reward: 1.0360706233047812\n",
      "161: reward: 0.0, discounted reward: 0.5057762603239269\n",
      "162: reward: 0.0, discounted reward: 0.5213501854178145\n",
      "163: reward: 0.0, discounted reward: 0.5374057782981112\n",
      "164: reward: 0.0, discounted reward: 0.5539579359066645\n",
      "165: reward: 0.0, discounted reward: 0.5710220159154823\n",
      "166: reward: 0.0, discounted reward: 0.5886138509761192\n",
      "167: reward: 0.0, discounted reward: 0.6067497634097655\n",
      "168: reward: 0.0, discounted reward: 0.625446580351669\n",
      "169: reward: 0.0, discounted reward: 0.6447216493639406\n",
      "170: reward: 0.0, discounted reward: 0.6645928545312307\n",
      "171: reward: 0.0, discounted reward: 0.6850786330542104\n",
      "172: reward: 0.0, discounted reward: 0.7061979923562512\n",
      "173: reward: 0.0, discounted reward: 0.7279705277191799\n",
      "174: reward: 0.0, discounted reward: 0.7504164404644672\n",
      "175: reward: 0.0, discounted reward: 0.7735565566967223\n",
      "176: reward: 0.0, discounted reward: 0.7974123466268821\n",
      "177: reward: 20.0, discounted reward: 0.8220059444930262\n",
      "178: reward: 0.0, discounted reward: 0.0976679762149266\n",
      "179: reward: 0.0, discounted reward: 0.10061999561472157\n",
      "180: reward: 0.0, discounted reward: 0.10366331458358236\n",
      "181: reward: 0.0, discounted reward: 0.10680075681952132\n",
      "182: reward: 0.0, discounted reward: 0.11003523335141716\n",
      "183: reward: 0.0, discounted reward: 0.11336974523996957\n",
      "184: reward: 0.0, discounted reward: 0.11680738636218856\n",
      "185: reward: 0.0, discounted reward: 0.12035134628200195\n",
      "186: reward: 0.0, discounted reward: 0.1240049132096446\n",
      "187: reward: 0.0, discounted reward: 0.1277714770525752\n",
      "188: reward: 0.0, discounted reward: 0.13165453256075105\n",
      "189: reward: 0.0, discounted reward: 0.13565768256917976\n",
      "190: reward: 0.0, discounted reward: 0.13978464134075574\n",
      "191: reward: 0.0, discounted reward: 0.14403923801248358\n",
      "192: reward: 0.0, discounted reward: 0.14842542014828547\n",
      "193: reward: 0.0, discounted reward: 0.1529472574016895\n",
      "194: reward: 0.0, discounted reward: 0.1576089452917967\n",
      "195: reward: 0.0, discounted reward: 0.16241480909603095\n",
      "196: reward: 0.0, discounted reward: 0.16736930786328275\n",
      "197: reward: 0.0, discounted reward: 0.17247703855117122\n",
      "198: reward: 0.0, discounted reward: 0.1777427402912624\n",
      "199: reward: 0.0, discounted reward: 0.1831712987862018\n",
      "200: reward: 0.0, discounted reward: 0.18876775084284036\n",
      "201: reward: 0.0, discounted reward: 0.19453728904556053\n",
      "202: reward: 0.0, discounted reward: 0.20048526657413798\n",
      "203: reward: 0.0, discounted reward: 0.20661720217060958\n",
      "204: reward: 0.0, discounted reward: 0.2129387852597556\n",
      "205: reward: 0.0, discounted reward: 0.21945588122794735\n",
      "206: reward: 0.0, discounted reward: 0.22617453686525843\n",
      "207: reward: 0.0, discounted reward: 0.2331009859758884\n",
      "208: reward: 0.0, discounted reward: 0.24024165516210488\n",
      "209: reward: 0.0, discounted reward: 0.24760316978707034\n",
      "210: reward: 0.0, discounted reward: 0.25519236012208624\n",
      "211: reward: 0.0, discounted reward: 0.2630162676839583\n",
      "212: reward: 0.0, discounted reward: 0.27108215176836253\n",
      "213: reward: 0.0, discounted reward: 0.2793974961852741\n",
      "214: reward: 0.0, discounted reward: 0.2879700162027087\n",
      "215: reward: 0.0, discounted reward: 0.2968076657052186\n",
      "216: reward: 0.0, discounted reward: 0.3059186445737855\n",
      "217: reward: 0.0, discounted reward: 0.31531140629395754\n",
      "218: reward: 0.0, discounted reward: 0.3249946657992896\n",
      "219: reward: 0.0, discounted reward: 0.3349774075573638\n",
      "220: reward: 0.0, discounted reward: 0.345268893905894\n",
      "221: reward: 0.0, discounted reward: 0.3558786736466467\n",
      "222: reward: 0.0, discounted reward: 0.36681659090515467\n",
      "223: reward: 0.0, discounted reward: 0.37809279426444126\n",
      "224: reward: 0.0, discounted reward: 0.38971774618123156\n",
      "225: reward: 0.0, discounted reward: 0.40170223269338645\n",
      "226: reward: 0.0, discounted reward: 0.41405737342756677\n",
      "227: reward: 0.0, discounted reward: 0.4267946319164125\n",
      "228: reward: 0.0, discounted reward: 0.43992582623481014\n",
      "229: reward: 0.0, discounted reward: 0.45346313996511695\n",
      "230: reward: 0.0, discounted reward: 0.46741913350151576\n",
      "231: reward: 0.0, discounted reward: 0.4818067557039888\n",
      "232: reward: 0.0, discounted reward: 0.49663935591272385\n",
      "233: reward: 0.0, discounted reward: 0.5119306963341002\n",
      "234: reward: 0.0, discounted reward: 0.5276949648097459\n",
      "235: reward: 0.0, discounted reward: 0.5439467879805145\n",
      "236: reward: 0.0, discounted reward: 0.5607012448575958\n",
      "237: reward: 0.0, discounted reward: 0.5779738808133497\n",
      "238: reward: 0.0, discounted reward: 0.5957807220048484\n",
      "239: reward: 0.0, discounted reward: 0.614138290243507\n",
      "240: reward: 0.0, discounted reward: 0.6330636183245983\n",
      "241: reward: 0.0, discounted reward: 0.652574265830878\n",
      "242: reward: 0.0, discounted reward: 0.6726883354249809\n",
      "243: reward: 0.0, discounted reward: 0.6934244896457052\n",
      "244: reward: 0.0, discounted reward: 0.7148019682237718\n",
      "245: reward: 0.0, discounted reward: 0.7368406059331187\n",
      "246: reward: 0.0, discounted reward: 0.759560850994301\n",
      "247: reward: 0.0, discounted reward: 0.7829837840470664\n",
      "248: reward: 0.0, discounted reward: 0.8071311377097111\n",
      "249: reward: 0.0, discounted reward: 0.8320253167433654\n",
      "250: reward: 0.0, discounted reward: 0.8576894188399162\n",
      "251: reward: 0.0, discounted reward: 0.8841472560528553\n",
      "252: reward: 0.0, discounted reward: 0.9114233768909367\n",
      "253: reward: 0.0, discounted reward: 0.9395430890951444\n",
      "254: reward: 0.0, discounted reward: 0.9685324831201008\n",
      "255: reward: 0.0, discounted reward: 0.9984184563417052\n",
      "256: reward: 0.0, discounted reward: 1.0292287380134624\n",
      "257: reward: 0.0, discounted reward: 1.0609919149946554\n",
      "258: reward: 30.0, discounted reward: 1.0937374582742359\n",
      "259: reward: 0.0, discounted reward: 0.002957460991482539\n",
      "260: reward: 0.0, discounted reward: 0.002980289198799859\n",
      "261: reward: 0.0, discounted reward: 0.003003823433147611\n",
      "262: reward: 0.0, discounted reward: 0.0030280855304133346\n",
      "263: reward: 0.0, discounted reward: 0.0030530980018212976\n",
      "264: reward: 0.0, discounted reward: 0.0030788840548191974\n",
      "265: reward: 0.0, discounted reward: 0.0031054676146108464\n",
      "266: reward: 0.0, discounted reward: 0.0031328733463548146\n",
      "267: reward: 0.0, discounted reward: 0.0031611266780496275\n",
      "268: reward: 0.0, discounted reward: 0.0031902538241267535\n",
      "269: reward: 0.0, discounted reward: 0.0032202818097732755\n",
      "270: reward: 0.0, discounted reward: 0.003251238496006804\n",
      "271: reward: 0.0, discounted reward: 0.0032831526055259043\n",
      "272: reward: 0.0, discounted reward: 0.003316053749360029\n",
      "273: reward: 0.0, discounted reward: 0.0033499724543436626\n",
      "274: reward: 0.0, discounted reward: 0.003384940191440192\n",
      "275: reward: 0.0, discounted reward: 0.003420989404941769\n",
      "276: reward: 0.0, discounted reward: 0.003458153542572261\n",
      "277: reward: 0.0, discounted reward: 0.0034964670865212212\n",
      "278: reward: 0.0, discounted reward: 0.0035359655854376756\n",
      "279: reward: 0.0, discounted reward: 0.0035766856874134014\n",
      "280: reward: 0.0, discounted reward: 0.003618665173986314\n",
      "281: reward: 0.0, discounted reward: 0.0036619429951955033\n",
      "282: reward: 0.0, discounted reward: 0.00370655930572044\n",
      "283: reward: 0.0, discounted reward: 0.003752555502137901\n",
      "284: reward: 0.0, discounted reward: 0.0037999742613311596\n",
      "285: reward: 0.0, discounted reward: 0.003848859580087096\n",
      "286: reward: 0.0, discounted reward: 0.0038992568159179584\n",
      "287: reward: 0.0, discounted reward: 0.003951212729145652\n",
      "288: reward: 0.0, discounted reward: 0.004004775526287604\n",
      "289: reward: 0.0, discounted reward: 0.004059994904784462\n",
      "290: reward: 0.0, discounted reward: 0.004116922099111119\n",
      "291: reward: 0.0, discounted reward: 0.004175609928313858\n",
      "292: reward: 0.0, discounted reward: 0.004236112845017714\n",
      "293: reward: 0.0, discounted reward: 0.004298486985949523\n",
      "294: reward: 0.0, discounted reward: 0.004362790224023554\n",
      "295: reward: 0.0, discounted reward: 0.004429082222038018\n",
      "296: reward: 0.0, discounted reward: 0.0044974244880323115\n",
      "297: reward: 0.0, discounted reward: 0.004567880432356326\n",
      "298: reward: 0.0, discounted reward: 0.004640515426504794\n",
      "299: reward: 0.0, discounted reward: 0.004715396863771254\n",
      "300: reward: 0.0, discounted reward: 0.004792594221777915\n",
      "301: reward: 0.0, discounted reward: 0.004872179126939422\n",
      "302: reward: 0.0, discounted reward: 0.004954225420920356\n",
      "303: reward: 0.0, discounted reward: 0.005038809229148123\n",
      "304: reward: 0.0, discounted reward: 0.005126009031444792\n",
      "305: reward: 0.0, discounted reward: 0.005215905734843418\n",
      "306: reward: 0.0, discounted reward: 0.005308582748656435\n",
      "307: reward: 0.0, discounted reward: 0.005404126061865732\n",
      "308: reward: 0.0, discounted reward: 0.005502624322906242\n",
      "309: reward: 0.0, discounted reward: 0.005604168921917079\n",
      "310: reward: 0.0, discounted reward: 0.005708854075536497\n",
      "311: reward: 0.0, discounted reward: 0.005816776914319403\n",
      "312: reward: 0.0, discounted reward: 0.005928037572858482\n",
      "313: reward: 0.0, discounted reward: 0.006042739282692582\n",
      "314: reward: 0.0, discounted reward: 0.006160988468088563\n",
      "315: reward: 0.0, discounted reward: 0.006282894844785451\n",
      "316: reward: 0.0, discounted reward: 0.006408571521792551\n",
      "317: reward: 0.0, discounted reward: 0.0065381351063359535\n",
      "318: reward: 0.0, discounted reward: 0.006671705812050801\n",
      "319: reward: 0.0, discounted reward: 0.0068094075705197165\n",
      "320: reward: 0.0, discounted reward: 0.006951368146260866\n",
      "321: reward: 0.0, discounted reward: 0.007097719255272361\n",
      "322: reward: 0.0, discounted reward: 0.007248596687242974\n",
      "323: reward: 0.0, discounted reward: 0.007404140431542575\n",
      "324: reward: 0.0, discounted reward: 0.007564494807109174\n",
      "325: reward: 0.0, discounted reward: 0.007729808596353091\n",
      "326: reward: 0.0, discounted reward: 0.00790023518320249\n",
      "327: reward: 0.0, discounted reward: 0.008075932695418364\n",
      "328: reward: 0.0, discounted reward: 0.008257064151311019\n",
      "329: reward: 0.0, discounted reward: 0.008443797610994168\n",
      "330: reward: 0.0, discounted reward: 0.008636306332317002\n",
      "331: reward: 0.0, discounted reward: 0.008834768931618891\n",
      "332: reward: 0.0, discounted reward: 0.009039369549455892\n",
      "333: reward: 0.0, discounted reward: 0.0092502980214528\n",
      "334: reward: 0.0, discounted reward: 0.009467750054439302\n",
      "335: reward: 0.0, discounted reward: 0.009691927408033636\n",
      "336: reward: 0.0, discounted reward: 0.009923038081842226\n",
      "337: reward: 0.0, discounted reward: 0.010161296508449021\n",
      "338: reward: 0.0, discounted reward: 0.01040692375237355\n",
      "339: reward: 0.0, discounted reward: 0.010660147715182346\n",
      "340: reward: 0.0, discounted reward: 0.01092120334694399\n",
      "341: reward: 0.0, discounted reward: 0.011190332864224035\n",
      "342: reward: 0.0, discounted reward: 0.01146778597482202\n",
      "343: reward: 0.0, discounted reward: 0.011753820109459119\n",
      "344: reward: 0.0, discounted reward: 0.012048700660631382\n",
      "345: reward: 0.0, discounted reward: 0.012352701228850215\n",
      "346: reward: 0.0, discounted reward: 0.012666103876498492\n",
      "347: reward: 0.0, discounted reward: 0.012989199389537957\n",
      "348: reward: 0.0, discounted reward: 0.013322287547310598\n",
      "349: reward: 0.0, discounted reward: 0.013665677400684457\n",
      "350: reward: 0.0, discounted reward: 0.014019687558801836\n",
      "351: reward: 0.0, discounted reward: 0.014384646484696041\n",
      "352: reward: 0.0, discounted reward: 0.014760892800050891\n",
      "353: reward: 0.0, discounted reward: 0.015148775599385789\n",
      "354: reward: 0.0, discounted reward: 0.015548654773957848\n",
      "355: reward: 0.0, discounted reward: 0.015960901345681622\n",
      "356: reward: 0.0, discounted reward: 0.016385897811376237\n",
      "357: reward: 0.0, discounted reward: 0.01682403849765934\n",
      "358: reward: 0.0, discounted reward: 0.01727572992681718\n",
      "359: reward: 0.0, discounted reward: 0.017741391193990208\n",
      "360: reward: 0.0, discounted reward: 0.01822145435602426\n",
      "361: reward: 0.0, discounted reward: 0.018716364832348023\n",
      "362: reward: 0.0, discounted reward: 0.019226581818248815\n",
      "363: reward: 0.0, discounted reward: 0.01975257871093004\n",
      "364: reward: 0.0, discounted reward: 0.020294843548745736\n",
      "365: reward: 0.0, discounted reward: 0.020853879464019648\n",
      "366: reward: 0.0, discounted reward: 0.021430205149869045\n",
      "367: reward: 0.0, discounted reward: 0.022024355341466358\n",
      "368: reward: 0.0, discounted reward: 0.02263688131218524\n",
      "369: reward: 0.0, discounted reward: 0.023268351385091306\n",
      "370: reward: 0.0, discounted reward: 0.023919351460252196\n",
      "371: reward: 0.0, discounted reward: 0.024590485558356207\n",
      "372: reward: 0.0, discounted reward: 0.025282376381143848\n",
      "373: reward: 0.0, discounted reward: 0.02599566588917234\n",
      "374: reward: 0.0, discounted reward: 0.02673101589744914\n",
      "375: reward: 0.0, discounted reward: 0.027489108689487077\n",
      "376: reward: 0.0, discounted reward: 0.02827064765035093\n",
      "377: reward: 0.0, discounted reward: 0.029076357919282734\n",
      "378: reward: 0.0, discounted reward: 0.0299069870625114\n",
      "379: reward: 0.0, discounted reward: 0.03076330576687085\n",
      "380: reward: 0.0, discounted reward: 0.03164610855487028\n",
      "381: reward: 0.0, discounted reward: 0.032556214521880004\n",
      "382: reward: 0.0, discounted reward: 0.033494468096116836\n",
      "383: reward: 0.0, discounted reward: 0.034461739822134185\n",
      "384: reward: 0.0, discounted reward: 0.035458927168543826\n",
      "385: reward: 0.0, discounted reward: 0.03648695536071871\n",
      "386: reward: 0.0, discounted reward: 0.03754677823924952\n",
      "387: reward: 0.0, discounted reward: 0.038639379144951386\n",
      "388: reward: 0.0, discounted reward: 0.03976577183124197\n",
      "389: reward: 0.0, discounted reward: 0.040927001404737415\n",
      "390: reward: 0.0, discounted reward: 0.0421241452949389\n",
      "391: reward: 0.0, discounted reward: 0.04335831425390951\n",
      "392: reward: 0.0, discounted reward: 0.044630653386868904\n",
      "393: reward: 0.0, discounted reward: 0.0459423432146621\n",
      "394: reward: 0.0, discounted reward: 0.04729460076908806\n",
      "395: reward: 0.0, discounted reward: 0.04868868072210453\n",
      "396: reward: 0.0, discounted reward: 0.050125876549956545\n",
      "397: reward: 0.0, discounted reward: 0.051607521733309146\n",
      "398: reward: 0.0, discounted reward: 0.05313499099449739\n",
      "399: reward: 0.0, discounted reward: 0.054709701573041974\n",
      "400: reward: 0.0, discounted reward: 0.05633311454061372\n",
      "401: reward: 0.0, discounted reward: 0.05800673615666705\n",
      "402: reward: 0.0, discounted reward: 0.05973211926600038\n",
      "403: reward: 0.0, discounted reward: 0.061510864739539914\n",
      "404: reward: 0.0, discounted reward: 0.06334462295968375\n",
      "405: reward: 0.0, discounted reward: 0.06523509535158462\n",
      "406: reward: 0.0, discounted reward: 0.06718403596179168\n",
      "407: reward: 0.0, discounted reward: 0.0691932530857165\n",
      "408: reward: 0.0, discounted reward: 0.07126461094543282\n",
      "409: reward: 0.0, discounted reward: 0.07340003141936717\n",
      "410: reward: 0.0, discounted reward: 0.07560149582548505\n",
      "411: reward: 0.0, discounted reward: 0.07787104675962718\n",
      "412: reward: 0.0, discounted reward: 0.08021078999070155\n",
      "413: reward: 0.0, discounted reward: 0.08262289641448957\n",
      "414: reward: 0.0, discounted reward: 0.08510960406787926\n",
      "415: reward: 0.0, discounted reward: 0.08767322020539443\n",
      "416: reward: 0.0, discounted reward: 0.09031612343994615\n",
      "417: reward: 0.0, discounted reward: 0.09304076594979327\n",
      "418: reward: 0.0, discounted reward: 0.09584967575375937\n",
      "419: reward: 0.0, discounted reward: 0.09874545905681721\n",
      "420: reward: 0.0, discounted reward: 0.10173080266821705\n",
      "421: reward: 0.0, discounted reward: 0.10480847649440246\n",
      "422: reward: 0.0, discounted reward: 0.10798133610902658\n",
      "423: reward: 0.0, discounted reward: 0.1112523254024535\n",
      "424: reward: 0.0, discounted reward: 0.11462447931320291\n",
      "425: reward: 0.0, discounted reward: 0.11810092664387241\n",
      "426: reward: 0.0, discounted reward: 0.12168489296415023\n",
      "427: reward: 0.0, discounted reward: 0.12537970360361192\n",
      "428: reward: 0.0, discounted reward: 0.12918878673707757\n",
      "429: reward: 0.0, discounted reward: 0.1331156765653927\n",
      "430: reward: 0.0, discounted reward: 0.13716401659458352\n",
      "431: reward: 0.0, discounted reward: 0.14133756301642975\n",
      "432: reward: 0.0, discounted reward: 0.1456401881935908\n",
      "433: reward: 0.0, discounted reward: 0.1500758842525197\n",
      "434: reward: 0.0, discounted reward: 0.15464876678749795\n",
      "435: reward: 0.0, discounted reward: 0.15936307867922814\n",
      "436: reward: 0.0, discounted reward: 0.1642231940315273\n",
      "437: reward: 0.0, discounted reward: 0.16923362222977384\n",
      "438: reward: 0.0, discounted reward: 0.17439901212487335\n",
      "439: reward: 0.0, discounted reward: 0.17972415634662545\n",
      "440: reward: 0.0, discounted reward: 0.1852139957504936\n",
      "441: reward: 0.0, discounted reward: 0.19087362400190402\n",
      "442: reward: 0.0, discounted reward: 0.19670829230232717\n",
      "443: reward: 0.0, discounted reward: 0.20272341426152632\n",
      "444: reward: 0.0, discounted reward: 0.2089245709204945\n",
      "445: reward: 0.0, discounted reward: 0.21531751592974002\n",
      "446: reward: 0.0, discounted reward: 0.2219081808877251\n",
      "447: reward: 0.0, discounted reward: 0.22870268084441076\n",
      "448: reward: 0.0, discounted reward: 0.2357073199750145\n",
      "449: reward: 0.0, discounted reward: 0.24292859742924522\n",
      "450: reward: 0.0, discounted reward: 0.2503732133614418\n",
      "451: reward: 0.0, discounted reward: 0.2580480751472115\n",
      "452: reward: 0.0, discounted reward: 0.2659603037923349\n",
      "453: reward: 0.0, discounted reward: 0.27411724053988473\n",
      "454: reward: 0.0, discounted reward: 0.2825264536816887\n",
      "455: reward: 0.0, discounted reward: 0.29119574558045574\n",
      "456: reward: 0.0, discounted reward: 0.3001331599090815\n",
      "457: reward: 0.0, discounted reward: 0.3093469891138504\n",
      "458: reward: 0.0, discounted reward: 0.31884578210845743\n",
      "459: reward: 0.0, discounted reward: 0.32863835220599047\n",
      "460: reward: 0.0, discounted reward: 0.3387337852962307\n",
      "461: reward: 0.0, discounted reward: 0.34914144827585986\n",
      "462: reward: 0.0, discounted reward: 0.35987099773939507\n",
      "463: reward: 0.0, discounted reward: 0.3709323889389159\n",
      "464: reward: 0.0, discounted reward: 0.3823358850208961\n",
      "465: reward: 0.0, discounted reward: 0.39409206654871076\n",
      "466: reward: 0.0, discounted reward: 0.4062118413196537\n",
      "467: reward: 0.0, discounted reward: 0.41870645448557425\n",
      "468: reward: 0.0, discounted reward: 0.43158749898652327\n",
      "469: reward: 0.0, discounted reward: 0.44486692630708924\n",
      "470: reward: 0.0, discounted reward: 0.45855705756540477\n",
      "471: reward: 5.0, discounted reward: 0.4726705949451115\n",
      "472: reward: 0.0, discounted reward: 0.2997975852602986\n",
      "473: reward: 0.0, discounted reward: 0.3090010358676824\n",
      "474: reward: 0.0, discounted reward: 0.31848912927735645\n",
      "475: reward: 0.0, discounted reward: 0.32827066887495854\n",
      "476: reward: 0.0, discounted reward: 0.33835473031578545\n",
      "477: reward: 0.0, discounted reward: 0.3487506699455039\n",
      "478: reward: 0.0, discounted reward: 0.35946813348129614\n",
      "479: reward: 0.0, discounted reward: 0.37051706496149434\n",
      "480: reward: 0.0, discounted reward: 0.3819077159720079\n",
      "481: reward: 0.0, discounted reward: 0.3936506551581044\n",
      "482: reward: 10.0, discounted reward: 0.4057567780303688\n",
      "483: reward: 0.0, discounted reward: 0.04339122063275437\n",
      "484: reward: 0.0, discounted reward: 0.04466457748877082\n",
      "485: reward: 0.0, discounted reward: 0.0459773165155919\n",
      "486: reward: 0.0, discounted reward: 0.04733065571850023\n",
      "487: reward: 0.0, discounted reward: 0.04872585077304491\n",
      "488: reward: 0.0, discounted reward: 0.05016419619010127\n",
      "489: reward: 0.0, discounted reward: 0.0516470265169635\n",
      "490: reward: 0.0, discounted reward: 0.05317571757558435\n",
      "491: reward: 0.0, discounted reward: 0.05475168773911101\n",
      "492: reward: 0.0, discounted reward: 0.056376399247901374\n",
      "493: reward: 0.0, discounted reward: 0.05805135956624197\n",
      "494: reward: 15.0, discounted reward: 0.05977812278102607\n",
      "495: reward: 0.0, discounted reward: -0.5007108536180888\n",
      "496: reward: 0.0, discounted reward: -0.5162653959657066\n",
      "497: reward: 0.0, discounted reward: -0.532301006633354\n",
      "498: reward: 0.0, discounted reward: -0.5488325640226811\n",
      "499: reward: 0.0, discounted reward: -0.5658754066920905\n",
      "500: reward: 0.0, discounted reward: -0.5834453475883888\n",
      "501: reward: 0.0, discounted reward: -0.6015586887185933\n",
      "502: reward: 0.0, discounted reward: -0.6202322362755051\n",
      "503: reward: 0.0, discounted reward: -0.6394833162310843\n",
      "504: reward: 0.0, discounted reward: -0.6593297904120938\n",
      "505: reward: 0.0, discounted reward: -0.6797900730729284\n",
      "506: reward: 0.0, discounted reward: -0.7008831479810053\n",
      "507: reward: 0.0, discounted reward: -0.7226285860305691\n",
      "508: reward: 0.0, discounted reward: -0.7450465634012534\n",
      "509: reward: 0.0, discounted reward: -0.7681578802782476\n",
      "510: reward: 20.0, discounted reward: -0.7919839801514373\n",
      "511: reward: 0.0, discounted reward: -1.5662391625937988\n",
      "512: reward: 0.0, discounted reward: -1.6147481887241706\n",
      "513: reward: 0.0, discounted reward: -1.6647574940132135\n",
      "514: reward: 0.0, discounted reward: -1.7163134788472785\n",
      "515: reward: 0.0, discounted reward: -1.7694639786762116\n",
      "516: reward: 0.0, discounted reward: -1.8242583083967614\n",
      "517: reward: 0.0, discounted reward: -1.880747308108668\n",
      "518: reward: 0.0, discounted reward: -1.9389833902858915\n",
      "519: reward: 0.0, discounted reward: -1.9990205884067405\n",
      "520: reward: 0.0, discounted reward: -2.060914607088028\n",
      "521: reward: 0.0, discounted reward: -2.124722873769768\n",
      "522: reward: 0.0, discounted reward: -2.1905045919983657\n",
      "523: reward: 0.0, discounted reward: -2.258320796357745\n",
      "524: reward: 0.0, discounted reward: -2.3282344090993727\n",
      "525: reward: 0.0, discounted reward: -2.4003102985237312\n",
      "526: reward: 0.0, discounted reward: -2.4746153391674\n",
      "527: reward: 0.0, discounted reward: -2.5512184738515944\n",
      "528: reward: 0.0, discounted reward: -2.6301907776497333\n",
      "529: reward: 0.0, discounted reward: -2.711605523833381\n",
      "530: reward: 0.0, discounted reward: -2.795538251857761\n",
      "531: reward: 0.0, discounted reward: -2.882066837449905\n",
      "532: reward: 0.0, discounted reward: -2.9712715648644865\n",
      "533: reward: 0.0, discounted reward: -3.063235201374364\n",
      "534: reward: 0.0, discounted reward: -3.1580430740649597\n",
      "535: reward: 0.0, discounted reward: -3.255783149003718\n",
      "536: reward: 0.0, discounted reward: -3.356546112858108\n",
      "537: reward: 0.0, discounted reward: -3.4604254570378914\n",
      "538: reward: 0.0, discounted reward: -3.56751756443973\n",
      "539: reward: 0.0, discounted reward: -3.6779217988746153\n",
      "540: reward: 0.0, discounted reward: -3.7917405972610947\n",
      "541: reward: 0.0, discounted reward: -3.9090795646698364\n",
      "542: reward: 0.0, discounted reward: -4.030047572307715\n",
      "543: reward: 0.0, discounted reward: -4.154756858532331\n",
      "544: reward: 0.0, discounted reward: -4.283323132990699\n",
      "545: reward: -121.66666666666667, discounted reward: -4.415865683978707\n",
      "546: reward: 0.0, discounted reward: 0.00812001758011258\n",
      "547: reward: 0.0, discounted reward: 0.008302512486047324\n",
      "548: reward: 0.0, discounted reward: 0.008490651564330565\n",
      "549: reward: 0.0, discounted reward: 0.0086846093769937\n",
      "550: reward: 0.0, discounted reward: 0.00888456588489384\n",
      "551: reward: 0.0, discounted reward: 0.009090706614687796\n",
      "552: reward: 0.0, discounted reward: 0.009303222830970227\n",
      "553: reward: 0.0, discounted reward: 0.009522311713735621\n",
      "554: reward: 0.0, discounted reward: 0.009748176541328809\n",
      "555: reward: 0.0, discounted reward: 0.009981026879053745\n",
      "556: reward: 0.0, discounted reward: 0.010221078773615536\n",
      "557: reward: 0.0, discounted reward: 0.010468554953576143\n",
      "558: reward: 0.0, discounted reward: 0.010723685036009761\n",
      "559: reward: 0.0, discounted reward: 0.010986705739549572\n",
      "560: reward: 0.0, discounted reward: 0.011257861104023603\n",
      "561: reward: 0.0, discounted reward: 0.011537402716883432\n",
      "562: reward: 0.0, discounted reward: 0.011825589946635831\n",
      "563: reward: 0.0, discounted reward: 0.012122690183493972\n",
      "564: reward: 0.0, discounted reward: 0.012428979087471441\n",
      "565: reward: 0.0, discounted reward: 0.012744740844149243\n",
      "566: reward: 0.0, discounted reward: 0.01307026842835316\n",
      "567: reward: 0.0, discounted reward: 0.013405863875986065\n",
      "568: reward: 0.0, discounted reward: 0.013751838564267411\n",
      "569: reward: 0.0, discounted reward: 0.014108513500639934\n",
      "570: reward: 0.0, discounted reward: 0.014476219620611606\n",
      "571: reward: 0.0, discounted reward: 0.014855298094809207\n",
      "572: reward: 0.0, discounted reward: 0.015246100645528383\n",
      "573: reward: 0.0, discounted reward: 0.015648989873073924\n",
      "574: reward: 0.0, discounted reward: 0.016064339592193038\n",
      "575: reward: 0.0, discounted reward: 0.016492535178913775\n",
      "576: reward: 0.0, discounted reward: 0.016933973928110413\n",
      "577: reward: 0.0, discounted reward: 0.017389065422127564\n",
      "578: reward: 0.0, discounted reward: 0.01785823191080504\n",
      "579: reward: 0.0, discounted reward: 0.018341908703256048\n",
      "580: reward: 0.0, discounted reward: 0.01884054457176224\n",
      "581: reward: 0.0, discounted reward: 0.019354602168160373\n",
      "582: reward: 0.0, discounted reward: 0.019884558453106906\n",
      "583: reward: 0.0, discounted reward: 0.020430905138618793\n",
      "584: reward: 0.0, discounted reward: 0.02099414914430115\n",
      "585: reward: 0.0, discounted reward: 0.021574813067685026\n",
      "586: reward: 0.0, discounted reward: 0.022173435669111705\n",
      "587: reward: 0.0, discounted reward: 0.022790572371613433\n",
      "588: reward: 0.0, discounted reward: 0.02342679577625439\n",
      "589: reward: 0.0, discounted reward: 0.02408269619341001\n",
      "590: reward: 0.0, discounted reward: 0.024758882190477664\n",
      "591: reward: 0.0, discounted reward: 0.025455981156526794\n",
      "592: reward: 0.0, discounted reward: 0.02617463988441249\n",
      "593: reward: 0.0, discounted reward: 0.02691552517089259\n",
      "594: reward: 0.0, discounted reward: 0.027679324435305068\n",
      "595: reward: 0.0, discounted reward: 0.02846674635737979\n",
      "596: reward: 0.0, discounted reward: 0.029278521534776406\n",
      "597: reward: 0.0, discounted reward: 0.030115403160958485\n",
      "598: reward: 0.0, discounted reward: 0.030978167724032793\n",
      "599: reward: 0.0, discounted reward: 0.03186761572720218\n",
      "600: reward: 0.0, discounted reward: 0.03278457243150052\n",
      "601: reward: 0.0, discounted reward: 0.03372988862149882\n",
      "602: reward: 0.0, discounted reward: 0.03470444139469293\n",
      "603: reward: 0.0, discounted reward: 0.03570913497530541\n",
      "604: reward: 0.0, discounted reward: 0.03674490155325643\n",
      "605: reward: 0.0, discounted reward: 0.03781270214908222\n",
      "606: reward: 0.0, discounted reward: 0.03891352750560366\n",
      "607: reward: 0.0, discounted reward: 0.04004839900717215\n",
      "608: reward: 0.0, discounted reward: 0.04121836962734585\n",
      "609: reward: 0.0, discounted reward: 0.042424524905875435\n",
      "610: reward: 0.0, discounted reward: 0.04366798395590594\n",
      "611: reward: 0.0, discounted reward: 0.04494990050232914\n",
      "612: reward: 0.0, discounted reward: 0.046271463952249965\n",
      "613: reward: 0.0, discounted reward: 0.04763390049856009\n",
      "614: reward: 0.0, discounted reward: 0.0490384742576427\n",
      "615: reward: 0.0, discounted reward: 0.050486488442263944\n",
      "616: reward: 0.0, discounted reward: 0.051979286570739455\n",
      "617: reward: 0.0, discounted reward: 0.05351825371349771\n",
      "618: reward: 0.0, discounted reward: 0.055104817778196945\n",
      "619: reward: 0.0, discounted reward: 0.05674045083458791\n",
      "620: reward: 0.0, discounted reward: 0.05842667048035179\n",
      "621: reward: 0.0, discounted reward: 0.06016504124918053\n",
      "622: reward: 0.0, discounted reward: 0.061957176062406034\n",
      "623: reward: 0.0, discounted reward: 0.06380473772552511\n",
      "624: reward: 0.0, discounted reward: 0.0657094404710087\n",
      "625: reward: 0.0, discounted reward: 0.06767305154882683\n",
      "626: reward: 0.0, discounted reward: 0.06969739286616511\n",
      "627: reward: 0.0, discounted reward: 0.07178434267785405\n",
      "628: reward: 0.0, discounted reward: 0.07393583732907978\n",
      "629: reward: 0.0, discounted reward: 0.07615387305199289\n",
      "630: reward: 0.0, discounted reward: 0.0784405078178827\n",
      "631: reward: 0.0, discounted reward: 0.08079786324663508\n",
      "632: reward: 0.0, discounted reward: 0.08322812657524578\n",
      "633: reward: 0.0, discounted reward: 0.08573355268721558\n",
      "634: reward: 0.0, discounted reward: 0.0883164662047102\n",
      "635: reward: 0.0, discounted reward: 0.09097926364542631\n",
      "636: reward: 0.0, discounted reward: 0.09372441564616459\n",
      "637: reward: 0.0, discounted reward: 0.09655446925517311\n",
      "638: reward: 0.0, discounted reward: 0.09947205029538807\n",
      "639: reward: 0.0, discounted reward: 0.10247986580076432\n",
      "640: reward: 0.0, discounted reward: 0.10558070652795634\n",
      "641: reward: 0.0, discounted reward: 0.10877744954568007\n",
      "642: reward: 0.0, discounted reward: 0.11207306090415814\n",
      "643: reward: 0.0, discounted reward: 0.11547059838712521\n",
      "644: reward: 0.0, discounted reward: 0.11897321434894695\n",
      "645: reward: 0.0, discounted reward: 0.12258415863948482\n",
      "646: reward: 0.0, discounted reward: 0.12630678161942077\n",
      "647: reward: 0.0, discounted reward: 0.1301445372688393\n",
      "648: reward: 0.0, discounted reward: 0.13410098639195114\n",
      "649: reward: 0.0, discounted reward: 0.13817979992093243\n",
      "650: reward: 0.0, discounted reward: 0.1423847623219441\n",
      "651: reward: 0.0, discounted reward: 0.14671977510649217\n",
      "652: reward: 0.0, discounted reward: 0.1511888604513871\n",
      "653: reward: 0.0, discounted reward: 0.15579616493066026\n",
      "654: reward: 0.0, discounted reward: 0.1605459633629006\n",
      "655: reward: 0.0, discounted reward: 0.16544266277758138\n",
      "656: reward: 0.0, discounted reward: 0.1704908065040564\n",
      "657: reward: 0.0, discounted reward: 0.17569507838702034\n",
      "658: reward: 0.0, discounted reward: 0.181060307132344\n",
      "659: reward: 0.0, discounted reward: 0.18659147078731683\n",
      "660: reward: 0.0, discounted reward: 0.19229370135945378\n",
      "661: reward: 0.0, discounted reward: 0.19817228957815167\n",
      "662: reward: 0.0, discounted reward: 0.2042326898036134\n",
      "663: reward: 0.0, discounted reward: 0.21048052508759454\n",
      "664: reward: 0.0, discounted reward: 0.2169215923906679\n",
      "665: reward: 0.0, discounted reward: 0.22356186796084665\n",
      "666: reward: 0.0, discounted reward: 0.23040751287855665\n",
      "667: reward: 0.0, discounted reward: 0.23746487877310307\n",
      "668: reward: 0.0, discounted reward: 0.24474051371593444\n",
      "669: reward: 0.0, discounted reward: 0.25224116829617294\n",
      "670: reward: 0.0, discounted reward: 0.25997380188404773\n",
      "671: reward: 0.0, discounted reward: 0.26794558908804234\n",
      "672: reward: 0.0, discounted reward: 0.2761639264117481\n",
      "673: reward: 0.0, discounted reward: 0.2846364391165994\n",
      "674: reward: 0.0, discounted reward: 0.2933709882968585\n",
      "675: reward: 0.0, discounted reward: 0.30237567817341426\n",
      "676: reward: 0.0, discounted reward: 0.3116588636131625\n",
      "677: reward: 0.0, discounted reward: 0.32122915788094414\n",
      "678: reward: 0.0, discounted reward: 0.3310954406312345\n",
      "679: reward: 0.0, discounted reward: 0.3412668661469978\n",
      "680: reward: 0.0, discounted reward: 0.3517528718333517\n",
      "681: reward: 0.0, discounted reward: 0.36256318697392276\n",
      "682: reward: 0.0, discounted reward: 0.3737078417580166\n",
      "683: reward: 0.0, discounted reward: 0.38519717658697933\n",
      "684: reward: 0.0, discounted reward: 0.3970418516683842\n",
      "685: reward: 0.0, discounted reward: 0.4092528569069459\n",
      "686: reward: 0.0, discounted reward: 0.42184152210133946\n",
      "687: reward: 0.0, discounted reward: 0.43481952745638436\n",
      "688: reward: 0.0, discounted reward: 0.4481989144203481\n",
      "689: reward: 0.0, discounted reward: 0.46199209685742415\n",
      "690: reward: 0.0, discounted reward: 0.47621187256575\n",
      "691: reward: 0.0, discounted reward: 0.49087143515165294\n",
      "692: reward: 0.0, discounted reward: 0.5059843862711405\n",
      "693: reward: 0.0, discounted reward: 0.5215647482499937\n",
      "694: reward: 0.0, discounted reward: 0.5376269770941722\n",
      "695: reward: 0.0, discounted reward: 0.5541859759026037\n",
      "696: reward: 0.0, discounted reward: 0.571257108694801\n",
      "697: reward: 0.0, discounted reward: 0.5888562146661385\n",
      "698: reward: 0.0, discounted reward: 0.6069996228840123\n",
      "699: reward: 0.0, discounted reward: 0.6257041674385213\n",
      "700: reward: 0.0, discounted reward: 0.6449872030617264\n",
      "701: reward: 0.0, discounted reward: 0.6648666212299791\n",
      "702: reward: 0.0, discounted reward: 0.6853608667642602\n",
      "703: reward: 0.0, discounted reward: 0.7064889549439314\n",
      "704: reward: 0.0, discounted reward: 0.728270489149778\n",
      "705: reward: 0.0, discounted reward: 0.7507256790527127\n",
      "706: reward: 0.0, discounted reward: 0.7738753593650166\n",
      "707: reward: 0.0, discounted reward: 0.7977410091715155\n",
      "708: reward: 0.0, discounted reward: 0.8223447718586275\n",
      "709: reward: 0.0, discounted reward: 0.8477094756597741\n",
      "710: reward: 0.0, discounted reward: 0.8738586548362138\n",
      "711: reward: 0.0, discounted reward: 0.9008165715129559\n",
      "712: reward: 0.0, discounted reward: 0.9286082381900096\n",
      "713: reward: 0.0, discounted reward: 0.9572594409498587\n",
      "714: reward: 0.0, discounted reward: 0.986796763382693\n",
      "715: reward: 0.0, discounted reward: 1.017247611251594\n",
      "716: reward: 0.0, discounted reward: 1.0486402379205644\n",
      "717: reward: 0.0, discounted reward: 1.0810037705689874\n",
      "718: reward: 0.0, discounted reward: 1.114368237216846\n",
      "719: reward: 0.0, discounted reward: 1.1487645945857725\n",
      "720: reward: 0.0, discounted reward: 1.1842247568217794\n",
      "721: reward: 0.0, discounted reward: 1.2207816251063226\n",
      "722: reward: 0.0, discounted reward: 1.2584691181831709\n",
      "723: reward: 0.0, discounted reward: 1.2973222038294066\n",
      "724: reward: 10.0, discounted reward: 1.3373769312997525\n",
      "725: reward: 0.0, discounted reward: 1.00382436833315\n",
      "726: reward: 0.0, discounted reward: 1.0348018431592818\n",
      "727: reward: 0.0, discounted reward: 1.0667373842171495\n",
      "728: reward: 0.0, discounted reward: 1.0996606224211372\n",
      "729: reward: 0.0, discounted reward: 1.1336021051056602\n",
      "730: reward: 0.0, discounted reward: 1.168593324368055\n",
      "731: reward: 0.0, discounted reward: 1.20466674628805\n",
      "732: reward: 0.0, discounted reward: 1.2418558410509315\n",
      "733: reward: 0.0, discounted reward: 1.2801951140023555\n",
      "734: reward: 0.0, discounted reward: 1.3197201376636174\n",
      "735: reward: 0.0, discounted reward: 1.3604675847370835\n",
      "736: reward: 0.0, discounted reward: 1.402475262132409\n",
      "737: reward: 15.0, discounted reward: 1.4457821460451157\n",
      "738: reward: 0.0, discounted reward: 0.9281592734582924\n",
      "739: reward: 0.0, discounted reward: 0.956796590710975\n",
      "740: reward: 0.0, discounted reward: 0.9863195981879673\n",
      "741: reward: 0.0, discounted reward: 1.0167556883704336\n",
      "742: reward: 0.0, discounted reward: 1.0481331009296773\n",
      "743: reward: 0.0, discounted reward: 1.0804809489288976\n",
      "744: reward: 0.0, discounted reward: 1.1138292458353103\n",
      "745: reward: 0.0, discounted reward: 1.1482089333676944\n",
      "746: reward: 0.0, discounted reward: 1.1836519102052039\n",
      "747: reward: 0.0, discounted reward: 1.2201910615840794\n",
      "748: reward: 0.0, discounted reward: 1.2578602898097246\n",
      "749: reward: 0.0, discounted reward: 1.2966945457124517\n",
      "750: reward: 0.0, discounted reward: 1.3367298610760878\n",
      "751: reward: 20.0, discounted reward: 1.3780033820695268\n",
      "752: reward: 0.0, discounted reward: 0.6708612108298758\n",
      "753: reward: 0.0, discounted reward: 0.6915408560425043\n",
      "754: reward: 0.0, discounted reward: 0.7128600779111933\n",
      "755: reward: 0.0, discounted reward: 0.7348386571572646\n",
      "756: reward: 0.0, discounted reward: 0.7574969862769257\n",
      "757: reward: 0.0, discounted reward: 0.7808560884621434\n",
      "758: reward: 0.0, discounted reward: 0.8049376371066975\n",
      "759: reward: 0.0, discounted reward: 0.8297639759155164\n",
      "760: reward: 0.0, discounted reward: 0.8553581396359482\n",
      "761: reward: 0.0, discounted reward: 0.8817438754302077\n",
      "762: reward: 0.0, discounted reward: 0.9089456649088259\n",
      "763: reward: 0.0, discounted reward: 0.9369887468455457\n",
      "764: reward: 0.0, discounted reward: 0.9658991405947412\n",
      "765: reward: 0.0, discounted reward: 0.9957036702330873\n",
      "766: reward: 0.0, discounted reward: 1.0264299894478768\n",
      "767: reward: 0.0, discounted reward: 1.0581066071950826\n",
      "768: reward: 0.0, discounted reward: 1.090762914150965\n",
      "769: reward: 25.0, discounted reward: 1.1244292099817714\n",
      "770: reward: 0.0, discounted reward: 0.22202149035386481\n",
      "771: reward: 0.0, discounted reward: 0.22881949472702903\n",
      "772: reward: 0.0, discounted reward: 0.23582774665812614\n",
      "773: reward: 0.0, discounted reward: 0.2430527486489479\n",
      "774: reward: 0.0, discounted reward: 0.2505012043095889\n",
      "775: reward: 0.0, discounted reward: 0.258180024578291\n",
      "776: reward: 0.0, discounted reward: 0.2660963341336539\n",
      "777: reward: 0.0, discounted reward: 0.2742574780051621\n",
      "778: reward: 0.0, discounted reward: 0.2826710283881602\n",
      "779: reward: 0.0, discounted reward: 0.29134479166960164\n",
      "780: reward: 0.0, discounted reward: 0.30028681567108756\n",
      "781: reward: 0.0, discounted reward: 0.30950539711591846\n",
      "782: reward: 0.0, discounted reward: 0.31900908932708427\n",
      "783: reward: 0.0, discounted reward: 0.3288067101633377\n",
      "784: reward: 0.0, discounted reward: 0.3389073502007124\n",
      "785: reward: 5.0, discounted reward: 0.3493203811670781\n",
      "786: reward: 0.0, discounted reward: 0.1726324164169651\n",
      "787: reward: 0.0, discounted reward: 0.17790292365806026\n",
      "788: reward: 0.0, discounted reward: 0.18333643627774598\n",
      "789: reward: 0.0, discounted reward: 0.18893799567948383\n",
      "790: reward: 0.0, discounted reward: 0.19471279918643006\n",
      "791: reward: 0.0, discounted reward: 0.20066620486369421\n",
      "792: reward: 0.0, discounted reward: 0.20680373648973974\n",
      "793: reward: 0.0, discounted reward: 0.21313108868153924\n",
      "794: reward: 0.0, discounted reward: 0.21965413217823973\n",
      "795: reward: 0.0, discounted reward: 0.22637891928824028\n",
      "796: reward: 0.0, discounted reward: 0.2333116895047357\n",
      "797: reward: 0.0, discounted reward: 0.24045887529493712\n",
      "798: reward: 0.0, discounted reward: 0.24782710806834066\n",
      "799: reward: 0.0, discounted reward: 0.2554232243295814\n",
      "800: reward: 0.0, discounted reward: 0.26325427202158225\n",
      "801: reward: 0.0, discounted reward: 0.271327517064882\n",
      "802: reward: 0.0, discounted reward: 0.2796504500992117\n",
      "803: reward: 0.0, discounted reward: 0.2882307934335722\n",
      "804: reward: 0.0, discounted reward: 0.29707650821126347\n",
      "805: reward: 0.0, discounted reward: 0.3061958017965122\n",
      "806: reward: 0.0, discounted reward: 0.3155971353895521\n",
      "807: reward: 0.0, discounted reward: 0.3252892318772221\n",
      "808: reward: 0.0, discounted reward: 0.3352810839263664\n",
      "809: reward: 0.0, discounted reward: 0.3455819623275461\n",
      "810: reward: 0.0, discounted reward: 0.35620142459680354\n",
      "811: reward: 0.0, discounted reward: 0.3671493238434607\n",
      "812: reward: 0.0, discounted reward: 0.3784358179121794\n",
      "813: reward: 0.0, discounted reward: 0.3900713788077657\n",
      "814: reward: 0.0, discounted reward: 0.402066802411463\n",
      "815: reward: 0.0, discounted reward: 0.4144332184977488\n",
      "816: reward: 0.0, discounted reward: 0.42718210106092996\n",
      "817: reward: 0.0, discounted reward: 0.44032527896111684\n",
      "818: reward: 0.0, discounted reward: 0.4538749468994538\n",
      "819: reward: 0.0, discounted reward: 0.46784367673279087\n",
      "820: reward: 0.0, discounted reward: 0.482244429138293\n",
      "821: reward: 0.0, discounted reward: 0.4970905656388107\n",
      "822: reward: 0.0, discounted reward: 0.5123958610001691\n",
      "823: reward: 0.0, discounted reward: 0.5281745160118788\n",
      "824: reward: 0.0, discounted reward: 0.5444411706631259\n",
      "825: reward: 0.0, discounted reward: 0.5612109177262673\n",
      "826: reward: 0.0, discounted reward: 0.5784993167604336\n",
      "827: reward: 0.0, discounted reward: 0.596322408548234\n",
      "828: reward: 0.0, discounted reward: 0.6146967299789561\n",
      "829: reward: 0.0, discounted reward: 0.6336393293920716\n",
      "830: reward: 0.0, discounted reward: 0.6531677823952833\n",
      "831: reward: 0.0, discounted reward: 0.6733002081717904\n",
      "832: reward: 0.0, discounted reward: 0.6940552862919007\n",
      "833: reward: 0.0, discounted reward: 0.7154522740445919\n",
      "834: reward: 0.0, discounted reward: 0.7375110243050982\n",
      "835: reward: 0.0, discounted reward: 0.7602520039551046\n",
      "836: reward: 0.0, discounted reward: 0.7836963128726371\n",
      "837: reward: 0.0, discounted reward: 0.8078657035092686\n",
      "838: reward: 0.0, discounted reward: 0.8327826010728061\n",
      "839: reward: 0.0, discounted reward: 0.8584701243341849\n",
      "840: reward: 0.0, discounted reward: 0.8849521070778745\n",
      "841: reward: 0.0, discounted reward: 0.9122531202156988\n",
      "842: reward: 0.0, discounted reward: 0.9403984945845898\n",
      "843: reward: 0.0, discounted reward: 0.969414344449426\n",
      "844: reward: 0.0, discounted reward: 0.9993275917327622\n",
      "845: reward: 0.0, discounted reward: 1.0301659909939336\n",
      "846: reward: 0.0, discounted reward: 1.061958155180708\n",
      "847: reward: 0.0, discounted reward: 1.094733582177383\n",
      "848: reward: 0.0, discounted reward: 1.128522682173955\n",
      "849: reward: 0.0, discounted reward: 1.1633568058817612\n",
      "850: reward: 0.0, discounted reward: 1.1992682736217677\n",
      "851: reward: 10.0, discounted reward: 1.2362904053124957\n",
      "852: reward: 0.0, discounted reward: 0.8996114549442253\n",
      "853: reward: 0.0, discounted reward: 0.9273658499748234\n",
      "854: reward: 0.0, discounted reward: 0.9559786283568833\n",
      "855: reward: 0.0, discounted reward: 0.9854763380291098\n",
      "856: reward: 0.0, discounted reward: 1.0158863480004774\n",
      "857: reward: 0.0, discounted reward: 1.0472368737441555\n",
      "858: reward: 0.0, discounted reward: 1.0795570033768134\n",
      "859: reward: 0.0, discounted reward: 1.1128767246475944\n",
      "860: reward: 0.0, discounted reward: 1.147226952761802\n",
      "861: reward: 0.0, discounted reward: 1.1826395590651084\n",
      "862: reward: 15.0, discounted reward: 1.219147400614909\n",
      "863: reward: 0.0, discounted reward: 0.6945152060044711\n",
      "864: reward: 0.0, discounted reward: 0.7159264180781695\n",
      "865: reward: 5.0, discounted reward: 0.737999832587137\n",
      "866: reward: 0.0, discounted reward: 0.5733328817984692\n",
      "867: reward: 0.0, discounted reward: 0.5909961869379614\n",
      "868: reward: 0.0, discounted reward: 0.609205779865273\n",
      "869: reward: 0.0, discounted reward: 0.6279785560789962\n",
      "870: reward: 0.0, discounted reward: 0.6473319336189172\n",
      "871: reward: 0.0, discounted reward: 0.667283869227083\n",
      "872: reward: 0.0, discounted reward: 0.6878528750086973\n",
      "873: reward: 0.0, discounted reward: 0.7090580356082996\n",
      "874: reward: 0.0, discounted reward: 0.7309190259171681\n",
      "875: reward: 0.0, discounted reward: 0.7534561293283725\n",
      "876: reward: 0.0, discounted reward: 0.7766902565564184\n",
      "877: reward: 0.0, discounted reward: 0.80064296503894\n",
      "878: reward: 20.0, discounted reward: 0.8253364789384466\n",
      "879: reward: 0.0, discounted reward: 0.10110151688030852\n",
      "880: reward: 0.0, discounted reward: 0.1041597282594452\n",
      "881: reward: 0.0, discounted reward: 0.10731252349566858\n",
      "882: reward: 0.0, discounted reward: 0.11056282786290918\n",
      "883: reward: 0.0, discounted reward: 0.11391365710748713\n",
      "884: reward: 0.0, discounted reward: 0.11736812024622727\n",
      "885: reward: 0.0, discounted reward: 0.12092942245111403\n",
      "886: reward: 0.0, discounted reward: 0.12460086802316221\n",
      "887: reward: 0.0, discounted reward: 0.12838586345826344\n",
      "888: reward: 0.0, discounted reward: 0.13228792060785236\n",
      "889: reward: 0.0, discounted reward: 0.13631065993732544\n",
      "890: reward: 0.0, discounted reward: 0.14045781388523584\n",
      "891: reward: 0.0, discounted reward: 0.14473323032638058\n",
      "892: reward: 0.0, discounted reward: 0.1491408761419937\n",
      "893: reward: 0.0, discounted reward: 0.15368484090035775\n",
      "894: reward: 0.0, discounted reward: 0.15836934065124852\n",
      "895: reward: 0.0, discounted reward: 0.16319872183773387\n",
      "896: reward: 0.0, discounted reward: 0.16817746532895586\n",
      "897: reward: 0.0, discounted reward: 0.17331019057763836\n",
      "898: reward: 0.0, discounted reward: 0.17860165990617702\n",
      "899: reward: 0.0, discounted reward: 0.184056782925289\n",
      "900: reward: 0.0, discounted reward: 0.189680621089322\n",
      "901: reward: 0.0, discounted reward: 0.19547839239244877\n",
      "902: reward: 0.0, discounted reward: 0.20145547621010526\n",
      "903: reward: 0.0, discounted reward: 0.2076174182901635\n",
      "904: reward: 0.0, discounted reward: 0.21396993589847096\n",
      "905: reward: 0.0, discounted reward: 0.2205189231235302\n",
      "906: reward: 0.0, discounted reward: 0.22727045634524073\n",
      "907: reward: 0.0, discounted reward: 0.2342307998727774\n",
      "908: reward: 0.0, discounted reward: 0.2414064117568358\n",
      "909: reward: 0.0, discounted reward: 0.24880394978163828\n",
      "910: reward: 0.0, discounted reward: 0.2564302776422594\n",
      "911: reward: 0.0, discounted reward: 0.2642924713130028\n",
      "912: reward: 0.0, discounted reward: 0.2723978256127383\n",
      "913: reward: 0.0, discounted reward: 0.2807538609732903\n",
      "914: reward: 0.0, discounted reward: 0.2893683304171584\n",
      "915: reward: 0.0, discounted reward: 0.29824922675104304\n",
      "916: reward: 0.0, discounted reward: 0.30740478998185194\n",
      "917: reward: 0.0, discounted reward: 0.31684351496206725\n",
      "918: reward: 0.0, discounted reward: 0.3265741592715677\n",
      "919: reward: 0.0, discounted reward: 0.3366057513432175\n",
      "920: reward: 0.0, discounted reward: 0.3469475988397638\n",
      "921: reward: 0.0, discounted reward: 0.35760929728981145\n",
      "922: reward: 25.0, discounted reward: 0.3686007389908916\n",
      "923: reward: 0.0, discounted reward: -0.5571831189150835\n",
      "924: reward: 0.0, discounted reward: -0.574484226168794\n",
      "925: reward: 0.0, discounted reward: -0.5923204192138564\n",
      "926: reward: 0.0, discounted reward: -0.610708247095364\n",
      "927: reward: 0.0, discounted reward: -0.6296647706845472\n",
      "928: reward: 0.0, discounted reward: -0.6492075785084472\n",
      "929: reward: 0.0, discounted reward: -0.669354803069169\n",
      "930: reward: 0.0, discounted reward: -0.6901251376678512\n",
      "931: reward: 0.0, discounted reward: -0.7115378537489669\n",
      "932: reward: 0.0, discounted reward: -0.733612818781045\n",
      "933: reward: 0.0, discounted reward: -0.7563705146904038\n",
      "934: reward: 0.0, discounted reward: -0.7798320568650006\n",
      "935: reward: 0.0, discounted reward: -0.804019213746028\n",
      "936: reward: 0.0, discounted reward: -0.8289544270254379\n",
      "937: reward: 0.0, discounted reward: -0.8546608324681284\n",
      "938: reward: 0.0, discounted reward: -0.8811622813781187\n",
      "939: reward: 0.0, discounted reward: -0.9084833627286241\n",
      "940: reward: 0.0, discounted reward: -0.936649425976568\n",
      "941: reward: 0.0, discounted reward: -0.9656866045826955\n",
      "942: reward: 0.0, discounted reward: -0.9956218402591157\n",
      "943: reward: 0.0, discounted reward: -1.0264829079667654\n",
      "944: reward: 30.0, discounted reward: -1.058298441685992\n",
      "945: reward: 0.0, discounted reward: -2.2156362503077216\n",
      "946: reward: 0.0, discounted reward: -2.284229722449864\n",
      "947: reward: 0.0, discounted reward: -2.354944642184032\n",
      "948: reward: 0.0, discounted reward: -2.427846621291421\n",
      "949: reward: 0.0, discounted reward: -2.503003300783575\n",
      "950: reward: 0.0, discounted reward: -2.5804844136620844\n",
      "951: reward: 0.0, discounted reward: -2.6603618496193104\n",
      "952: reward: 0.0, discounted reward: -2.7427097217401624\n",
      "953: reward: 0.0, discounted reward: -2.8276044352668137\n",
      "954: reward: 0.0, discounted reward: -2.915124758490166\n",
      "955: reward: 0.0, discounted reward: -3.0053518958338272\n",
      "956: reward: 0.0, discounted reward: -3.0983695631984265\n",
      "957: reward: 0.0, discounted reward: -3.194264065636158\n",
      "958: reward: 0.0, discounted reward: -3.2931243774276338\n",
      "959: reward: 0.0, discounted reward: -3.3950422246353407\n",
      "960: reward: 0.0, discounted reward: -3.5001121702102966\n",
      "961: reward: 0.0, discounted reward: -3.6084317017308694\n",
      "962: reward: 0.0, discounted reward: -3.7201013218551715\n",
      "963: reward: 0.0, discounted reward: -3.8352246415709463\n",
      "964: reward: 0.0, discounted reward: -3.953908476329478\n",
      "965: reward: 0.0, discounted reward: -4.076262945152706\n",
      "966: reward: 0.0, discounted reward: -4.202401572805519\n",
      "967: reward: 0.0, discounted reward: -4.332441395128005\n",
      "968: reward: 0.0, discounted reward: -4.466503067625414\n",
      "969: reward: 0.0, discounted reward: -4.604710977416557\n",
      "970: reward: -121.66666666666667, discounted reward: -4.747193358644541\n",
      "971: reward: 0.0, discounted reward: -0.3334549047557984\n",
      "972: reward: 0.0, discounted reward: -0.34383658270561357\n",
      "973: reward: 0.0, discounted reward: -0.3545393434786188\n",
      "974: reward: 0.0, discounted reward: -0.36557311747140775\n",
      "975: reward: 0.0, discounted reward: -0.37694814220624173\n",
      "976: reward: 0.0, discounted reward: -0.3886749718297819\n",
      "977: reward: 0.0, discounted reward: -0.40076448690559646\n",
      "978: reward: 0.0, discounted reward: -0.41322790450952895\n",
      "979: reward: 0.0, discounted reward: -0.4260767886372945\n",
      "980: reward: 0.0, discounted reward: -0.43932306093396\n",
      "981: reward: 0.0, discounted reward: -0.4529790117552646\n",
      "982: reward: 0.0, discounted reward: -0.4670573115710426\n",
      "983: reward: 0.0, discounted reward: -0.4815710227213292\n",
      "984: reward: 0.0, discounted reward: -0.4965336115360576\n",
      "985: reward: 0.0, discounted reward: -0.511958960829592\n",
      "986: reward: 0.0, discounted reward: -0.5278613827816894\n",
      "987: reward: 0.0, discounted reward: -0.5442556322168413\n",
      "988: reward: 0.0, discounted reward: -0.5611569202943175\n",
      "989: reward: 0.0, discounted reward: -0.5785809286216126\n",
      "990: reward: 0.0, discounted reward: -0.596543823804391\n",
      "991: reward: 0.0, discounted reward: -0.6150622724464306\n",
      "992: reward: 0.0, discounted reward: -0.6341534566134818\n",
      "993: reward: 0.0, discounted reward: -0.6538350897753902\n",
      "994: reward: 0.0, discounted reward: -0.6741254332412752\n",
      "995: reward: 0.0, discounted reward: -0.6950433131030123\n",
      "996: reward: 0.0, discounted reward: -0.7166081377027412\n",
      "997: reward: 0.0, discounted reward: -0.738839915640606\n",
      "998: reward: 0.0, discounted reward: -0.7617592743394359\n",
      "999: reward: 0.0, discounted reward: -0.7853874791835903\n",
      "1000: reward: 0.0, discounted reward: -0.8097464532497288\n",
      "1001: reward: 0.0, discounted reward: -0.8348587976478099\n",
      "1002: reward: 0.0, discounted reward: -0.8607478124911924\n",
      "1003: reward: 0.0, discounted reward: -0.8874375185152981\n",
      "1004: reward: 0.0, discounted reward: -0.9149526793648914\n",
      "1005: reward: 0.0, discounted reward: -0.943318824570658\n",
      "1006: reward: 0.0, discounted reward: -0.9725622732363965\n",
      "1007: reward: 0.0, discounted reward: -1.0027101584588074\n",
      "1008: reward: 0.0, discounted reward: -1.03379045250253\n",
      "1009: reward: 0.0, discounted reward: -1.0658319927537905\n",
      "1010: reward: 0.0, discounted reward: -1.0988645084767394\n",
      "1011: reward: 0.0, discounted reward: -1.1329186483973053\n",
      "1012: reward: 0.0, discounted reward: -1.1680260091401566\n",
      "1013: reward: 0.0, discounted reward: -1.2042191645451579\n",
      "1014: reward: 0.0, discounted reward: -1.2415316958905203\n",
      "1015: reward: 0.0, discounted reward: -1.2799982230506877\n",
      "1016: reward: 0.0, discounted reward: -1.3196544366178704\n",
      "1017: reward: 0.0, discounted reward: -1.360537131017028\n",
      "1018: reward: 0.0, discounted reward: -1.4026842386450253\n",
      "1019: reward: 0.0, discounted reward: -1.4461348650656414\n",
      "1020: reward: 0.0, discounted reward: -1.4909293252930806\n",
      "1021: reward: 0.0, discounted reward: -1.537109181197657\n",
      "1022: reward: 0.0, discounted reward: -1.5847172800683544\n",
      "1023: reward: 0.0, discounted reward: -1.6337977943680422\n",
      "1024: reward: 0.0, discounted reward: -1.684396262718236\n",
      "1025: reward: 0.0, discounted reward: -1.7365596321514256\n",
      "1026: reward: 0.0, discounted reward: -1.7903363016701774\n",
      "1027: reward: 0.0, discounted reward: -1.8457761671534272\n",
      "1028: reward: 10.0, discounted reward: -1.9029306676516227\n",
      "1029: reward: 0.0, discounted reward: -2.3366989295548657\n",
      "1030: reward: 0.0, discounted reward: -2.4090366082716623\n",
      "1031: reward: 0.0, discounted reward: -2.4836115347838237\n",
      "1032: reward: 0.0, discounted reward: -2.560492902322135\n",
      "1033: reward: 0.0, discounted reward: -2.6397520441142075\n",
      "1034: reward: 0.0, discounted reward: -2.721462499569953\n",
      "1035: reward: 0.0, discounted reward: -2.8057000825140204\n",
      "1036: reward: 0.0, discounted reward: -2.8925429515285233\n",
      "1037: reward: 0.0, discounted reward: -2.982071682471309\n",
      "1038: reward: 0.0, discounted reward: -3.074369343237068\n",
      "1039: reward: 0.0, discounted reward: -3.1695215708306335\n",
      "1040: reward: 0.0, discounted reward: -3.2676166508240003\n",
      "1041: reward: 15.0, discounted reward: -3.3687455992707704\n",
      "1042: reward: 0.0, discounted reward: -4.035271391815817\n",
      "1043: reward: 0.0, discounted reward: -4.160142239468519\n",
      "1044: reward: 0.0, discounted reward: -4.288875072100171\n",
      "1045: reward: -121.66666666666667, discounted reward: -4.421589332545174\n",
      "1046: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1047: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1048: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1049: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1050: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1051: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1052: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1053: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1054: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1055: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1056: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1057: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1058: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1059: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1060: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1061: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1062: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1063: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1064: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1065: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1066: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1067: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1068: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1069: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1070: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1071: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1072: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1073: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1074: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1075: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1076: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1077: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1078: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1079: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1080: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1081: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1082: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1083: reward: 0.0, discounted reward: 0.0022193489548892084\n",
      "1084: reward: 0.0, discounted reward: 0.0022193489548892084\n"
     ]
    }
   ],
   "source": [
    "print(\"num_frames: \" + str(num_frames))\n",
    "\n",
    "\n",
    "#show_observation(frames[:,:,125])\n",
    "#show_observation(concatenated_frames)\n",
    "\n",
    "\n",
    "# print(\"rewards: \" + str(rewards))\n",
    "# print(\"discounted_rewards: \" + str(discounted_rewards))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(\"action_taken_one_hot_reshaped: \" + str(action_taken_one_hot_reshaped))\n",
    "#rint(\"all_logits: \" + str(all_logits))\n",
    "\n",
    "\n",
    "\n",
    "# num_frames = np.ma.size(frames, axis=2)\n",
    "\n",
    "# X_input = np.empty([n_steps, 210 * 160])\n",
    "# input_index = 0\n",
    "\n",
    "# for i in range(num_frames, num_frames-n_steps, -1):\n",
    "#     this_frame = frames[:,:,i-1]\n",
    "#     X_input[input_index] = this_frame.flatten()\n",
    "#     input_index = input_index + 1\n",
    "    \n",
    "#obs_greyscale_reshape.shape\n",
    "\n",
    "\n",
    "\n",
    "#show_observation(obs_greyscale_reshape[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_input = np.empty([n_steps, 10])\n",
    "for i in range(0,  29):\n",
    "    y_input[i] = np.zeros(10)\n",
    "    \n",
    "y_input  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#len(rewards)\n",
    "#len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 180.31735229, -245.90328979,  -55.46500015,  -39.02265549,\n",
       "        -171.08708191,   39.90904999, -253.30400085,   10.52585697,\n",
       "           9.43663216], dtype=float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(all_gradients)\n",
    "np.shape(all_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getsizeof(all_variables))\n",
    "#np.shape(all_gradients[1])\n",
    "#all_gradients\n",
    "#all_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8392"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "def get_size(obj, seen=None):\n",
    "    \"\"\"Recursively finds size of objects\"\"\"\n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    # Important mark as seen *before* entering recursion to gracefully handle\n",
    "    # self-referential objects\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "    return size\n",
    "\n",
    "get_size(all_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
