{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.0392279, 3.488031, 2.87559, 2.1951, 2.439, 1.71, 0.9]\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "load_model           = False\n",
    "environment_name     = \"BipedalWalker-v2\"\n",
    "saver_file           = \"./models/bipedal\"\n",
    "learning_rate        =.00005\n",
    "frame_limit          = 500\n",
    "discount_decay_rate  = .90\n",
    "n_epochs             = 5001\n",
    "random_action_probability_range  = [.0, .99]\n",
    "random_action_repeat_steps_range = [ 1,  20]\n",
    "\n",
    "    \n",
    "# def helper_discount_rewards(rewards, discount_rate, begin_index, end_index):\n",
    "#     #Takes in rewards and applies discount rate\n",
    "#     discounted_rewards = np.zeros(len(rewards))\n",
    "#     cumulative_rewards = 0\n",
    "#     for step in reversed(range(len(rewards))):\n",
    "#         cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "#         discounted_rewards[step] = cumulative_rewards\n",
    "        \n",
    "#     relevant_discounted_rewards = discounted_rewards[begin_index:end_index]\n",
    "    \n",
    "#     reward_mean = relevant_discounted_rewards.mean()\n",
    "#     reward_std = relevant_discounted_rewards.std()\n",
    "        \n",
    "#     return [(discounted_reward - reward_mean)/reward_std for discounted_reward in discounted_rewards]\n",
    "\n",
    "def helper_discount_rewards(rewards, discount_rate, begin_index, end_index):\n",
    "    #Takes in rewards and applies discount rate\n",
    "    discounted_rewards = []\n",
    "    \n",
    "    for i in range(len(rewards)):\n",
    "        this_reward = 0.0\n",
    "        exponent = 1\n",
    "        for j in range(i,len(rewards)):\n",
    "            this_reward += rewards[j]*discount_rate**exponent\n",
    "            exponent += 1\n",
    "        discounted_rewards.append(this_reward)\n",
    "    return discounted_rewards\n",
    "    \n",
    "reward_test = [1,1,1,0,1,1,1]\n",
    "print(helper_discount_rewards(reward_test, 0.9, 0, 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the NN\n",
    "reg_param = 0.00001\n",
    "\n",
    "value_scale = 0.5  #how much does the value vs progressive gains influence the loss\n",
    "\n",
    "activ = tf.nn.sigmoid\n",
    "\n",
    "n_inputs = 4\n",
    "n_outputs = 4\n",
    "n_obs_inputs = 24\n",
    "\n",
    "n_hidden1 = 256\n",
    "n_hidden2 = 256\n",
    "n_hidden3 = 256\n",
    "n_hidden4 = 256\n",
    "n_hidden5 = 256\n",
    "n_hidden6 = 128\n",
    "n_hidden7 = 64\n",
    "n_hidden8 = 32\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf_input_obs = tf.placeholder(tf.float32, shape=(None, n_obs_inputs))\n",
    "tf_input_action = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
    "tf_input_learning_rate = tf.placeholder(tf.float32)\n",
    "tf_input_reward = tf.placeholder(tf.float32)\n",
    "\n",
    "input_layer = tf.layers.dense(tf_input_obs, n_obs_inputs, activation=activ, name=\"input\", kernel_initializer=initializer)\n",
    "\n",
    "hidden1 = tf.layers.dense(input_layer, n_hidden1, activation=activ, name=\"hidden1\", kernel_initializer=initializer)\n",
    "hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=activ, name=\"hidden2\", kernel_initializer=initializer)\n",
    "hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=activ, name=\"hidden3\", kernel_initializer=initializer)\n",
    "hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=activ, name=\"hidden4\", kernel_initializer=initializer)\n",
    "hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=activ, name=\"hidden5\", kernel_initializer=initializer)\n",
    "hidden6 = tf.layers.dense(hidden5, n_hidden6, activation=activ, name=\"hidden6\", kernel_initializer=initializer)\n",
    "hidden7 = tf.layers.dense(hidden6, n_hidden7, activation=activ, name=\"hidden7\", kernel_initializer=initializer)\n",
    "hidden8 = tf.layers.dense(hidden7, n_hidden8, activation=activ, name=\"hidden8\", kernel_initializer=initializer)\n",
    "\n",
    "logits = tf.layers.dense(hidden8, n_outputs, name=\"output\")\n",
    "value  = tf.layers.dense(hidden8, 1,         name='value')\n",
    "\n",
    "#pg_loss    = tf.reduce_mean(1.0 - tf_input_reward) * tf.reduce_mean(tf.square(logits-tf_input_action))\n",
    "#value_loss = value_scale * tf.reduce_mean(tf.square(1.0- tf_input_reward))\n",
    "#loss = pg_loss + value_loss\n",
    "\n",
    "#loss = tf.reduce_mean(tf.square(logits-tf_input_action))\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf_input_action)\n",
    "optimizer = tf.train.AdamOptimizer(tf_input_learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "# regularization loss\n",
    "#policy_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"policy_network\")\n",
    "        \n",
    "# compute policy loss and regularization loss\n",
    "# cross_entropy_loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_input_action)\n",
    "# pg_loss            = tf.reduce_mean(cross_entropy_loss)\n",
    "# reg_loss           = tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in policy_network_variables])\n",
    "# loss               = pg_loss + reg_param * reg_loss\n",
    "\n",
    "# gradients = optimizer.compute_gradients(loss)\n",
    "\n",
    "# compute policy gradients\n",
    "# for i, (grad, var) in enumerate(gradients):\n",
    "#     if grad is not None:\n",
    "#         gradients[i] = (grad * tf_input_reward, var)\n",
    "\n",
    "\n",
    "# training_op = optimizer.apply_gradients(gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating new model before training: ./models/bipedal\n",
      "Epoch: 0, reward total: -95.15154892678373, frames trained: 18.181818181818183% , average loss: [[0.22185704 0.4891042  0.4268044  0.7381619 ]]\n",
      "\n",
      "Epoch: 1, reward total: -96.49784169271092, frames trained: 17.6% , average loss: [[0.03749263 0.37573993 0.29450443 0.7242451 ]]\n",
      "\n",
      "Epoch: 2, reward total: -122.17897984290681, frames trained: 0.0%\n",
      "\n",
      "Epoch: 3, reward total: -122.36671444379849, frames trained: 0.0%\n",
      "\n",
      "Epoch: 4, reward total: -31.71634482024176, frames trained: 0.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name)\n",
    "global_step = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if load_model:\n",
    "        print(\"Loading existing model before training: {}\".format(saver_file))\n",
    "        saver.restore(sess, saver_file)\n",
    "    else:\n",
    "        print(\"Creating new model before training: {}\".format(saver_file))\n",
    "        sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):  #play n_epochs games\n",
    "        game_step_counter = 0\n",
    "        random_action_counter = 0\n",
    "        \n",
    "        all_actions = []\n",
    "        all_rewards = []\n",
    "        all_observations = []\n",
    "        all_logits = []\n",
    "\n",
    "        observation = env.reset()\n",
    "        input_action = env.action_space.sample()  #set an initial input value\n",
    "        \n",
    "        while True:\n",
    "            random_action_probability  = random.uniform(random_action_probability_range[0], random_action_probability_range[1])\n",
    "            random_action_repeat_steps = random.randint(random_action_repeat_steps_range[0], random_action_repeat_steps_range[1])\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                random_action_probability = 0.0\n",
    "                \n",
    "            #feed data into the AI to get action from the AI\n",
    "            feed_dict = {tf_input_obs    : np.reshape(observation, (1, len(observation))), \n",
    "                         tf_input_action : np.reshape(input_action, (1, len(input_action))),\n",
    "                         tf_input_learning_rate: 0.0,\n",
    "                         tf_input_reward: 0.0}\n",
    "            logits_out = sess.run([logits], feed_dict=feed_dict)\n",
    "\n",
    "            # set the input value to feed into the next step\n",
    "            \n",
    "            if random_action_counter > 0 and random_action_counter < random_action_repeat_steps:\n",
    "                random_action_counter += 1\n",
    "            elif random_action_counter >= random_action_repeat_steps:\n",
    "                random_action_counter = 0\n",
    "            elif random.uniform(0.0, 1.0) < random_action_probability:\n",
    "                input_action = env.action_space.sample()\n",
    "                random_action_counter += 1\n",
    "                #print(\"Random action {}: \".format(input_action))\n",
    "            else:\n",
    "                input_action = logits_out[0][0]\n",
    "\n",
    "            #run the next step given the input from the logits\n",
    "            observation, reward_float, done_bool, info_dict = env.step(input_action)\n",
    "                \n",
    "            #add the data to our lists\n",
    "            all_observations.append(observation)\n",
    "            all_logits.append(logits_out[0][0])\n",
    "            all_actions.append(input_action)\n",
    "            all_rewards.append(reward_float)\n",
    "\n",
    "            if done_bool:\n",
    "                break\n",
    "                \n",
    "            if game_step_counter > frame_limit:\n",
    "                break\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                env.render()  #display the current frame.\n",
    "               \n",
    "            game_step_counter += 1\n",
    "        \n",
    "        if epoch % 50 == 0 and epoch > 0:\n",
    "            print(\"Saving model at epoch {}: {}\".format(epoch, saver_file))\n",
    "            saver.save(sess, saver_file)\n",
    "\n",
    "        num_frames = len(all_actions)\n",
    "        discounted_rewards = helper_discount_rewards(all_rewards, discount_decay_rate, 0, num_frames) #-1-frames_to_skip_end\n",
    "        discounted_rewards_median = np.median(discounted_rewards)\n",
    "        discounted_rewards_mean = np.mean(discounted_rewards)\n",
    "        \n",
    "        loss_out_sum = 0.0\n",
    "        training_frame_counter = 0\n",
    "        #use stored data to train\n",
    "        for i in range(num_frames): \n",
    "            train_input_action = all_actions[i]\n",
    "            \n",
    "            if discounted_rewards[i] + all_rewards[i] >= 1.0:\n",
    "                train_input_action = all_actions[i]\n",
    "                training_frame_counter += 1\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            feed_dict = {tf_input_obs : np.reshape(all_observations[i], (1, len(all_observations[i]))), \n",
    "                         tf_input_action : np.reshape(train_input_action, (1, len(train_input_action))),\n",
    "                         tf_input_learning_rate: learning_rate,\n",
    "                         tf_input_reward: discounted_rewards[i]}\n",
    "        \n",
    "            loss_out, _, logits_out = sess.run([loss, training_op, logits], feed_dict=feed_dict)\n",
    "            loss_out_sum += abs(loss_out)\n",
    "\n",
    "        if training_frame_counter > 0:\n",
    "            print(\"Epoch: \" + str(epoch) + \", reward total: \" + str(np.sum(all_rewards)) + \", frames trained: \" + str(100*training_frame_counter/num_frames) + \"% , average loss: \" + str(loss_out_sum/training_frame_counter))\n",
    "        else:\n",
    "            print(\"Epoch: \" + str(epoch) + \", reward total: \" + str(np.sum(all_rewards)) + \", frames trained: \" + str(training_frame_counter/num_frames) + \"%\")\n",
    "        #TODO - should learning rate decrease over time?\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_frames)\n",
    "print(len(all_observations))\n",
    "print(len(all_actions))\n",
    "print(len(all_rewards))\n",
    "print(len(all_logits))\n",
    "print(len(discounted_rewards))\n",
    "\n",
    "values = discounted_rewards\n",
    "\n",
    "for value in all_rewards:\n",
    "    print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "obs.shape: (24,)\n",
      "env.action_space: Box(4,)\n",
      "action: [0.09762701 0.43037874 0.20552675 0.08976637]\n",
      "<class 'numpy.ndarray'>\n",
      "observation: [ 0.00245932 -0.00691658  0.00590502  0.01961307 -0.29061297 -0.71013689\n",
      "  1.4722293   0.99358988  1.          0.30079773 -0.01692069  0.1633203\n",
      "  0.33325795  1.          0.45289648  0.45803979  0.4740701   0.50296849\n",
      "  0.54874223  0.61897415  0.72858626  0.91021472  1.          1.        ]\n",
      "reward_float: -0.01773284123155096\n",
      "done_bool: False\n",
      "info_dict: {}\n",
      "action: [-0.1526904   0.29178822 -0.12482557  0.78354603]\n",
      "<class 'numpy.ndarray'>\n",
      "observation: [ 0.00255965  0.00256462  0.00806203  0.00679228 -0.01382717 -0.39574927\n",
      "  1.08375649  0.          1.          0.26067981 -0.41715193  0.26995051\n",
      "  0.24234966  1.          0.45600694  0.46118557  0.47732601  0.50642282\n",
      "  0.55251098  0.62322521  0.73359013  0.916466    1.          1.        ]\n",
      "reward_float: -0.044960508224245105\n",
      "done_bool: False\n",
      "info_dict: {}\n",
      "action: [ 0.92732555 -0.23311697  0.5834501   0.05778984]\n",
      "<class 'numpy.ndarray'>\n",
      "observation: [-0.02495774 -0.05563801 -0.03339949 -0.01141822  0.04447325  0.99102986\n",
      "  0.99809675 -0.62425939  1.          0.34255442  0.9341526   0.21233523\n",
      " -0.53334411  1.          0.45618621  0.46136689  0.47751364  0.5066219\n",
      "  0.55272818  0.62347025  0.73387855  0.91682631  1.          1.        ]\n",
      "reward_float: -0.18504284861062845\n",
      "done_bool: False\n",
      "info_dict: {}\n",
      "action: [ 0.13608912  0.85119325 -0.85792786 -0.8257414 ]\n",
      "<class 'numpy.ndarray'>\n",
      "observation: [-3.80832516e-02 -2.68256044e-02 -1.13563272e-02 -3.00161578e-04\n",
      "  1.67297900e-01  1.97402969e-01  4.35448945e-01  5.47744334e-04\n",
      "  1.00000000e+00  3.60631257e-01  4.27372485e-01  1.38471365e-01\n",
      " -9.99812365e-01  0.00000000e+00  4.60161686e-01  4.65387523e-01\n",
      "  4.81674999e-01  5.11036932e-01  5.57545006e-01  6.28903508e-01\n",
      "  7.40274012e-01  9.24816072e-01  1.00000000e+00  1.00000000e+00]\n",
      "reward_float: -0.1501898294687271\n",
      "done_bool: False\n",
      "info_dict: {}\n",
      "action: [-0.9595632   0.6652397   0.5563135   0.74002427]\n",
      "<class 'numpy.ndarray'>\n",
      "observation: [-0.04031942 -0.00449467  0.02325811  0.07421227  0.13379088 -0.42001471\n",
      "  0.50488901  0.59256999  1.          0.33028516 -0.36620191  0.26052594\n",
      "  1.00000032  0.          0.4636263   0.46889147  0.48530158  0.51488459\n",
      "  0.56174278  0.63363862  0.74584758  0.93177909  1.          1.        ]\n",
      "reward_float: -0.07605603107810022\n",
      "done_bool: False\n",
      "info_dict: {}\n",
      "action: [ 0.9572367   0.59831715 -0.07704128  0.56105834]\n",
      "<class 'numpy.ndarray'>\n",
      "observation: [-0.0575121  -0.03442437  0.01532184  0.06768165  0.11695749 -0.21027276\n",
      "  0.57577738  0.5988518   1.          0.3139213  -0.18875003  0.38332766\n",
      "  1.          0.          0.46672022  0.47202054  0.48854014  0.51832056\n",
      "  0.5654915   0.63786709  0.75082487  0.93799716  1.          1.        ]\n",
      "reward_float: -0.13584959641098976\n",
      "done_bool: False\n",
      "info_dict: {}\n",
      "action: [-0.76345116  0.27984205 -0.71329343  0.88933784]\n",
      "<class 'numpy.ndarray'>\n",
      "observation: [-0.04738653  0.02020624  0.05192268  0.08245699  0.04249695 -0.9231286\n",
      "  0.69652346  0.99830691  1.          0.23311709 -1.00021863  0.50612652\n",
      "  0.99999698  0.          0.47059003  0.47593427  0.49259087  0.52261823\n",
      "  0.57018024  0.64315593  0.75705034  0.94577456  1.          1.        ]\n",
      "reward_float: 0.013999748607475375\n",
      "done_bool: False\n",
      "info_dict: {}\n",
      "action: [ 0.04369664 -0.17067613 -0.47088876  0.5484674 ]\n",
      "<class 'numpy.ndarray'>\n",
      "observation: [-0.03245258  0.02983283  0.04393652  0.10632439  0.02263976 -0.23718901\n",
      "  0.62805572 -0.58512243  1.          0.15254909 -0.99771792  0.62711972\n",
      "  0.9860425   1.          0.47559673  0.48099783  0.49783164  0.52817845\n",
      "  0.5762465   0.64999861  0.76510471  0.95583683  1.          1.        ]\n",
      "reward_float: 0.0716859445075206\n",
      "done_bool: False\n",
      "info_dict: {}\n",
      "action: [-0.08769934  0.1368679  -0.9624204   0.23527099]\n",
      "<class 'numpy.ndarray'>\n",
      "observation: [-0.0162439   0.0324132   0.05277648  0.09182726 -0.05635    -0.98199493\n",
      "  0.73870862  0.90852849  0.          0.07197365 -0.99999791  0.74933478\n",
      "  1.00000381  1.          0.47991383  0.48536396  0.50235057  0.53297287\n",
      "  0.58147722  0.65589881  0.77204978  0.96451318  1.          1.        ]\n",
      "reward_float: 0.07905402829746523\n",
      "done_bool: False\n",
      "info_dict: {}\n",
      "action: [0.22419144 0.23386799 0.8874962  0.3636406 ]\n",
      "<class 'numpy.ndarray'>\n",
      "observation: [-0.02062005 -0.00872072  0.02946876  0.07379979 -0.10302593 -0.58170056\n",
      "  0.85976222  1.          0.          0.05181687 -0.24316923  0.87116364\n",
      "  0.99999984  0.          0.4832851   0.48877352  0.50587946  0.53671688\n",
      "  0.58556199  0.66050631  0.77747321  0.97128862  1.          1.        ]\n",
      "reward_float: -0.04835829955339432\n",
      "done_bool: False\n",
      "info_dict: {}\n",
      "action: [-0.2809842  -0.12593609  0.3952624  -0.8795491 ]\n",
      "<class 'numpy.ndarray'>\n",
      "observation: [-0.02859461 -0.01595236  0.01479919  0.0523875  -0.13851036 -0.44336003\n",
      "  0.92711887  0.55970204  0.          0.12493905  0.92015588  0.75256321\n",
      " -1.          0.          0.48567629  0.49119186  0.50838244  0.53937238\n",
      "  0.58845919  0.66377437  0.78131998  0.97609437  1.          1.        ]\n",
      "reward_float: -0.07604714767138282\n",
      "done_bool: False\n",
      "info_dict: {}\n",
      "action: [ 0.33353344  0.34127575 -0.5792349  -0.7421474 ]\n",
      "<class 'numpy.ndarray'>\n",
      "observation: [-0.03232238 -0.00747836  0.01739497  0.03612902 -0.1464325  -0.09828897\n",
      "  0.92725426  0.          0.          0.17044182  0.58050734  0.63544267\n",
      " -0.99999992  0.          0.48732746  0.49286178  0.5101108   0.54120612\n",
      "  0.59045982  0.666031    0.78397626  0.97941285  1.          1.        ]\n",
      "reward_float: -0.06178938996295252\n",
      "done_bool: False\n",
      "info_dict: {}\n",
      "action: [-0.3691433  -0.27257845  0.14039354 -0.12279698]\n",
      "<class 'numpy.ndarray'>\n",
      "observation: [-3.24292332e-02 -2.53614150e-04  1.71360075e-02  1.78231645e-02\n",
      " -1.36974990e-01  1.29949376e-01  8.09045702e-01 -9.99999841e-01\n",
      "  0.00000000e+00  2.16673538e-01  5.92344165e-01  5.18796951e-01\n",
      " -1.00000000e+00  0.00000000e+00  4.88139540e-01  4.93683100e-01\n",
      "  5.10960877e-01  5.42107999e-01  5.91443717e-01  6.67140901e-01\n",
      "  7.85282671e-01  9.81044948e-01  1.00000000e+00  1.00000000e+00]\n",
      "reward_float: -0.013350043684244157\n",
      "done_bool: False\n",
      "info_dict: {}\n",
      "action: [ 0.9767477  -0.79591036 -0.5822465  -0.677381  ]\n",
      "<class 'numpy.ndarray'>\n",
      "observation: [-0.03975289 -0.0146663   0.00941567  0.00440036 -0.08186159  0.7035104\n",
      "  0.69190088 -1.00000008  0.          0.24895115  0.42257336  0.40293765\n",
      " -0.99999984  0.          0.48828036  0.49382553  0.51110822  0.5422644\n",
      "  0.59161437  0.6673333   0.78550923  0.98132795  1.          1.        ]\n",
      "reward_float: -0.11447619311014574\n",
      "done_bool: False\n",
      "info_dict: {}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(environment_name)\n",
    "observation = env.reset()\n",
    "\n",
    "print(\"obs.shape: {}\".format(observation.shape)) #obs.shape: (210, 160, 3)\n",
    "print(\"env.action_space: {}\".format(env.action_space)) #env.action_space: Discrete(9)\n",
    "\n",
    "\n",
    "for _ in range(14):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    print(\"action: {}\".format(action))\n",
    "    print(type(action))\n",
    "    observation, reward_float, done_bool, info_dict = env.step(action) # take a random action\n",
    "    \n",
    "    print(\"observation: {}\".format(observation))\n",
    "    print(\"reward_float: {}\".format(reward_float))\n",
    "    print(\"done_bool: {}\".format(done_bool))\n",
    "    print(\"info_dict: {}\".format(info_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
